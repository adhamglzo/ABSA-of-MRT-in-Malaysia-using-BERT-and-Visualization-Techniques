{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ebe1861-793d-43fa-90e6-5988e493adeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADDED] KG04 KWASA DAMANSARA\n",
      "[ADDED] KG06 KOTA DAMANSARA\n",
      "[ADDED] KG07 SURIAN\n",
      "[ADDED] KG08 MUTIARA DAMANSARA\n",
      "[ADDED] KG09 BANDAR UTAMA\n",
      "[ADDED] KG10 TTDI\n",
      "[ADDED] KG11 PHILEO DAMANSARA\n",
      "[ADDED] KG13 PUSAT BANDAR DAMANSARA\n",
      "[ADDED] KG14 SEMANTAN\n",
      "[ADDED] KG15 MUZIUM NEGARA\n",
      "[ADDED] KG16 PASAR SENI\n",
      "[ADDED] KG17 MERDEKA\n",
      "[ADDED] KG18A BUKIT BINTANG\n",
      "[ADDED] KG20 TRX\n",
      "[ADDED] KG21 COCHRANE\n",
      "[ADDED] KG22 MALURI\n",
      "[ADDED] KG23 TAMAN PERTAMA\n",
      "[ADDED] KG24 TAMAN MIDAH\n",
      "[ADDED] KG25 TAMAN MUTIARA\n",
      "[ADDED] KG27 TAMAN SUNTEX\n",
      "[ADDED] KG28 SRI RAYA\n",
      "[ADDED] KG29 BTHO\n",
      "[ADDED] KG30 BATU 11\n",
      "[ADDED] KG31 BUKIT DUKONG\n",
      "[ADDED] KG33 SUNGAI JERNIH\n",
      "[ADDED] KG34 STADIUM KAJANG\n",
      "[ADDED] KG35 KAJANG\n",
      "✅ All station names inserted.\n"
     ]
    }
   ],
   "source": [
    "# Python script to insert station names from CSV filenames\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# ✅ Updated absolute paths\n",
    "csv_folder = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews_csvs\"\n",
    "db_path = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\instance\\mrt_reviews.db\"\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Loop through CSV files and insert station names\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        station_name = filename.replace(\".csv\", \"\").strip().upper()\n",
    "        try:\n",
    "            cursor.execute(\"INSERT OR IGNORE INTO stations (station_name) VALUES (?)\", (station_name,))\n",
    "            print(f\"[ADDED] {station_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to add {station_name}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"✅ All station names inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7534dadd-df1b-4bfe-a3c7-01b92c83e33f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: stations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get station_name → station_id map from stations table\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT station_id, station_name FROM stations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m station_map \u001b[38;5;241m=\u001b[39m {name\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper(): sid \u001b[38;5;28;01mfor\u001b[39;00m sid, name \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mfetchall()}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Process CSV files\u001b[39;00m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: stations"
     ]
    }
   ],
   "source": [
    "#insert reviews to reviews table \n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# File and DB paths\n",
    "csv_folder = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews_csvs\"\n",
    "db_path = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\instance\\mrt_reviews.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get station_name → station_id map from stations table\n",
    "cursor.execute(\"SELECT station_id, station_name FROM stations\")\n",
    "station_map = {name.strip().upper(): sid for sid, name in cursor.fetchall()}\n",
    "\n",
    "# Process CSV files\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        station_name_raw = filename.replace(\".csv\", \"\").strip().upper()\n",
    "        station_id = station_map.get(station_name_raw)\n",
    "\n",
    "        if not station_id:\n",
    "            print(f\"[WARNING] {station_name_raw} not found in stations table.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(csv_folder, filename))\n",
    "            if 'date' not in df.columns or 'cleaned_reviews' not in df.columns:\n",
    "                print(f\"[ERROR] Missing required columns in {filename}\")\n",
    "                continue\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                review_date = str(row['date']).strip()\n",
    "                review = str(row['cleaned_reviews']).strip()\n",
    "\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO reviews (station_id, station_name, review_date, raw_reviews)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                \"\"\", (station_id, station_name_raw, review_date, review))\n",
    "\n",
    "            print(f\"[SUCCESS] Imported reviews from {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FAILED] Could not import {filename}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"✅ All review data inserted into reviews table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db4f197-700c-4705-aebe-1a09e2017560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAILED] Error importing KG04 KWASA DAMANSARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG06 KOTA DAMANSARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG07 SURIAN.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG08 MUTIARA DAMANSARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG09 BANDAR UTAMA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG10 TTDI.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG11 PHILEO DAMANSARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG13 PUSAT BANDAR DAMANSARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG14 SEMANTAN.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG15 MUZIUM NEGARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG16 PASAR SENI.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG17 MERDEKA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG18A BUKIT BINTANG.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG20 TRX.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG21 COCHRANE.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG22 MALURI.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG23 TAMAN PERTAMA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG24 TAMAN MIDAH.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG25 TAMAN MUTIARA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG27 TAMAN SUNTEX.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG28 SRI RAYA.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG29 BTHO.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG30 BATU 11.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG31 BUKIT DUKONG.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG33 SUNGAI JERNIH.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG34 STADIUM KAJANG.csv: no such table: station_reviews\n",
      "[FAILED] Error importing KG35 KAJANG.csv: no such table: station_reviews\n",
      "✅ All CSVs imported to mrt_reviews.\n",
      "[INFO] Connecting to DB at: C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# === SETTINGS ===\n",
    "csv_folder = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews_csvs\"\n",
    "db_path = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews.db\"\n",
    "\n",
    "# === CONNECT TO DATABASE ===\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# === LOAD STATION NAME TO ID MAPPING ===\n",
    "cursor.execute(\"SELECT id, station_name FROM stations\")\n",
    "station_map = {name.strip().upper(): id for id, name in cursor.fetchall()}\n",
    "\n",
    "# === PROCESS EACH CSV FILE ===\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(csv_folder, filename)\n",
    "\n",
    "        station_name_raw = filename.replace(\".csv\", \"\").strip().upper()\n",
    "        station_id = station_map.get(station_name_raw)\n",
    "\n",
    "        if station_id is None:\n",
    "            print(f\"[WARNING] Station '{station_name_raw}' not found in stations table. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            if \"date\" not in df.columns or \"cleaned_reviews\" not in df.columns:\n",
    "                print(f\"[ERROR] Missing required columns in: {filename}\")\n",
    "                continue\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                review_date = row['date']\n",
    "                review_text = row['cleaned_reviews']\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO station_reviews (station_id, station_name, review_date, review)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                \"\"\", (station_id, station_name_raw, review_date, review_text))\n",
    "\n",
    "            print(f\"[SUCCESS] Imported: {filename} -> Station ID {station_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[FAILED] Error importing {filename}: {e}\")\n",
    "\n",
    "# === COMMIT & CLOSE ===\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"✅ All CSVs imported to mrt_reviews.\")\n",
    "print(\"[INFO] Connecting to DB at:\", db_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b860deb-c768-4055-b608-03bd56fe5426",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3429854308.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    DATABASE_PATH = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews.db  # change this to your actual SQLite DB path\u001b[0m\n\u001b[1;37m                                                                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "DATABASE_PATH = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews.db  # change this to your actual SQLite DB path\n",
    "CSV_FOLDER = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews_csvs\"\n",
    "\n",
    "# === CONNECT TO DATABASE ===\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# === LOAD station_name TO station_id MAP ===\n",
    "cursor.execute(\"SELECT id, station_name FROM stations\")\n",
    "station_map = {name.strip().upper(): id for id, name in cursor.fetchall()}\n",
    "\n",
    "# === LOOP THROUGH EACH CSV FILE ===\n",
    "for file_name in os.listdir(CSV_FOLDER):\n",
    "    if not file_name.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    station_name_raw = file_name.replace(\".csv\", \"\").strip().upper()\n",
    "    station_id = station_map.get(station_name_raw)\n",
    "\n",
    "    if station_id is None:\n",
    "        print(f\"⚠️ Station not found in DB: {station_name_raw}\")\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(CSV_FOLDER, file_name)\n",
    "    df = pd.read_csv(full_path)\n",
    "\n",
    "    # Rename for clarity\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    if 'date' not in df.columns or 'cleaned_review' not in df.columns:\n",
    "        print(f\"❌ Skipping {file_name}: Missing required columns.\")\n",
    "        continue\n",
    "\n",
    "    # Insert each row\n",
    "    rows = [(station_id, station_name_raw, row['date'], row['cleaned_review']) for _, row in df.iterrows()]\n",
    "    cursor.executemany(\"\"\"\n",
    "        INSERT INTO mrt_reviews (station_id, station_name, review_date, review)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    print(f\"✅ Inserted {len(rows)} rows from {file_name}\")\n",
    "\n",
    "# === COMMIT & CLOSE ===\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"✅ All CSV files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f24b45-71a1-4180-98db-c2c72fbe2ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLEANED] KG04 KWASA DAMANSARA.csv\n",
      "[CLEANED] KG06 KOTA DAMANSARA.csv\n",
      "[CLEANED] KG07 SURIAN.csv\n",
      "[CLEANED] KG08 MUTIARA DAMANSARA.csv\n",
      "[CLEANED] KG09 BANDAR UTAMA.csv\n",
      "[CLEANED] KG10 TTDI.csv\n",
      "[CLEANED] KG11 PHILEO DAMANSARA.csv\n",
      "[CLEANED] KG13 PUSAT BANDAR DAMANSARA.csv\n",
      "[CLEANED] KG14 SEMANTAN.csv\n",
      "[CLEANED] KG15 MUZIUM NEGARA.csv\n",
      "[CLEANED] KG16 PASAR SENI.csv\n",
      "[CLEANED] KG17 MERDEKA.csv\n",
      "[CLEANED] KG18A BUKIT BINTANG.csv\n",
      "[CLEANED] KG20 TRX.csv\n",
      "[CLEANED] KG21 COCHRANE.csv\n",
      "[CLEANED] KG22 MALURI.csv\n",
      "[CLEANED] KG23 TAMAN PERTAMA.csv\n",
      "[CLEANED] KG24 TAMAN MIDAH.csv\n",
      "[CLEANED] KG25 TAMAN MUTIARA.csv\n",
      "[CLEANED] KG27 TAMAN SUNTEX.csv\n",
      "[CLEANED] KG28 SRI RAYA.csv\n",
      "[CLEANED] KG29 BTHO.csv\n",
      "[CLEANED] KG30 BATU 11.csv\n",
      "[CLEANED] KG31 BUKIT DUKONG.csv\n",
      "[CLEANED] KG33 SUNGAI JERNIH.csv\n",
      "[CLEANED] KG34 STADIUM KAJANG.csv\n",
      "[CLEANED] KG35 KAJANG.csv\n"
     ]
    }
   ],
   "source": [
    "#TO CLEAN ALL THE NULL VALUES IN EACH CSV MRT REVIEWS\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\FYP\\mrt_absa_webapp\\mrt_reviews_csvs\"\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = os.path.join(folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if \"cleaned_reviews\" in df.columns:\n",
    "                df = df.dropna(subset=['cleaned_reviews'])\n",
    "                df = df[df['cleaned_reviews'].str.strip() != \"\"]\n",
    "                df.to_csv(path, index=False)\n",
    "                print(f\"[CLEANED] {file}\")\n",
    "            else:\n",
    "                print(f\"[SKIPPED] {file} missing 'cleaned_reviews' column.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4338e248-c3e9-40c6-9ec6-712643804566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking filenames...\n",
      "\n",
      "[OK] Matched: KG04 KWASA DAMANSARA.csv\n",
      "[OK] Matched: KG06 KOTA DAMANSARA.csv\n",
      "[OK] Matched: KG07 SURIAN.csv\n",
      "[OK] Matched: KG08 MUTIARA DAMANSARA.csv\n",
      "[OK] Matched: KG09 BANDAR UTAMA.csv\n",
      "[OK] Matched: KG10 TTDI.csv\n",
      "[OK] Matched: KG11 PHILEO DAMANSARA.csv\n",
      "[OK] Matched: KG13 PUSAT BANDAR DAMANSARA.csv\n",
      "[OK] Matched: KG14 SEMANTAN.csv\n",
      "[OK] Matched: KG15 MUZIUM NEGARA.csv\n",
      "[OK] Matched: KG16 PASAR SENI.csv\n",
      "[OK] Matched: KG17 MERDEKA.csv\n",
      "[OK] Matched: KG18A BUKIT BINTANG.csv\n",
      "[OK] Matched: KG20 TRX.csv\n",
      "[OK] Matched: KG21 COCHRANE.csv\n",
      "[OK] Matched: KG22 MALURI.csv\n",
      "[OK] Matched: KG23 TAMAN PERTAMA.csv\n",
      "[OK] Matched: KG24 TAMAN MIDAH.csv\n",
      "[OK] Matched: KG25 TAMAN MUTIARA.csv\n",
      "[OK] Matched: KG27 TAMAN SUNTEX.csv\n",
      "[OK] Matched: KG28 SRI RAYA.csv\n",
      "[OK] Matched: KG29 BTHO.csv\n",
      "[OK] Matched: KG30 BATU 11.csv\n",
      "[OK] Matched: KG31 BUKIT DUKONG.csv\n",
      "[OK] Matched: KG33 SUNGAI JERNIH.csv\n",
      "[OK] Matched: KG34 STADIUM KAJANG.csv\n",
      "[OK] Matched: KG35 KAJANG.csv\n"
     ]
    }
   ],
   "source": [
    "#check name \n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Set your paths\n",
    "csv_folder = \"C:/Users/unitf/OneDrive/Desktop/FYP/mrt_absa_webapp/mrt_reviews_csvs\"\n",
    "db_path = \"C:/Users/unitf/OneDrive/Desktop/FYP/mrt_absa_webapp/mrt_reviews.db\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch station names and normalize them\n",
    "cursor.execute(\"SELECT station_name FROM stations\")\n",
    "valid_station_names = set(name.strip().upper() for (name,) in cursor.fetchall())\n",
    "\n",
    "# Check filenames\n",
    "all_files = os.listdir(csv_folder)\n",
    "csv_files = [f for f in all_files if f.endswith(\".csv\")]\n",
    "\n",
    "print(\"Checking filenames...\\n\")\n",
    "\n",
    "for file in csv_files:\n",
    "    station_key = file.replace(\".csv\", \"\").strip().upper()\n",
    "    if station_key in valid_station_names:\n",
    "        print(f\"[OK] Matched: {file}\")\n",
    "    else:\n",
    "        print(f\"[❌] MISMATCH: '{file}' not found in stations table\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0632e025-9da7-4fc2-ac21-cbd062d8e365",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create table\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43mCREATE TABLE IF NOT EXISTS stations (\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;43m    id INTEGER PRIMARY KEY AUTOINCREMENT,\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;43m    station_name TEXT NOT NULL UNIQUE\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Add your stations\u001b[39;00m\n\u001b[0;32m     16\u001b[0m stations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG04 KWASA DAMANSARA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG06 KOTA DAMANSARA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG07 SURIAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG08 MUTIARA DAMANSARA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG09 BANDAR UTAMA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG10 TTDI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG34 STADIUM KAJANG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG35 KAJANG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m ]\n",
      "\u001b[1;31mOperationalError\u001b[0m: database is locked"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to your database\n",
    "conn = sqlite3.connect(\"C:/Users/unitf/OneDrive/Desktop/FYP/mrt_absa_webapp/mrt_reviews.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS stations (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    station_name TEXT NOT NULL UNIQUE\n",
    ")\n",
    "''')\n",
    "\n",
    "# Add your stations\n",
    "stations = [\n",
    "    \"KG04 KWASA DAMANSARA\", \"KG06 KOTA DAMANSARA\", \"KG07 SURIAN\",\n",
    "    \"KG08 MUTIARA DAMANSARA\", \"KG09 BANDAR UTAMA\", \"KG10 TTDI\",\n",
    "    \"KG11 PHILEO DAMANSARA\", \"KG13 PUSAT BANDAR DAMANSARA\", \"KG14 SEMANTAN\",\n",
    "    \"KG15 MUZIUM NEGARA\", \"KG16 PASAR SENI\", \"KG17 MERDEKA\",\n",
    "    \"KG18A BUKIT BINTANG\", \"KG20 TRX\", \"KG21 COCHRANE\", \"KG22 MALURI\",\n",
    "    \"KG23 TAMAN PERTAMA\", \"KG24 TAMAN MIDAH\", \"KG25 TAMAN MUTIARA\",\n",
    "    \"KG27 TAMAN SUNTEX\", \"KG28 SRI RAYA\", \"KG29 BTHO\", \n",
    "    \"KG30 BATU 11 CHERAS\", \"KG31 BUKIT DUKUNG\", \"KG33 SUNGAI JERNIH\",\n",
    "    \"KG34 STADIUM KAJANG\", \"KG35 KAJANG\"\n",
    "]\n",
    "\n",
    "for name in stations:\n",
    "    try:\n",
    "        cursor.execute(\"INSERT OR IGNORE INTO stations (station_name) VALUES (?)\", (name,))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert {name}: {e}\")\n",
    "\n",
    "# Commit & close\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"✅ Station table created and populated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c947f4c-8aef-46d8-a51f-19a4788c4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text_data(text):\n",
    "    \"\"\"\n",
    "    Cleans a single string by converting to lowercase, removing URLs,\n",
    "    removing punctuation, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) # Handle non-string inputs like NaN\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "csv_file_name = \"KG06 KOTA DAMANSARA.csv\" \n",
    "\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "original_reviews_column_name = 'reviews'\n",
    "\n",
    "df['cleaned_reviews'] = df[original_reviews_column_name].apply(clean_text_data)\n",
    "\n",
    "print(\"DataFrame with Cleaned Reviews:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_csv('KG06 KOTA DAMANSARA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38dae06f-cb15-4904-88b1-a0d0dea7a057",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KG06 kota damansara.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m     19\u001b[0m csv_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKG06 kota damansara.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file_name)\n\u001b[0;32m     23\u001b[0m original_reviews_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[original_reviews_column_name]\u001b[38;5;241m.\u001b[39mapply(clean_text_data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG06 kota damansara.csv'"
     ]
    }
   ],
   "source": [
    "#clean data\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text_data(text):\n",
    "    \"\"\"\n",
    "    Cleans a single string by converting to lowercase, removing URLs,\n",
    "    removing punctuation, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) # Handle non-string inputs like NaN\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "csv_file_name = \"KG06 kota damansara.csv\" \n",
    "\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "original_reviews_column_name = 'reviews'\n",
    "\n",
    "df['cleaned_reviews'] = df[original_reviews_column_name].apply(clean_text_data)\n",
    "\n",
    "print(\"DataFrame with Cleaned Reviews:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_csv('KG06 KOTA DAMANSARA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2df32-7296-408d-9284-c92779de392e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
