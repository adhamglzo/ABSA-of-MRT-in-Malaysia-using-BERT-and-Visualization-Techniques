{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a29e53-9831-4128-9cae-0edd6ac07586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\unitf\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.26.4\n",
    "pip install matplotlib\n",
    "pip install scikit-learn\n",
    "pip install transformers\n",
    "pip install torch\n",
    "pip install pandas\n",
    "pip install huggingface_hub[hf_xet]\n",
    "pip install nltk\n",
    "pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e691c19e-d173-4423-8391-a3e8da6b0f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\unitf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\unitf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import nltk; nltk.download('punkt'); nltk.download('punkt_tab')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb05c66-dd7a-4bff-9c8e-430a3a759e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================== DEVICE INFO ===========================\n",
      "GPU detected and in use: NVIDIA GeForce GTX 1650\n",
      "==================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Select training mode:\n",
      "1) ATE Only\n",
      "2) ABSA Only\n",
      "3) Both ATE and ABSA models\n",
      "Enter choice (1, 2, or 3 (or type 'q' to quit)):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model will be saved to project directory as ate_model_v1.pkl\n",
      "\n",
      "Starting to train ATE model...\n",
      "Starting ATE Training...\n",
      "Epoch 1/5...\n",
      "  Batch 10/223 - Loss: 0.6723\n",
      "  Batch 20/223 - Loss: 0.7826\n",
      "  Batch 30/223 - Loss: 0.7012\n",
      "  Batch 40/223 - Loss: 0.8429\n",
      "  Batch 50/223 - Loss: 0.6393\n",
      "  Batch 60/223 - Loss: 0.6803\n",
      "  Batch 70/223 - Loss: 0.6521\n",
      "  Batch 80/223 - Loss: 0.5064\n",
      "  Batch 90/223 - Loss: 0.5495\n",
      "  Batch 100/223 - Loss: 0.3538\n",
      "  Batch 110/223 - Loss: 0.4656\n",
      "  Batch 120/223 - Loss: 0.3510\n",
      "  Batch 130/223 - Loss: 0.1603\n",
      "  Batch 140/223 - Loss: 0.1444\n",
      "  Batch 150/223 - Loss: 0.4826\n",
      "  Batch 160/223 - Loss: 0.2810\n",
      "  Batch 170/223 - Loss: 0.3125\n",
      "  Batch 180/223 - Loss: 0.2373\n",
      "  Batch 190/223 - Loss: 0.3672\n",
      "  Batch 200/223 - Loss: 0.1351\n",
      "  Batch 210/223 - Loss: 0.0837\n",
      "  Batch 220/223 - Loss: 0.1682\n",
      "Epoch 1 completed. Average Loss: 0.4548\n",
      "Epoch 2/5...\n",
      "  Batch 10/223 - Loss: 0.1636\n",
      "  Batch 20/223 - Loss: 0.1204\n",
      "  Batch 30/223 - Loss: 0.1818\n",
      "  Batch 40/223 - Loss: 0.1873\n",
      "  Batch 50/223 - Loss: 0.1730\n",
      "  Batch 60/223 - Loss: 0.1814\n",
      "  Batch 70/223 - Loss: 0.1586\n",
      "  Batch 80/223 - Loss: 0.1891\n",
      "  Batch 90/223 - Loss: 0.2112\n",
      "  Batch 100/223 - Loss: 0.3621\n",
      "  Batch 110/223 - Loss: 0.1556\n",
      "  Batch 120/223 - Loss: 0.1079\n",
      "  Batch 130/223 - Loss: 0.1656\n",
      "  Batch 140/223 - Loss: 0.1335\n",
      "  Batch 150/223 - Loss: 0.1907\n",
      "  Batch 160/223 - Loss: 0.0870\n",
      "  Batch 170/223 - Loss: 0.3124\n",
      "  Batch 180/223 - Loss: 0.0500\n",
      "  Batch 190/223 - Loss: 0.4278\n",
      "  Batch 200/223 - Loss: 0.2214\n",
      "  Batch 210/223 - Loss: 0.1445\n",
      "  Batch 220/223 - Loss: 0.1453\n",
      "Epoch 2 completed. Average Loss: 0.2166\n",
      "Epoch 3/5...\n",
      "  Batch 10/223 - Loss: 0.0881\n",
      "  Batch 20/223 - Loss: 0.1889\n",
      "  Batch 30/223 - Loss: 0.0772\n",
      "  Batch 40/223 - Loss: 0.3437\n",
      "  Batch 50/223 - Loss: 0.0178\n",
      "  Batch 60/223 - Loss: 0.2368\n",
      "  Batch 70/223 - Loss: 0.1390\n",
      "  Batch 80/223 - Loss: 0.0773\n",
      "  Batch 90/223 - Loss: 0.1199\n",
      "  Batch 100/223 - Loss: 0.1607\n",
      "  Batch 110/223 - Loss: 0.0980\n",
      "  Batch 120/223 - Loss: 0.1106\n",
      "  Batch 130/223 - Loss: 0.1866\n",
      "  Batch 140/223 - Loss: 0.1403\n",
      "  Batch 150/223 - Loss: 0.3465\n",
      "  Batch 160/223 - Loss: 0.1071\n",
      "  Batch 170/223 - Loss: 0.1176\n",
      "  Batch 180/223 - Loss: 0.1246\n",
      "  Batch 190/223 - Loss: 0.0951\n",
      "  Batch 200/223 - Loss: 0.1581\n",
      "  Batch 210/223 - Loss: 0.2758\n",
      "  Batch 220/223 - Loss: 0.1394\n",
      "Epoch 3 completed. Average Loss: 0.1604\n",
      "Epoch 4/5...\n",
      "  Batch 10/223 - Loss: 0.1794\n",
      "  Batch 20/223 - Loss: 0.0480\n",
      "  Batch 30/223 - Loss: 0.3320\n",
      "  Batch 40/223 - Loss: 0.1363\n",
      "  Batch 50/223 - Loss: 0.0964\n",
      "  Batch 60/223 - Loss: 0.1167\n",
      "  Batch 70/223 - Loss: 0.1495\n",
      "  Batch 80/223 - Loss: 0.0713\n",
      "  Batch 90/223 - Loss: 0.1510\n",
      "  Batch 100/223 - Loss: 0.1863\n",
      "  Batch 110/223 - Loss: 0.1706\n",
      "  Batch 120/223 - Loss: 0.0869\n",
      "  Batch 130/223 - Loss: 0.1557\n",
      "  Batch 140/223 - Loss: 0.0520\n",
      "  Batch 150/223 - Loss: 0.1193\n",
      "  Batch 160/223 - Loss: 0.0733\n",
      "  Batch 170/223 - Loss: 0.1555\n",
      "  Batch 180/223 - Loss: 0.1047\n",
      "  Batch 190/223 - Loss: 0.0768\n",
      "  Batch 200/223 - Loss: 0.1186\n",
      "  Batch 210/223 - Loss: 0.2321\n",
      "  Batch 220/223 - Loss: 0.0584\n",
      "Epoch 4 completed. Average Loss: 0.1256\n",
      "Epoch 5/5...\n",
      "  Batch 10/223 - Loss: 0.1437\n",
      "  Batch 20/223 - Loss: 0.0490\n",
      "  Batch 30/223 - Loss: 0.0883\n",
      "  Batch 40/223 - Loss: 0.1085\n",
      "  Batch 50/223 - Loss: 0.1360\n",
      "  Batch 60/223 - Loss: 0.0749\n",
      "  Batch 70/223 - Loss: 0.0125\n",
      "  Batch 80/223 - Loss: 0.0320\n",
      "  Batch 90/223 - Loss: 0.1307\n",
      "  Batch 100/223 - Loss: 0.0609\n",
      "  Batch 110/223 - Loss: 0.0684\n",
      "  Batch 120/223 - Loss: 0.2305\n",
      "  Batch 130/223 - Loss: 0.0112\n",
      "  Batch 140/223 - Loss: 0.0190\n",
      "  Batch 150/223 - Loss: 0.1059\n",
      "  Batch 160/223 - Loss: 0.0554\n",
      "  Batch 170/223 - Loss: 0.0636\n",
      "  Batch 180/223 - Loss: 0.0177\n",
      "  Batch 190/223 - Loss: 0.0198\n",
      "  Batch 200/223 - Loss: 0.2005\n",
      "  Batch 210/223 - Loss: 0.0449\n",
      "  Batch 220/223 - Loss: 0.1424\n",
      "Epoch 5 completed. Average Loss: 0.0945\n",
      "ATE Training Completed.\n",
      "\n",
      "Starting to test ATE model...\n",
      "True labels: {0, 1, 2}\n",
      "Predicted labels: {0, 1, 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Aspect       0.97      0.96      0.96      2072\n",
      "      B-Term       0.82      0.83      0.82       468\n",
      "      I-Term       0.81      0.85      0.83       292\n",
      "\n",
      "    accuracy                           0.93      2832\n",
      "   macro avg       0.87      0.88      0.87      2832\n",
      "weighted avg       0.93      0.93      0.93      2832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TRAIN CUBAA\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from data_processing import dataset_ATM, dataset_ABSA\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "import torch\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Initialize device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\n=========================== DEVICE INFO ===========================\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"GPU detected and in use: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU. Consider enabling a GPU for faster performance.\")\n",
    "\n",
    "print(\"==================================================================\\n\")\n",
    "\n",
    "\n",
    "pretrain_model_name = \"bert-base-uncased\"  # Changed to base model\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "\n",
    "# Initialize models\n",
    "# Load model on CPU first, then move to GPU\n",
    "ate_model = bert_ATE.from_pretrained(pretrain_model_name, num_labels=3)\n",
    "absa_model = bert_ABSA.from_pretrained(pretrain_model_name, num_labels=3)\n",
    "\n",
    "ate_model.to(DEVICE)\n",
    "absa_model.to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "lr = 2e-5\n",
    "optimizer_ATE = AdamW(ate_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "optimizer_ABSA = AdamW(absa_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "# Helper functions\n",
    "def evl_time(t):\n",
    "    min, sec = divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "# Save and Load Models as .pkl\n",
    "def save_model_pkl(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model_pkl(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "# ATE: Create mini-batches\n",
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    return ids_tensors, tags_tensors, masks_tensors\n",
    "\n",
    "# ATE: Train Model with Debugging\n",
    "def train_ate(loader, model, optimizer, epochs):\n",
    "    model.train()\n",
    "    print(\"Starting ATE Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            batch_count += 1\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, labels=tags_tensors)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"  Batch {batch_count}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {epoch_loss / len(loader):.4f}\")\n",
    "    print(\"ATE Training Completed.\")\n",
    "\n",
    "# ATE: Test Model with Flattened Output\n",
    "def test_ate(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=2)\n",
    "\n",
    "            # Flatten the batch outputs\n",
    "            for pred, truth in zip(preds, tags_tensors):\n",
    "                predictions.extend(pred.cpu().tolist())\n",
    "                truths.extend(truth.cpu().tolist())\n",
    "\n",
    "    return truths, predictions\n",
    "\n",
    "# ABSA: Create mini-batches\n",
    "def create_mini_batch_absa(samples):\n",
    "    max_len = max([len(s[1]) for s in samples])\n",
    "    ids_tensors = [torch.cat([s[1], torch.zeros(max_len - len(s[1]), dtype=torch.long)]) for s in samples]\n",
    "    ids_tensors = torch.stack(ids_tensors)\n",
    "    segments_tensors = [torch.cat([s[2], torch.zeros(max_len - len(s[2]), dtype=torch.long)]) for s in samples]\n",
    "    segments_tensors = torch.stack(segments_tensors)\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "# ABSA: Train Model with Debugging and Dynamic Batch Adjustment\n",
    "def train_absa(loader, val_loader, model, optimizer, epochs):\n",
    "    print(\"Starting ABSA Training...\")\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            try:\n",
    "                batch_count += 1\n",
    "                ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                    ids_tensors.to(DEVICE),\n",
    "                    segments_tensors.to(DEVICE),\n",
    "                    masks_tensors.to(DEVICE),\n",
    "                    label_ids.to(DEVICE),\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors, labels=label_ids)\n",
    "                loss = outputs[\"loss\"]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Print progress every 10 batches\n",
    "                if batch_count % 10 == 0:\n",
    "                    print(f\"  Batch {batch_count}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(\"  [WARNING] CUDA out of memory. Skipping batch.\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"  Validating...\")\n",
    "        val_loss = validate_absa(val_loader, model)\n",
    "        print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model_pkl(model, \"absa_model_v1.pkl\")\n",
    "            print(\"  Model saved with improved validation loss.\")\n",
    "    print(\"ABSA Training Completed.\")\n",
    "\n",
    "# ABSA: Validate Model\n",
    "def validate_absa(loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                segments_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "                label_ids.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors, labels=label_ids)\n",
    "            loss = outputs[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# ABSA: Test Model\n",
    "def test_absa(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                segments_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "                label_ids.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            truths.extend(label_ids.cpu().tolist())\n",
    "    return truths, predictions\n",
    "\n",
    "# ATE Data\n",
    "ate_train_ds = dataset_ATM(pd.read_csv(\"mrt_train.csv\"), tokenizer)\n",
    "ate_test_ds = dataset_ATM(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "ate_train_loader = DataLoader(ate_train_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=True)\n",
    "ate_test_loader = DataLoader(ate_test_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=False)\n",
    "\n",
    "# ABSA Data\n",
    "absa_train_ds = dataset_ABSA(pd.read_csv(\"mrt_train.csv\"), tokenizer)\n",
    "absa_val_ds = dataset_ABSA(pd.read_csv(\"mrt_val.csv\"), tokenizer)\n",
    "absa_test_ds = dataset_ABSA(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "absa_train_loader = DataLoader(absa_train_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=True)\n",
    "absa_val_loader = DataLoader(absa_val_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=False)\n",
    "absa_test_loader = DataLoader(absa_test_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=False)\n",
    "\n",
    "\n",
    "#PROMPT USER FOR TRAINING MODE\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_mode = input(\n",
    "        \"================================================================\\n\"\n",
    "        \"Select training mode:\\n\"\n",
    "        \"1) ATE Only\\n\"\n",
    "        \"2) ABSA Only\\n\"\n",
    "        \"3) Both ATE and ABSA models\\n\"\n",
    "        \"Enter choice (1, 2, or 3 (or type 'q' to quit)): \"\n",
    "    ).strip()\n",
    "\n",
    "    # TRAIN/TEST ATE IF CHOSEN\n",
    "    if train_mode in [\"1\", \"3\"]:\n",
    "        print(\"\\nModel will be saved to project directory as ate_model_v1.pkl\")\n",
    "        print(\"\\nStarting to train ATE model...\")\n",
    "        train_ate(ate_train_loader, ate_model, optimizer_ATE, epochs=5)\n",
    "        save_model_pkl(ate_model, \"ate_model_v1.pkl\")\n",
    "\n",
    "        print(\"\\nStarting to test ATE model...\")\n",
    "        truths, predictions = test_ate(ate_test_loader, ate_model)\n",
    "        print(\"True labels:\", set(truths))\n",
    "        print(\"Predicted labels:\", set(predictions))\n",
    "        print(classification_report(truths, predictions, target_names=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]))\n",
    "\n",
    "    # TRAIN/TEST ABSA IF CHOSEN\n",
    "    if train_mode in [\"2\", \"3\"]:\n",
    "        print(\"\\nModel will be saved to project directory as absa_model_v1.pkl\")\n",
    "        print(\"\\nStarting to train ABSA model...\")\n",
    "        train_absa(absa_train_loader, absa_val_loader, absa_model, optimizer_ABSA, epochs=8)\n",
    "\n",
    "        print(\"\\nStarting to test ABSA model...\")\n",
    "        truths, predictions = test_absa(absa_test_loader, absa_model)\n",
    "        print(\"True labels:\", set(truths))\n",
    "        print(\"Predicted labels:\", set(predictions))\n",
    "        print(classification_report(\n",
    "        truths,\n",
    "         predictions,\n",
    "    labels=[0, 1, 2],\n",
    "    target_names=[\"Negative\", \"Neutral\", \"Positive\"]\n",
    "))\n",
    "\n",
    "\n",
    "    if train_mode.lower() in [\"q\", \"quit\"]:\n",
    "        print(\"\\nExiting...\\n\")\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d5e651-5cc0-4359-9dfb-800b648313dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================== DEVICE INFO ===========================\n",
      "GPU detected and in use: NVIDIA GeForce GTX 1650\n",
      "==================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Select training mode:\n",
      "1) ATE Only\n",
      "2) ABSA Only\n",
      "3) Both ATE and ABSA models\n",
      "Enter choice (1, 2, or 3 (or type 'q' to quit)):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model will be saved to project directory as absa_model_v1.pkl\n",
      "\n",
      "Starting to train ABSA model...\n",
      "Starting ABSA Training...\n",
      "Epoch 1/8...\n",
      "  Batch 10/117 - Loss: 1.3091\n",
      "  Batch 20/117 - Loss: 1.1544\n",
      "  Batch 30/117 - Loss: 1.0222\n",
      "  Batch 40/117 - Loss: 1.0003\n",
      "  Batch 50/117 - Loss: 1.2221\n",
      "  Batch 60/117 - Loss: 0.6898\n",
      "  Batch 70/117 - Loss: 0.7424\n",
      "  Batch 80/117 - Loss: 1.2912\n",
      "  Batch 90/117 - Loss: 0.7239\n",
      "  Batch 100/117 - Loss: 1.2577\n",
      "  Batch 110/117 - Loss: 0.5313\n",
      "Epoch 1 completed. Average Loss: 0.8890\n",
      "  Validating...\n",
      "  Validation Loss: 0.5952\n",
      "  ✅ Model saved with improved validation loss.\n",
      "Epoch 2/8...\n",
      "  Batch 10/117 - Loss: 0.2176\n",
      "  Batch 20/117 - Loss: 0.1738\n",
      "  Batch 30/117 - Loss: 0.1541\n",
      "  Batch 40/117 - Loss: 0.4173\n",
      "  Batch 50/117 - Loss: 0.1398\n",
      "  Batch 60/117 - Loss: 0.3729\n",
      "  Batch 70/117 - Loss: 0.1921\n",
      "  Batch 80/117 - Loss: 0.1465\n",
      "  Batch 90/117 - Loss: 0.1137\n",
      "  Batch 100/117 - Loss: 0.0622\n",
      "  Batch 110/117 - Loss: 0.5161\n",
      "Epoch 2 completed. Average Loss: 0.2546\n",
      "  Validating...\n",
      "  Validation Loss: 0.2190\n",
      "  ✅ Model saved with improved validation loss.\n",
      "Epoch 3/8...\n",
      "  Batch 10/117 - Loss: 0.0919\n",
      "  Batch 20/117 - Loss: 0.1111\n",
      "  Batch 30/117 - Loss: 0.0697\n",
      "  Batch 40/117 - Loss: 0.1196\n",
      "  Batch 50/117 - Loss: 0.1692\n",
      "  Batch 60/117 - Loss: 0.0262\n",
      "  Batch 70/117 - Loss: 0.0624\n",
      "  Batch 80/117 - Loss: 0.0406\n",
      "  Batch 90/117 - Loss: 0.0148\n",
      "  Batch 100/117 - Loss: 0.0154\n",
      "  Batch 110/117 - Loss: 0.0830\n",
      "Epoch 3 completed. Average Loss: 0.1210\n",
      "  Validating...\n",
      "  Validation Loss: 0.1851\n",
      "  ✅ Model saved with improved validation loss.\n",
      "Epoch 4/8...\n",
      "  Batch 10/117 - Loss: 0.0698\n",
      "  Batch 20/117 - Loss: 0.0140\n",
      "  Batch 30/117 - Loss: 0.0389\n",
      "  Batch 40/117 - Loss: 0.1305\n",
      "  Batch 50/117 - Loss: 0.0440\n",
      "  Batch 60/117 - Loss: 0.0114\n",
      "  Batch 70/117 - Loss: 0.0457\n",
      "  Batch 80/117 - Loss: 0.0978\n",
      "  Batch 90/117 - Loss: 0.0300\n",
      "  Batch 100/117 - Loss: 0.0605\n",
      "  Batch 110/117 - Loss: 0.1414\n",
      "Epoch 4 completed. Average Loss: 0.0648\n",
      "  Validating...\n",
      "  Validation Loss: 0.1768\n",
      "  ✅ Model saved with improved validation loss.\n",
      "Epoch 5/8...\n",
      "  Batch 10/117 - Loss: 0.0396\n",
      "  Batch 20/117 - Loss: 0.5861\n",
      "  Batch 30/117 - Loss: 0.0173\n",
      "  Batch 40/117 - Loss: 0.0100\n",
      "  Batch 50/117 - Loss: 0.0097\n",
      "  Batch 60/117 - Loss: 0.0182\n",
      "  Batch 70/117 - Loss: 0.0140\n",
      "  Batch 80/117 - Loss: 0.0042\n",
      "  Batch 90/117 - Loss: 0.0407\n",
      "  Batch 100/117 - Loss: 0.0283\n",
      "  Batch 110/117 - Loss: 0.0124\n",
      "Epoch 5 completed. Average Loss: 0.0426\n",
      "  Validating...\n",
      "  Validation Loss: 0.1658\n",
      "  ✅ Model saved with improved validation loss.\n",
      "Epoch 6/8...\n",
      "  Batch 10/117 - Loss: 0.0102\n",
      "  Batch 20/117 - Loss: 0.0095\n",
      "  Batch 30/117 - Loss: 0.0147\n",
      "  Batch 40/117 - Loss: 0.0061\n",
      "  Batch 50/117 - Loss: 0.0082\n",
      "  Batch 60/117 - Loss: 0.0100\n",
      "  Batch 70/117 - Loss: 0.0080\n",
      "  Batch 80/117 - Loss: 0.1280\n",
      "  Batch 90/117 - Loss: 0.0077\n",
      "  Batch 100/117 - Loss: 0.0071\n",
      "  Batch 110/117 - Loss: 0.0079\n",
      "Epoch 6 completed. Average Loss: 0.0270\n",
      "  Validating...\n",
      "  Validation Loss: 0.1715\n",
      "Epoch 7/8...\n",
      "  Batch 10/117 - Loss: 0.0069\n",
      "  Batch 20/117 - Loss: 0.0110\n",
      "  Batch 30/117 - Loss: 0.0059\n",
      "  Batch 40/117 - Loss: 0.1997\n",
      "  Batch 50/117 - Loss: 0.0067\n",
      "  Batch 60/117 - Loss: 0.0053\n",
      "  Batch 70/117 - Loss: 0.1258\n",
      "  Batch 80/117 - Loss: 0.0093\n",
      "  Batch 90/117 - Loss: 0.0036\n",
      "  Batch 100/117 - Loss: 0.0055\n",
      "  Batch 110/117 - Loss: 0.0112\n",
      "Epoch 7 completed. Average Loss: 0.0201\n",
      "  Validating...\n",
      "  Validation Loss: 0.1834\n",
      "Epoch 8/8...\n",
      "  Batch 10/117 - Loss: 0.0077\n",
      "  Batch 20/117 - Loss: 0.0091\n",
      "  Batch 30/117 - Loss: 0.0054\n",
      "  Batch 40/117 - Loss: 0.0059\n",
      "  Batch 50/117 - Loss: 0.0037\n",
      "  Batch 60/117 - Loss: 0.0066\n",
      "  Batch 70/117 - Loss: 0.0060\n",
      "  Batch 80/117 - Loss: 0.0059\n",
      "  Batch 90/117 - Loss: 0.0066\n",
      "  Batch 100/117 - Loss: 0.0051\n",
      "  Batch 110/117 - Loss: 0.0087\n",
      "Epoch 8 completed. Average Loss: 0.0188\n",
      "  Validating...\n",
      "  Validation Loss: 0.1840\n",
      "ABSA Training Completed.\n",
      "\n",
      "Starting to test ABSA model...\n",
      "True labels: {0, 1, 2}\n",
      "Predicted labels: {0, 1, 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      0.99      0.99       163\n",
      "     Neutral       0.82      0.91      0.86        45\n",
      "    Positive       0.97      0.95      0.96       194\n",
      "\n",
      "    accuracy                           0.96       402\n",
      "   macro avg       0.93      0.95      0.94       402\n",
      "weighted avg       0.96      0.96      0.96       402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train cuba 3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from data_processing import dataset_ATM, dataset_ABSA\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "import torch\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Initialize device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\n=========================== DEVICE INFO ===========================\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"GPU detected and in use: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU. Consider enabling a GPU for faster performance.\")\n",
    "\n",
    "print(\"==================================================================\\n\")\n",
    "\n",
    "\n",
    "pretrain_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "\n",
    "# Initialize models\n",
    "ate_model = bert_ATE.from_pretrained(pretrain_model_name, num_labels=3)\n",
    "absa_model = bert_ABSA.from_pretrained(pretrain_model_name, num_labels=3)\n",
    "\n",
    "ate_model.to(DEVICE)\n",
    "absa_model.to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "lr = 2e-5\n",
    "optimizer_ATE = AdamW(ate_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "optimizer_ABSA = AdamW(absa_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "# Helper functions\n",
    "def evl_time(t):\n",
    "    min, sec = divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "# Save and Load Models as .pkl\n",
    "def save_model_pkl(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model_pkl(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "# ATE: Create mini-batches (still needed for ATE)\n",
    "def create_mini_batch(samples):\n",
    "    # samples are (bert_tokens, ids_tensor, tags_tensor, pols_tensor) from dataset_ATM\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    # This function returns 3 tensors\n",
    "    return ids_tensors, tags_tensors, masks_tensors\n",
    "\n",
    "# ATE: Train Model with Debugging\n",
    "def train_ate(loader, model, optimizer, epochs):\n",
    "    model.train()\n",
    "    print(\"Starting ATE Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        # FIX: Removed '_' for pols_tensor. Now correctly unpacks 3 items.\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            batch_count += 1\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, labels=tags_tensors)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"  Batch {batch_count}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {epoch_loss / len(loader):.4f}\")\n",
    "    print(\"ATE Training Completed.\")\n",
    "\n",
    "# ATE: Test Model with Flattened Output\n",
    "def test_ate(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        # FIX: Removed '_' for pols_tensor. Now correctly unpacks 3 items.\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=2)\n",
    "\n",
    "            # Flatten the batch outputs\n",
    "            for pred, truth in zip(preds, tags_tensors):\n",
    "                predictions.extend(pred.cpu().tolist())\n",
    "                truths.extend(truth.cpu().tolist())\n",
    "\n",
    "    return truths, predictions\n",
    "\n",
    "# ABSA: Train Model with Debugging and Dynamic Batch Adjustment\n",
    "def train_absa(loader, val_loader, model, optimizer, epochs):\n",
    "    print(\"Starting ABSA Training...\")\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Define class weights: [Negative, Neutral, Positive]\n",
    "    # Ensure these weights are appropriate for your class distribution\n",
    "    class_weights = torch.tensor([4.0, 2.0, 1.0]).to(DEVICE) # Tune as needed\n",
    "    loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # ABSA DataLoader now yields input_ids, token_type_ids, attention_mask, sentiment_tensor\n",
    "        for input_ids, token_type_ids, attention_mask, sentiment_tensor in loader:\n",
    "            try:\n",
    "                batch_count += 1\n",
    "                input_ids, token_type_ids, attention_mask, sentiment_tensor = (\n",
    "                    input_ids.to(DEVICE),\n",
    "                    token_type_ids.to(DEVICE),\n",
    "                    attention_mask.to(DEVICE),\n",
    "                    sentiment_tensor.to(DEVICE),\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids # Pass token_type_ids\n",
    "                )\n",
    "                logits = outputs[\"logits\"]\n",
    "                loss = loss_fct(logits, sentiment_tensor) # Use sentiment_tensor as labels\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if batch_count % 10 == 0:\n",
    "                    print(f\"  Batch {batch_count}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(\"  [WARNING] CUDA out of memory. Skipping batch.\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"  Validating...\")\n",
    "        val_loss = validate_absa(val_loader, model)\n",
    "        print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model_pkl(model, \"absa_model_v1.pkl\")\n",
    "            print(\"  ✅ Model saved with improved validation loss.\")\n",
    "\n",
    "    print(\"ABSA Training Completed.\")\n",
    "\n",
    "\n",
    "# ABSA: Validate Model\n",
    "def validate_absa(loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # ABSA DataLoader now yields input_ids, token_type_ids, attention_mask, sentiment_tensor\n",
    "        for input_ids, token_type_ids, attention_mask, sentiment_tensor in loader:\n",
    "            input_ids, token_type_ids, attention_mask, sentiment_tensor = (\n",
    "                input_ids.to(DEVICE),\n",
    "                token_type_ids.to(DEVICE),\n",
    "                attention_mask.to(DEVICE),\n",
    "                sentiment_tensor.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=sentiment_tensor)\n",
    "            loss = outputs[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# ABSA: Test Model\n",
    "def test_absa(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        # ABSA DataLoader now yields input_ids, token_type_ids, attention_mask, sentiment_tensor\n",
    "        for input_ids, token_type_ids, attention_mask, sentiment_tensor in loader:\n",
    "            input_ids, token_type_ids, attention_mask, sentiment_tensor = (\n",
    "                input_ids.to(DEVICE),\n",
    "                token_type_ids.to(DEVICE),\n",
    "                attention_mask.to(DEVICE),\n",
    "                sentiment_tensor.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            truths.extend(sentiment_tensor.cpu().tolist()) # Use sentiment_tensor for truths\n",
    "    return truths, predictions\n",
    "\n",
    "# ATE Data\n",
    "ate_train_ds = dataset_ATM(pd.read_csv(\"mrt_train.csv\"), tokenizer)\n",
    "ate_test_ds = dataset_ATM(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "ate_train_loader = DataLoader(ate_train_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=True)\n",
    "ate_test_loader = DataLoader(ate_test_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=False)\n",
    "\n",
    "# ABSA Data - NO collate_fn needed here as dataset_ABSA handles it\n",
    "absa_train_ds = dataset_ABSA(pd.read_csv(\"mrt_train.csv\"), tokenizer)\n",
    "absa_val_ds = dataset_ABSA(pd.read_csv(\"mrt_val.csv\"), tokenizer)\n",
    "absa_test_ds = dataset_ABSA(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "absa_train_loader = DataLoader(absa_train_ds, batch_size=16, shuffle=True) # Removed collate_fn\n",
    "absa_val_loader = DataLoader(absa_val_ds, batch_size=16, shuffle=False) # Removed collate_fn\n",
    "absa_test_loader = DataLoader(absa_test_ds, batch_size=16, shuffle=False) # Removed collate_fn\n",
    "\n",
    "\n",
    "#PROMPT USER FOR TRAINING MODE\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_mode = input(\n",
    "        \"================================================================\\n\"\n",
    "        \"Select training mode:\\n\"\n",
    "        \"1) ATE Only\\n\"\n",
    "        \"2) ABSA Only\\n\"\n",
    "        \"3) Both ATE and ABSA models\\n\"\n",
    "        \"Enter choice (1, 2, or 3 (or type 'q' to quit)): \"\n",
    "    ).strip()\n",
    "\n",
    "    # TRAIN/TEST ATE IF CHOSEN\n",
    "    if train_mode in [\"1\", \"3\"]:\n",
    "        print(\"\\nModel will be saved to project directory as ate_model_v1.pkl\")\n",
    "        print(\"\\nStarting to train ATE model...\")\n",
    "        train_ate(ate_train_loader, ate_model, optimizer_ATE, epochs=5)\n",
    "        save_model_pkl(ate_model, \"ate_model_v1.pkl\")\n",
    "\n",
    "        print(\"\\nStarting to test ATE model...\")\n",
    "        truths, predictions = test_ate(ate_test_loader, ate_model)\n",
    "        print(\"True labels:\", set(truths))\n",
    "        print(\"Predicted labels:\", set(predictions))\n",
    "        print(classification_report(truths, predictions, target_names=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]))\n",
    "\n",
    "    # TRAIN/TEST ABSA IF CHOSEN\n",
    "    if train_mode in [\"2\", \"3\"]:\n",
    "        print(\"\\nModel will be saved to project directory as absa_model_v1.pkl\")\n",
    "        print(\"\\nStarting to train ABSA model...\")\n",
    "        train_absa(absa_train_loader, absa_val_loader, absa_model, optimizer_ABSA, epochs=8)\n",
    "\n",
    "        print(\"\\nStarting to test ABSA model...\")\n",
    "        truths, predictions = test_absa(absa_test_loader, absa_model)\n",
    "        print(\"True labels:\", set(truths))\n",
    "        print(\"Predicted labels:\", set(predictions))\n",
    "        print(classification_report(\n",
    "            truths,\n",
    "            predictions,\n",
    "            labels=[0, 1, 2],\n",
    "            target_names=[\"Negative\", \"Neutral\", \"Positive\"]\n",
    "        ))\n",
    "\n",
    "    if train_mode.lower() in [\"q\", \"quit\"]:\n",
    "        print(\"\\nExiting...\\n\")\n",
    "        sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63efd43c-5a1c-4797-8ab0-df2fc7ae52ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================== DEVICE INFO ===========================\n",
      "GPU detected and in use: NVIDIA GeForce GTX 1650\n",
      "==================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 265\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m#PROMPT USER FOR TRAINING MODE\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 265\u001b[0m     train_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m================================================================\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSelect training mode:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1) ATE Only\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2) ABSA Only\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3) Both ATE and ABSA models\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter choice (1, 2, or 3 (or type \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to quit)): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;66;03m# TRAIN/TEST ATE IF CHOSEN\u001b[39;00m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#TRAIN CUBAA 2\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from data_processing import dataset_ATM, dataset_ABSA\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "import torch\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Initialize device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\n=========================== DEVICE INFO ===========================\")\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"GPU detected and in use: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU. Consider enabling a GPU for faster performance.\")\n",
    "\n",
    "print(\"==================================================================\\n\")\n",
    "\n",
    "\n",
    "pretrain_model_name = \"bert-base-uncased\"  # Changed to base model\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "\n",
    "# Initialize models\n",
    "# Load model on CPU first, then move to GPU\n",
    "ate_model = bert_ATE.from_pretrained(pretrain_model_name, num_labels=3)\n",
    "absa_model = bert_ABSA.from_pretrained(pretrain_model_name, num_labels=3)\n",
    "\n",
    "ate_model.to(DEVICE)\n",
    "absa_model.to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "lr = 2e-5\n",
    "optimizer_ATE = AdamW(ate_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "optimizer_ABSA = AdamW(absa_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "# Helper functions\n",
    "def evl_time(t):\n",
    "    min, sec = divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "# Save and Load Models as .pkl\n",
    "def save_model_pkl(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model_pkl(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "# ATE: Create mini-batches\n",
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    return ids_tensors, tags_tensors, masks_tensors\n",
    "\n",
    "# ATE: Train Model with Debugging\n",
    "def train_ate(loader, model, optimizer, epochs):\n",
    "    model.train()\n",
    "    print(\"Starting ATE Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            batch_count += 1\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, labels=tags_tensors)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"  Batch {batch_count}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {epoch_loss / len(loader):.4f}\")\n",
    "    print(\"ATE Training Completed.\")\n",
    "\n",
    "# ATE: Test Model with Flattened Output\n",
    "def test_ate(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=2)\n",
    "\n",
    "            # Flatten the batch outputs\n",
    "            for pred, truth in zip(preds, tags_tensors):\n",
    "                predictions.extend(pred.cpu().tolist())\n",
    "                truths.extend(truth.cpu().tolist())\n",
    "\n",
    "    return truths, predictions\n",
    "\n",
    "# ABSA: Create mini-batches\n",
    "def create_mini_batch_absa(samples):\n",
    "    max_len = max([len(s[1]) for s in samples])\n",
    "    ids_tensors = [torch.cat([s[1], torch.zeros(max_len - len(s[1]), dtype=torch.long)]) for s in samples]\n",
    "    ids_tensors = torch.stack(ids_tensors)\n",
    "    segments_tensors = [torch.cat([s[2], torch.zeros(max_len - len(s[2]), dtype=torch.long)]) for s in samples]\n",
    "    segments_tensors = torch.stack(segments_tensors)\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "# ABSA: Train Model with Debugging and Dynamic Batch Adjustment\n",
    "def train_absa(loader, val_loader, model, optimizer, epochs):\n",
    "    print(\"Starting ABSA Training...\")\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Define class weights: [Negative, Neutral, Positive]\n",
    "    class_weights = torch.tensor([4.0, 2.0, 1.0]).to(DEVICE)  # Tune as needed\n",
    "    loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            try:\n",
    "                batch_count += 1\n",
    "                ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                    ids_tensors.to(DEVICE),\n",
    "                    segments_tensors.to(DEVICE),\n",
    "                    masks_tensors.to(DEVICE),\n",
    "                    label_ids.to(DEVICE),\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=ids_tensors,\n",
    "                    attention_mask=masks_tensors,\n",
    "                    token_type_ids=segments_tensors\n",
    "                )\n",
    "                logits = outputs[\"logits\"]\n",
    "                loss = loss_fct(logits, label_ids)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if batch_count % 10 == 0:\n",
    "                    print(f\"  Batch {batch_count}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(\"  [WARNING] CUDA out of memory. Skipping batch.\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"  Validating...\")\n",
    "        val_loss = validate_absa(val_loader, model)\n",
    "        print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model_pkl(model, \"absa_model_v1.pkl\")\n",
    "            print(\"  ✅ Model saved with improved validation loss.\")\n",
    "\n",
    "    print(\"ABSA Training Completed.\")\n",
    "\n",
    "\n",
    "# ABSA: Validate Model\n",
    "def validate_absa(loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                segments_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "                label_ids.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors, labels=label_ids)\n",
    "            loss = outputs[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# ABSA: Test Model\n",
    "def test_absa(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                segments_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "                label_ids.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            truths.extend(label_ids.cpu().tolist())\n",
    "    return truths, predictions\n",
    "\n",
    "# ATE Data\n",
    "ate_train_ds = dataset_ATM(pd.read_csv(\"mrt_train.csv\"), tokenizer)\n",
    "ate_test_ds = dataset_ATM(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "ate_train_loader = DataLoader(ate_train_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=True)\n",
    "ate_test_loader = DataLoader(ate_test_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=False)\n",
    "\n",
    "# ABSA Data\n",
    "absa_train_ds = dataset_ABSA(pd.read_csv(\"mrt_train.csv\"), tokenizer)\n",
    "absa_val_ds = dataset_ABSA(pd.read_csv(\"mrt_val.csv\"), tokenizer)\n",
    "absa_test_ds = dataset_ABSA(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "absa_train_loader = DataLoader(absa_train_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=True)\n",
    "absa_val_loader = DataLoader(absa_val_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=False)\n",
    "absa_test_loader = DataLoader(absa_test_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=False)\n",
    "\n",
    "\n",
    "#PROMPT USER FOR TRAINING MODE\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_mode = input(\n",
    "        \"================================================================\\n\"\n",
    "        \"Select training mode:\\n\"\n",
    "        \"1) ATE Only\\n\"\n",
    "        \"2) ABSA Only\\n\"\n",
    "        \"3) Both ATE and ABSA models\\n\"\n",
    "        \"Enter choice (1, 2, or 3 (or type 'q' to quit)): \"\n",
    "    ).strip()\n",
    "\n",
    "    # TRAIN/TEST ATE IF CHOSEN\n",
    "    if train_mode in [\"1\", \"3\"]:\n",
    "        print(\"\\nModel will be saved to project directory as ate_model_v1.pkl\")\n",
    "        print(\"\\nStarting to train ATE model...\")\n",
    "        train_ate(ate_train_loader, ate_model, optimizer_ATE, epochs=5)\n",
    "        save_model_pkl(ate_model, \"ate_model_v1.pkl\")\n",
    "\n",
    "        print(\"\\nStarting to test ATE model...\")\n",
    "        truths, predictions = test_ate(ate_test_loader, ate_model)\n",
    "        print(\"True labels:\", set(truths))\n",
    "        print(\"Predicted labels:\", set(predictions))\n",
    "        print(classification_report(truths, predictions, target_names=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]))\n",
    "\n",
    "    # TRAIN/TEST ABSA IF CHOSEN\n",
    "    if train_mode in [\"2\", \"3\"]:\n",
    "        print(\"\\nModel will be saved to project directory as absa_model_v1.pkl\")\n",
    "        print(\"\\nStarting to train ABSA model...\")\n",
    "        train_absa(absa_train_loader, absa_val_loader, absa_model, optimizer_ABSA, epochs=8)\n",
    "\n",
    "        print(\"\\nStarting to test ABSA model...\")\n",
    "        truths, predictions = test_absa(absa_test_loader, absa_model)\n",
    "        print(\"True labels:\", set(truths))\n",
    "        print(\"Predicted labels:\", set(predictions))\n",
    "        print(classification_report(\n",
    "        truths,\n",
    "         predictions,\n",
    "    labels=[0, 1, 2],\n",
    "    target_names=[\"Negative\", \"Neutral\", \"Positive\"]\n",
    "))\n",
    "\n",
    "\n",
    "    if train_mode.lower() in [\"q\", \"quit\"]:\n",
    "        print(\"\\nExiting...\\n\")\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a5c444-6dd7-4d8a-b66a-7d047f43b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "=== ATE Classification Report ===\n",
      "True labels: {0, 1, 2}\n",
      "Predicted labels: {0, 1, 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Aspect       0.97      0.96      0.96      2072\n",
      "      B-Term       0.82      0.83      0.82       468\n",
      "      I-Term       0.81      0.85      0.83       292\n",
      "\n",
      "    accuracy                           0.93      2832\n",
      "   macro avg       0.87      0.88      0.87      2832\n",
      "weighted avg       0.93      0.93      0.93      2832\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ABSA Classification Report ===\n",
      "True labels: {0, 1, 2}\n",
      "Predicted labels: {0, 1, 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      0.97      0.98       148\n",
      "     Neutral       0.79      0.77      0.78        43\n",
      "    Positive       0.94      0.96      0.95       190\n",
      "\n",
      "    accuracy                           0.94       381\n",
      "   macro avg       0.91      0.90      0.90       381\n",
      "weighted avg       0.94      0.94      0.94       381\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAHHCAYAAAChjmJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdU0lEQVR4nO3deVxN+f8H8Net3Nt625REG5Et2WYMg0IkM1nHVkwRxpIlDGNmKGuWsY+RMUaWjG1o7Cb7bjCDGUuUEMLYSqX1nt8fft3vXBX36tbt6PX0OI+H8zmf8znv05XefZZzJIIgCCAiIiISET1dB0BERESkKSYwREREJDpMYIiIiEh0mMAQERGR6DCBISIiItFhAkNERESiwwSGiIiIRIcJDBEREYkOExgiIiISHSYwRKQ1Z8+eRfPmzWFiYgKJRIILFy5otf3Dhw9DIpHg8OHDWm1XzJydnREUFKTrMIhKHRMYov/44YcfIJFI0LRp0yLrSCQSlc3ExAR16tTB9OnTkZGRoVI3OzsbixYtQsOGDSGXy2FhYYG6deti8ODBuHbtWqHtX716FRKJBIaGhnj+/LnG93DhwgX07dsXDg4OkMlksLKygre3N1atWoW8vDyN21NXTk4OevTogadPn2LBggVYu3YtnJycSux6pc3LywsSiQQ1atQo9HhsbKzy38SWLVs0bv/KlSsIDw/HrVu3ihkpUflgoOsAiMqS6OhoODs7448//kB8fDxcXV0LrdeuXTt8/vnnAIC0tDQcO3YMkyZNwsWLF7F582Zlve7du2PPnj3o06cPBg0ahJycHFy7dg07d+5E8+bNUatWrQJtr1u3DnZ2dnj27Bm2bNmCgQMHqh3/Tz/9hCFDhqBSpUro168fatSogRcvXuDAgQMIDg5GcnIyvv76aw2/KupJSEjA7du3sWLFCo1i1kSrVq3w8uVLSKXSEmn/bQwNDREfH48//vgDH374ocqx6OhoGBoaIjMz853avnLlCqZMmQIvLy84OzurfV5cXBz09Pi7KJVDAhEJgiAIN2/eFAAIW7duFWxsbITw8PBC6wEQhg8fXqD8s88+E/T09ISXL18KgiAIf/zxhwBAmDFjRoG6ubm5wuPHjwuUKxQKwdnZWRgzZozQtWtXwcvLS+34T506Jejr6wstWrQQUlNTCxw/e/assGrVKrXb09SRI0cEAMLmzZtL7Bq65OnpKdStW1dwc3MTRo8erXLs5cuXglwuF7p37/7OX4PNmzcLAIRDhw69ta5CoRAyMjI0vgbR+4RpO9H/i46OhqWlJT755BN89tlniI6O1uh8Ozs7SCQSGBi86thMSEgAAHz88ccF6urr68Pa2rpA+YkTJ3Dr1i307t0bvXv3xtGjR3H37l21rj9lyhRIJBJER0fDzMyswPEmTZqozJVIT0/H2LFjlUNNbm5u+O677yC89oJ6iUSCkJAQxMTEoF69epDJZKhbty727t2rrBMUFARPT08AQI8ePSCRSODl5QXg1dBL/t//KygoqEBPw4YNG9C4cWOYmZlBLpfD3d0dixYtUh4vag7M5s2b0bhxYxgZGaFixYro27cv7t27V+B6pqamuHfvHrp06QJTU1PY2Nhg3LhxGg2t9enTBxs3boRCoVCW7dixAxkZGejZs2eB+rdv38awYcPg5uYGIyMjWFtbo0ePHipDRVFRUejRowcAoHXr1sqhqPz7dHZ2xqeffop9+/ahSZMmMDIywvLly5XH8j9XQRDQunVr2NjY4NGjR8r2s7Oz4e7ujurVqyM9PV3teyUqy5jAEP2/6OhodOvWDVKpFH369MGNGzdw9uzZQutmZmbi8ePHePz4MW7fvo3169dj9erV8Pf3VyYw+fM/oqOjkZubq3YM1atXxwcffAA/Pz8YGxvjl19+eet5GRkZOHDgAFq1agVHR8e31hcEAZ06dcKCBQvQoUMHzJ8/H25ubvjyyy8xZsyYAvWPHz+OYcOGoXfv3pgzZw4yMzPRvXt3PHnyBADwxRdfKIemRo4cibVr1+Kbb75R657zxcbGok+fPrC0tMTs2bMxa9YseHl54cSJE288LyoqCj179oS+vj4iIiIwaNAgbN26FS1atCgwhygvLw8+Pj6wtrbGd999B09PT8ybNw8//vij2nH6+/sjOTlZJYlav3492rZtC1tb2wL1z549i5MnT6J3795YvHgxhgwZggMHDsDLy0s5Z6pVq1YYOXIkAODrr7/G2rVrsXbtWtSuXVvZTlxcHPr06YN27dph0aJFaNCgQYFrSSQS/Pzzz8jMzMSQIUOU5WFhYbh8+TJWrVoFExMTte+VqEzTcQ8QUZlw7tw5AYAQGxsrCMKrLvqqVasKo0aNKlAXQKFbly5dhMzMTGU9hUIheHp6CgCESpUqCX369BGWLl0q3L59u9AYsrOzBWtra+Gbb75Rlvn7+wseHh5vjf/ixYsCgELjLUxMTIwAQJg+fbpK+WeffSZIJBIhPj5e5X6lUqlKWf71lixZoiw7dOhQocMnnp6egqenZ4EYAgMDBScnJ+X+qFGjBLlcLuTm5hYZd/418odZsrOzBVtbW6FevXrKoTtBEISdO3cKAITJkyerXA+AMHXqVJU2GzZsKDRu3LjIa/73PurWrSsIgiA0adJECA4OFgRBEJ49eyZIpVJh9erVhX4NChvqOXXqlABAWLNmjbLsTUNITk5OAgBh7969hR4LDAxUKVu+fLkAQFi3bp1w+vRpQV9fv8CwF5HYsQeGCK96PipVqoTWrVsDePWbbK9evbBhw4ZChxc6d+6M2NhYxMbG4rfffsPEiROxd+9e+Pv7K4dgJBIJ9u3bh+nTp8PS0hK//PILhg8fDicnJ/Tq1atA78CePXvw5MkT9OnTR1nWp08fXLx4EZcvX35j/KmpqQBQ6NBRYXbv3g19fX3lb/35xo4dC0EQsGfPHpVyb29vVK9eXblfv359yOVy3Lx5U63rqcPCwgLp6emIjY1V+5xz587h0aNHGDZsGAwNDZXln3zyCWrVqoVdu3YVOOe/PRMA0LJlS43vw9/fH1u3bkV2dja2bNkCfX19dO3atdC6RkZGyr/n5OTgyZMncHV1hYWFBf7880+1r+ni4gIfHx+16g4ePBg+Pj4YMWIE+vXrh+rVq2PmzJlqX4tIDJjAULmXl5eHDRs2oHXr1khMTER8fDzi4+PRtGlTPHz4EAcOHChwTtWqVeHt7Q1vb2906tQJM2fOxPTp07F161bs3LlTWU8mk+Gbb77B1atXcf/+ffzyyy/46KOPsGnTJoSEhKi0uW7dOri4uEAmkyljqF69OoyNjd86H0culwMAXrx4odY93759G/b29gUSnvwhi9u3b6uUFzYsZWlpiWfPnql1PXUMGzYMNWvWhK+vL6pWrYoBAwaozLMpTH6cbm5uBY7VqlWrwH0YGhrCxsZGpexd7qN3795ISUnBnj17EB0djU8//bTI5PHly5eYPHmycq5RxYoVYWNjg+fPnyMlJUXta7q4uGgU48qVK5GRkYEbN24gKipKJZEieh8wgaFy7+DBg0hOTsaGDRtQo0YN5ZY/IVPdybxt27YFABw9erTQ45UrV1ZOzK1RowY2bdqknBuTmpqKHTt2IDExUSWGOnXqICMjA+vXry8wufa/XF1dYWBggL///luTW1ebvr5+oeVviimfRCIptPz1ni1bW1tcuHAB27dvR6dOnXDo0CH4+voiMDBQ84CLUNR9aKpy5crw8vLCvHnzcPToUfj7+xdZd8SIEZgxYwZ69uyJTZs24ffff0dsbCysra1VJgK/jaYJyOHDh5GVlQUAJfbvgkiX+BwYKveio6Nha2uLpUuXFji2detWbNu2DZGRkW/9AZKfjKSlpb2xXoUKFVC/fn3cuHEDjx8/hp2dHbZu3YrMzEwsW7YMFStWVKkfFxeHb7/9FidOnECLFi0KbdPY2Bht2rTBwYMHkZSUBAcHhzfG4OTkhP379+PFixcqPQf5D9fT5gPoLC0tCx2ieb13BACkUin8/Pzg5+cHhUKBYcOGYfny5Zg0aVKhz+TJjzMuLg5t2rRRORYXF1eiD9Lz9/fHwIEDYWFhgY4dOxZZb8uWLQgMDMS8efOUZZmZmQWGEItK9N5FcnIyRowYgfbt20MqlWLcuHHw8fF5rx4sSMQEhsq1ly9fYuvWrejRowc+++yzAsft7e3xyy+/YPv27ejVq9cb29qxYwcAwMPDAwBw48YNyGSyAsMvz58/x6lTp2Bpaakczli3bh2qVatWYH4GAGRlZWHWrFmIjo4uMoEBXq00OXDgAPr164edO3fC1NRU5fj58+fxzz//IDAwEB07dsSPP/6I77//HhMnTlTWWbBgASQSCXx9fd94r5qoXr06du/ejX///Vd5vxcvXsSJEydUEq0nT56oLC3X09ND/fr1AUDZk/C6Jk2awNbWFpGRkRgwYABkMhmAV/OJrl69ismTJ2vtPl732WefISkpCW5ubm98sJ6+vn6BnqolS5YU6IHKXx30Lk9fft2gQYOgUCiwcuVK6Ovro27duggODlY+LZjofcAEhsq17du348WLF+jUqVOhxz/66CPY2NggOjpaJYG5fv061q1bB+DVEubTp09j9erVcHV1Rb9+/QC8+iHt7+8PX19ftGzZElZWVrh37x5Wr16N+/fvY+HChdDX18f9+/dx6NChAhNq88lkMvj4+GDz5s1YvHgxKlSoUGi95s2bY+nSpRg2bBhq1aql8iTew4cPY/v27Zg+fToAwM/PD61bt8Y333yDW7duwcPDA7///jt+++03jB49WmXCbnENGDAA8+fPh4+PD4KDg/Ho0SNERkaibt26ysnHADBw4EA8ffoUbdq0QdWqVXH79m0sWbIEDRo0UFlO/F8VKlTA7Nmz0b9/f3h6eqJPnz54+PAhFi1aBGdnZ4SGhmrtPl5nbm6O8PDwt9b79NNPsXbtWpibm6NOnTo4deoU9u/fX+A5QA0aNIC+vj5mz56NlJQUyGQytGnTptCl2W+yatUq7Nq1C1FRUahatSqAVwlT3759sWzZMgwbNkyj9ojKLJ2ugSLSMT8/P8HQ0FBIT08vsk5QUJBQoUIF5ZNz8dryaX19faFq1arC4MGDhYcPHyrPe/jwoTBr1izB09NTqFy5smBgYCBYWloKbdq0EbZs2aKsN2/ePAGAcODAgSJjiIqKEgAIv/3221vv6fz584K/v79gb28vVKhQQbC0tBTatm0rrF69WsjLy1PWe/HihRAaGqqsV6NGDWHu3LmCQqFQaQ9FPHn49eW7RS2jFgRBWLdunVCtWjVBKpUKDRo0EPbt21dgGfWWLVuE9u3bC7a2toJUKhUcHR2FL774QkhOTi5wjdeXGm/cuFFo2LChIJPJBCsrKyEgIEC4e/euSp3AwEDBxMSkQGxhYWGCOv8V/ncZdVEK+xo8e/ZM6N+/v1CxYkXB1NRU8PHxEa5du1bo8ucVK1YI1apVE/T19VXu08nJSfjkk08KveZ/20lKShLMzc0FPz+/AvW6du0qmJiYCDdv3nzrvRKJgUQQ1JiFR0RERFSGcBUSERERiQ4TGCIiIhIdJjBEREQkOkxgiIiISHSYwBAREZHoMIEhIiIi0eGD7MoghUKB+/fvw8zMjE/NJCISGUEQ8OLFC9jb20NPr+T6CTIzM5Gdna2VtqRSqcob3cWACUwZdP/+/be+y4aIiMq2pKQk5dOQtS0zMxNGZtZAboZW2rOzs0NiYqKokhgmMGVQ/sv1bAJ+hJ5UszfQkvhcmOOn6xCISItevEhFzWqOKi9K1bbs7GwgNwOyOoGAftHv4lJLXjYeXFmN7OxsJjBUPPnDRnpSI+hJjXUcDZU0uVyu6xCIqASUyhQAA0NIipnACBJxTodlAkNERCRWEgDFTZREOtWSCQwREZFYSfRebcVtQ4TEGTURERGVa+yBISIiEiuJRAtDSOIcQ2ICQ0REJFYcQiIiIiISD/bAEBERiRWHkIiIiEh8tDCEJNLBGHFGTUREROUaExgiIiKxyh9CKu6mgaNHj8LPzw/29vaQSCSIiYl5LSRJodvcuXOVdZydnQscnzVrlkZxcAiJiIhIrHSwCik9PR0eHh4YMGAAunXrVuB4cnKyyv6ePXsQHByM7t27q5RPnToVgwYNUu5r+u4oJjBERESkNl9fX/j6+hZ53M7OTmX/t99+Q+vWrVGtWjWVcjMzswJ1NcEhJCIiIrHS4hBSamqqypaVlVXs8B4+fIhdu3YhODi4wLFZs2bB2toaDRs2xNy5c5Gbm6tR2+yBISIiEistDiE5ODioFIeFhSE8PLxYTa9evRpmZmYFhppGjhyJRo0awcrKCidPnsTEiRORnJyM+fPnq902ExgiIiKx0uJzYJKSkiCXy5XFMpmseO0C+PnnnxEQEABDQ0OV8jFjxij/Xr9+fUilUnzxxReIiIhQ+7pMYIiIiAhyuVwlgSmuY8eOIS4uDhs3bnxr3aZNmyI3Nxe3bt2Cm5ubWu0zgSEiIhKrMvwupJUrV6Jx48bw8PB4a90LFy5AT08Ptra2arfPBIaIiEisJBItJDCaDUGlpaUhPj5euZ+YmIgLFy7AysoKjo6OAF5NCN68eTPmzZtX4PxTp07hzJkzaN26NczMzHDq1CmEhoaib9++sLS0VDsOJjBERESktnPnzqF169bK/fz5LIGBgYiKigIAbNiwAYIgoE+fPgXOl8lk2LBhA8LDw5GVlQUXFxeEhoaqzItRBxMYIiIisdKTvNqK24YGvLy8IAjCG+sMHjwYgwcPLvRYo0aNcPr0aY2uWRgmMERERGJVhufAlDRxRk1ERETlGntgiIiIxEqLz4ERGyYwREREYsUhJCIiIiLxYA8MERGRWHEIiYiIiESnHA8hMYEhIiISq3LcAyPOtIuIiIjKNfbAEBERiRWHkIiIiEh0OIREREREJB7sgSEiIhItLQwhibQvgwkMERGRWHEIiYiIiEg82ANDREQkVhKJFlYhibMHhgkMERGRWJXjZdTijJqIiIjKNfbAEBERiVU5nsTLBIaIiEisyvEQEhMYIiIisSrHPTDiTLuIiIioXGMPDBERkVhxCImIiIhEh0NIREREROLBHhgiIiKRkkgkkJTTHhgmMERERCJVnhMYDiERERGR6LAHhoiISKwk/78Vtw0RYgJDREQkUhxCIiIiIhIR9sAQERGJVHnugWECQ0REJFJMYKhIzs7OGD16NEaPHq3rUETpg+rWGOxdE/UcLVDJ3Ahf/HgKsZeSC607vXcD+LeohmlbLmLV4YQCx6UGetg6zgt1qlrgk4gDuHovpaTDJy1aEPU7dh6+iBu3H8JIVgEfuLsgLKQzajhV0nVopGX8rEtPeU5gdDoHJigoCBKJBLNmzVIpj4mJKf4HoqGoqChYWFgUKD979iwGDx5cqrG8T4xlBrh6LwVhGy++sV77+vZo4GyFB89fFllnQud6eJSSqe0QqZSc/CsewZ+1xO8rx+LXxcORm5uHz0YuRfrLLF2HRlrGz5pKg84n8RoaGmL27Nl49uyZrkMplI2NDYyNjXUdhmgdufIQ83dewe+X7hdZp5K5IcJ6eCA06ixy8xSF1vGsUwkta9ti5ra/SypUKmGbFw2D/6cfoVa1yqhXsyq+n9wXdx88w8VrSboOjbSMn3UpkmhpEyGdJzDe3t6ws7NDREREkXWOHz+Oli1bwsjICA4ODhg5ciTS09OVx5OTk/HJJ5/AyMgILi4uWL9+PZydnbFw4UJlnfnz58Pd3R0mJiZwcHDAsGHDkJaWBgA4fPgw+vfvj5SUFGV3XHh4OACotOPv749evXqpxJaTk4OKFStizZo1AACFQoGIiAi4uLjAyMgIHh4e2LJlixa+Uu8niQSY93kTrDhwHTcevCi0TkUzGWb2aYSxa87hZXZeKUdIJSU17VVvmqWcvyC87/hZl5z8n1nF3cRI5wmMvr4+Zs6ciSVLluDu3bsFjickJKBDhw7o3r07Ll26hI0bN+L48eMICQlR1vn8889x//59HD58GL/++it+/PFHPHr0SKUdPT09LF68GJcvX8bq1atx8OBBjB8/HgDQvHlzLFy4EHK5HMnJyUhOTsa4ceMKxBIQEIAdO3YoEx8A2LdvHzIyMtC1a1cAQEREBNasWYPIyEhcvnwZoaGh6Nu3L44cOaKVr9f7Zki7mshTCIgqZM5Lvjl9G2P98Zv4+87z0guMSpRCocA3C35F0/rVULu6va7DoRLEz5pKSpmYxNu1a1c0aNAAYWFhWLlypcqxiIgIBAQEKCfR1qhRA4sXL4anpyeWLVuGW7duYf/+/Th79iyaNGkCAPjpp59Qo0YNlXb+OwnX2dkZ06dPx5AhQ/DDDz9AKpXC3NwcEokEdnZ2Rcbp4+MDExMTbNu2Df369QMArF+/Hp06dYKZmRmysrIwc+ZM7N+/H82aNQMAVKtWDcePH8fy5cvh6elZaLtZWVnIyvrf2HBqaqp6XziRq+dggSAvV/jNPlhknUDP6jA1NMCy3+NKMTIqaV/O3YyrN5Oxa/loXYdCJYyfdcmSSKCFSbzaiaW06bwHJt/s2bOxevVqXL16VaX84sWLiIqKgqmpqXLz8fGBQqFAYmIi4uLiYGBggEaNGinPcXV1haWlpUo7+/fvR9u2bVGlShWYmZmhX79+ePLkCTIyMtSO0cDAAD179kR0dDQAID09Hb/99hsCAgIAAPHx8cjIyEC7du1U4l2zZg0SEoruYYiIiIC5ublyc3BwUDsmMfugujWsTWU4PrUDri/qguuLuqCqtQm+7lYfR6f4AACa1bRBQxdrXFv46vihsPYAgN/Gt8bcfo11GT69o/FzN+H34//gtx9GoEoly7efQKLFz7rkSaCFISQNM5ijR4/Cz88P9vb2kEgkiImJUTmev0Dnv1uHDh1U6jx9+hQBAQGQy+WwsLBAcHCwyuiGOspEDwwAtGrVCj4+Ppg4cSKCgoKU5Wlpafjiiy8wcuTIAuc4Ojri+vXrb2371q1b+PTTTzF06FDMmDEDVlZWOH78OIKDg5Gdna3RJN2AgAB4enri0aNHiI2NhZGRkfKDyf/i79q1C1WqVFE5TyaTFdnmxIkTMWbMGOV+ampquUhitp1Nwom4f1XKooZ/jJg/7mDz6dsAgKlbLmL+zivK47bmhlgT0gIjV/2BC7fK5sRvKpwgCJjw3WbsOnIJ238YCSf7iroOiUoIP+v3W3p6Ojw8PDBgwAB069at0DodOnTAqlWrlPuv/wwMCAhAcnIyYmNjkZOTg/79+2Pw4MFYv3692nGUmQQGAGbNmoUGDRrAzc1NWdaoUSNcuXIFrq6uhZ7j5uaG3Nxc/PXXX2jc+NVv5PHx8Sqrms6fPw+FQoF58+ZBT+9Vp9OmTZtU2pFKpcjLe/sE0ebNm8PBwQEbN27Enj170KNHD1SoUAEAUKdOHchkMty5c6fI4aLCyGSyNyY4YmYs1YeTjaly38HaBLWrmCMlIxv3n73E8/Rslfq5eQr8m5qJxEevksH7z14C+N/S6vSsXADA7X/T37jkmsqeL+duwq/7zmPd3EEwNTHEwyevhkrlJoYwMpTqODrSJn7WpUcXz4Hx9fWFr6/vG+vIZLIip2RcvXoVe/fuVZn6sWTJEnTs2BHfffcd7O3VmytVphIYd3d3BAQEYPHixcqyCRMm4KOPPkJISAgGDhwIExMTXLlyBbGxsfj+++9Rq1YteHt7Y/DgwVi2bBkqVKiAsWPHwsjISPmhurq6IicnB0uWLIGfnx9OnDiByMhIlWs7OzsjLS0NBw4cgIeHB4yNjYvsmfH390dkZCSuX7+OQ4cOKcvNzMwwbtw4hIaGQqFQoEWLFkhJScGJEycgl8sRGBhYAl+1ss3dyRK/jGql3P+2e30AwJbTtzF+3XldhUU6sOrX4wCATkMXq5QvmRQA/08/0kVIVEL4WZeiMvo26sOHD8PW1haWlpZo06YNpk+fDmtrawDAqVOnYGFhoUxegFcrkvX09HDmzBnlopi3KVMJDABMnToVGzduVO7Xr18fR44cwTfffIOWLVtCEARUr15dZTnzmjVrEBwcjFatWimXZF++fBmGhoYAAA8PD8yfPx+zZ8/GxIkT0apVK0RERODzzz9XttG8eXMMGTIEvXr1wpMnTxAWFqZcSv26gIAAzJgxA05OTvj4449Vjk2bNg02NjaIiIjAzZs3YWFhgUaNGuHrr7/W4ldJPM7ceIxqIVvVrt8qbN8bj997mqFRe1R2PDmzRNchUCnhZy1Ory8gedfRgQ4dOqBbt25wcXFBQkICvv76a/j6+uLUqVPQ19fHgwcPYGtrq3KOgYEBrKys8ODBA7WvIxEEQdA4ujLu7t27cHBwUE7cFZvU1FSYm5ujUv+10JPyuQnvu/jF6v22QUTikJqaiso2FkhJSYFcLi+xa5ibm8Oyz8pi/5xQZGfg2S/BBcrf9It8PolEgm3btqFLly5F1rl58yaqV6+u/Jk8c+ZMrF69GnFxqqtLbW1tMWXKFAwdOlStuMtcD8y7OHjwINLS0uDu7o7k5GSMHz8ezs7OaNWq1dtPJiIiEiltzIHJPz8pKUkl4dLW3Mxq1aqhYsWKiI+PR9u2bWFnZ1fgWW25ubl4+vTpGx9l8rr3IoHJycnB119/jZs3b8LMzAzNmzdHdHS0cnItERHR+0ibCYxcLi+RHqO7d+/iyZMnqFy5MgCgWbNmeP78Oc6fP69cfHPw4EEoFAo0bdpU7XbfiwTGx8cHPj4+ug6DiIjovZeWlob4+HjlfmJiIi5cuAArKytYWVlhypQp6N69O+zs7JCQkIDx48fD1dVV+XO6du3a6NChAwYNGoTIyEjk5OQgJCQEvXv3VnsFElCGHmRHREREGtLByxzPnTuHhg0bomHDhgCAMWPGoGHDhpg8eTL09fVx6dIldOrUCTVr1kRwcDAaN26MY8eOqQxJRUdHo1atWmjbti06duyIFi1a4Mcff9QojveiB4aIiKg80uYQkrq8vLzwpvU/+/a9eTUpAFhZWWn00LrCsAeGiIiIRIc9MERERCKlix6YsoIJDBERkUiV5wSGQ0hEREQkOuyBISIiEqny3APDBIaIiEisyujLHEsDh5CIiIhIdNgDQ0REJFIcQiIiIiLRYQJDREREolOeExjOgSEiIiLRYQ8MERGRWJXjVUhMYIiIiESKQ0hEREREIsIeGCIiIpEqzz0wTGCIiIhESgItJDAinQTDISQiIiISHfbAEBERiRSHkIiIiEh8yvEyag4hERERkeiwB4aIiEikOIREREREosMEhoiIiERHInm1FbcNMeIcGCIiIhId9sAQERGJ1KsemOIOIWkpmFLGBIaIiEistDCExGXURERERKWEPTBEREQixVVIREREJDpchUREREQkIuyBISIiEik9PQn09IrXhSIU83xdYQJDREQkUhxCIiIiIhIR9sAQERGJFFchERERkeiU5yEkJjBEREQiVZ57YDgHhoiIiESHPTBEREQiVZ57YJjAEBERiVR5ngPDISQiIiISHSYwREREIiWBRDmM9M4bNOuCOXr0KPz8/GBvbw+JRIKYmBjlsZycHEyYMAHu7u4wMTGBvb09Pv/8c9y/f1+lDWdn5wJxzJo1S6M4mMAQERGJVP4QUnE3TaSnp8PDwwNLly4tcCwjIwN//vknJk2ahD///BNbt25FXFwcOnXqVKDu1KlTkZycrNxGjBihURycA0NERERq8/X1ha+vb6HHzM3NERsbq1L2/fff48MPP8SdO3fg6OioLDczM4Odnd07x8EeGCIiIpEq9vDRf1YxpaamqmxZWVlaiTElJQUSiQQWFhYq5bNmzYK1tTUaNmyIuXPnIjc3V6N22QNDREQkUtpcheTg4KBSHhYWhvDw8GK1nZmZiQkTJqBPnz6Qy+XK8pEjR6JRo0awsrLCyZMnMXHiRCQnJ2P+/Plqt80EhoiIiJCUlKSSZMhksmK1l5OTg549e0IQBCxbtkzl2JgxY5R/r1+/PqRSKb744gtERESofV0mMERERCKlzQfZyeVylQSmOPKTl9u3b+PgwYNvbbdp06bIzc3FrVu34ObmptY1mMAQERGJVFl8kF1+8nLjxg0cOnQI1tbWbz3nwoUL0NPTg62trdrXYQJDREQkUrp4lUBaWhri4+OV+4mJibhw4QKsrKxQuXJlfPbZZ/jzzz+xc+dO5OXl4cGDBwAAKysrSKVSnDp1CmfOnEHr1q1hZmaGU6dOITQ0FH379oWlpaXacTCBISIiIrWdO3cOrVu3Vu7nz2cJDAxEeHg4tm/fDgBo0KCBynmHDh2Cl5cXZDIZNmzYgPDwcGRlZcHFxQWhoaEq82LUwQSmDLs4109r45FUdiU8TNd1CFSK7C0NdR0ClbDMnLzSu5gWhpA0fBAvvLy8IAhCkcffdAwAGjVqhNOnT2t20UIwgSEiIhKp8vw2aj7IjoiIiESHPTBEREQiVRZXIZUWJjBEREQixSEkIiIiIhFhDwwREZFIcQiJiIiIRIdDSEREREQiwh4YIiIikSrPPTBMYIiIiESKc2CIiIhIdMpzDwznwBAREZHosAeGiIhIpDiERERERKLDISQiIiIiEWEPDBERkUhJoIUhJK1EUvqYwBAREYmUnkQCvWJmMMU9X1c4hERERESiwx4YIiIikeIqJCIiIhKd8rwKiQkMERGRSOlJXm3FbUOMOAeGiIiIRIc9MERERGIl0cIQkEh7YJjAEBERiVR5nsTLISQiIiISHfbAEBERiZTk//8Utw0xYgJDREQkUlyFRERERCQi7IEhIiISKT7I7i22b9+udoOdOnV652CIiIhIfeV5FZJaCUyXLl3UakwikSAvL6848RARERG9lVoJjEKhKOk4iIiISEN6Egn0itmFUtzzdaVYc2AyMzNhaGiorViIiIhIA+V5CEnjVUh5eXmYNm0aqlSpAlNTU9y8eRMAMGnSJKxcuVLrARIREVHh8ifxFncTI40TmBkzZiAqKgpz5syBVCpVlterVw8//fSTVoMjIiIiKozGCcyaNWvw448/IiAgAPr6+spyDw8PXLt2TavBERERUdHyh5CKu4mRxnNg7t27B1dX1wLlCoUCOTk5WgmKiIiI3q48T+LVuAemTp06OHbsWIHyLVu2oGHDhloJioiIiOhNNO6BmTx5MgIDA3Hv3j0oFAps3boVcXFxWLNmDXbu3FkSMRIREVEhJP+/FbcNMdK4B6Zz587YsWMH9u/fDxMTE0yePBlXr17Fjh070K5du5KIkYiIiAqhi1VIR48ehZ+fH+zt7SGRSBATE6NyXBAETJ48GZUrV4aRkRG8vb1x48YNlTpPnz5FQEAA5HI5LCwsEBwcjLS0NI3ieKeXObZs2RKxsbF49OgRMjIycPz4cbRv3/5dmiIiIiIRSU9Ph4eHB5YuXVro8Tlz5mDx4sWIjIzEmTNnYGJiAh8fH2RmZirrBAQE4PLly4iNjcXOnTtx9OhRDB48WKM43vlBdufOncPVq1cBvJoX07hx43dtioiIiN6BnuTVVtw2NOHr6wtfX99CjwmCgIULF+Lbb79F586dAbxavVypUiXExMSgd+/euHr1Kvbu3YuzZ8+iSZMmAIAlS5agY8eO+O6772Bvb69e3JqFDdy9exctW7bEhx9+iFGjRmHUqFH44IMP0KJFC9y9e1fT5oiIiOgdlbUH2SUmJuLBgwfw9vZWlpmbm6Np06Y4deoUAODUqVOwsLBQJi8A4O3tDT09PZw5c0bta2mcwAwcOBA5OTm4evUqnj59iqdPn+Lq1atQKBQYOHCgps0RERFRGZCamqqyZWVladzGgwcPAACVKlVSKa9UqZLy2IMHD2Bra6ty3MDAAFZWVso66tA4gTly5AiWLVsGNzc3ZZmbmxuWLFmCo0ePatocERERFYO2HmLn4OAAc3Nz5RYREaG7m1KDxnNgHBwcCn1gXV5entrjVkRERFR82hgCyj8/KSkJcrlcWS6TyTRuy87ODgDw8OFDVK5cWVn+8OFDNGjQQFnn0aNHKufl5ubi6dOnyvPVoXEPzNy5czFixAicO3dOWXbu3DmMGjUK3333nabNERER0TvKn8Rb3A0A5HK5yvYuCYyLiwvs7Oxw4MABZVlqairOnDmDZs2aAQCaNWuG58+f4/z588o6Bw8ehEKhQNOmTdW+llo9MJaWlioZXnp6Opo2bQoDg1en5+bmwsDAAAMGDECXLl3UvjgRERGJS1paGuLj45X7iYmJuHDhAqysrODo6IjRo0dj+vTpqFGjBlxcXDBp0iTY29sr84PatWujQ4cOGDRoECIjI5GTk4OQkBD07t1bo5EctRKYhQsXanRzREREVPK0OYSkrnPnzqF169bK/TFjxgAAAgMDERUVhfHjxyM9PR2DBw/G8+fP0aJFC+zduxeGhobKc6KjoxESEoK2bdtCT08P3bt3x+LFizWLWxAEQaMzqMSlpqbC3NwcDx4/VxmPpPdTwsN0XYdApcje0vDtlUjUUlNT4WJvjZSUlBL7Pzz/50TAypOQGpsWq63sjDREBzcv0XhLwjs/yA4AMjMzkZ2drVImppsnIiIicdI4gUlPT8eECROwadMmPHnypMDxvLw8rQRGREREb6YnkUCvmENIxT1fVzRehTR+/HgcPHgQy5Ytg0wmw08//YQpU6bA3t4ea9asKYkYiYiIqBDFfQbM68+CERONe2B27NiBNWvWwMvLC/3790fLli3h6uoKJycnREdHIyAgoCTiJCIiIlLSuAfm6dOnqFatGoBX812ePn0KAGjRogWfxEtERFSKytq7kEqTxj0w1apVQ2JiIhwdHVGrVi1s2rQJH374IXbs2AELC4sSCJHedyf/jMeSdQdw8dodPHicirVzBuITLw9dh0XFtGX3afy65zSSHz4DAFRzrITg3m3xcZNXryGZ+f1W/HExHo+fpsLIUIb6tR0xItAXzg62b2qWRCItIxNzVuzG3qN/48mzNNStWQVTR3VDg9qOug7tvaKNISCR5i+a98D0798fFy9eBAB89dVXWLp0KQwNDREaGoovv/xS6wGK3eHDhyGRSPD8+XNdh1JmpWdmoV6NKpjzZU9dh0JaZFtRjpDADlizcARWLwhBk/rVMW7GGiTcfggAqOVaBZNHfYZNP4zBkikDIAhAyOSVyMtT6Dhy0oZxszbg2NnrWDypL/avGQ/PD9zQe/QPSP73ua5Do/eExglMaGgoRo4cCeDV66+vXbuG9evX46+//sKoUaO0HmC+oKAgSCQSzJo1S6U8JiZGq91ft27dgkQiwYULF7TWJr1Zu+Z18c3QT/Fpa/a6vE9afVgHHzepBUf7inCqYoNhn/vA2FCKf+LuAAC6dWiKRvWqwb6SFWq5VsHQvu3x8HEKkh8903HkVFwvs7Kx+8glfDPMDx81qA6XqjYYG+wL5yoVsWbbCV2H917JX4VU3E2MivUcGABwcnKCk5OTNmJ5K0NDQ8yePRtffPEFLC0tS+WaRcnOzoZUKtVpDERikZenwIETf+NlZjbcaxUcQniZmY0d+8/BvpIVKlU010GEpE15eQrk5Skgk1ZQKTeUVcDZSzd1FNX7qTwPIamVwGjyeN/83pmS4O3tjfj4eERERGDOnDmF1jl+/DgmTpyIc+fOoWLFiujatSsiIiJgYmIC4NWEp23btqm8s8nCwgILFy5EUFAQXFxcAAANGzYEAHh6euLw4cMICgrC8+fP8cEHH2Dp0qWQyWRITEzE2rVrsWjRIsTFxcHExARt2rTBwoULYWvLcXyi+FsPMODLH5CdnQsjIynmftMP1RwrKY9v3nUKS6L24GVmNpyq2GDptGBUqFDs36tIx0yNDdG4njMWRe1DDedKsLE0Q8z+P3H+8i04V6mo6/DeK7p4lUBZodb/FAsWLFCrMYlEUqIJjL6+PmbOnAl/f3+MHDkSVatWVTmekJCADh06YPr06fj555/x77//IiQkBCEhIVi1apVa1/jjjz/w4YcfYv/+/ahbt65KL8uBAwcgl8sRGxurLMvJycG0adPg5uaGR48eYcyYMQgKCsLu3bvVvq+srCxkZWUp91NTU9U+l6gsc6pSEdGLRiItIxMHTvyD8AWbsTxisDKJ8fVqiKYNa+Dx01Ss23YME2evx09zhhT4zZ3EZ/Gkvhgb8QsadwmDvr4e3GtWRRfvRrgUl6Tr0Og9oVYCk5iYWNJxqK1r165o0KABwsLCsHLlSpVjERERCAgIwOjRowEANWrUwOLFi+Hp6Ylly5apvEiqKDY2NgAAa2tr2NnZqRwzMTHBTz/9pJLUDBgwQPn3atWqYfHixfjggw+QlpYGU1P13k8RERGBKVOmqFWXSEwqVDCAg/2r37hru1bFlRt3sWH7CXwd0g0AYGpiCFMTQzjaV4S7myPa9JmCw6cuw8ezgQ6jJm1wrlIRv34/Ahkvs/AiPROVKppjyOQoONqzB0ab9PAOk1kLaUOMRBn37NmzsXr1aly9elWl/OLFi4iKioKpqaly8/HxgUKh0EoS5u7uXmDey/nz5+Hn5wdHR0eYmZnB09MTAHDnzh212504cSJSUlKUW1ISf0Oh95MgKJCdk1v4MQCCgCKPkzgZG8lQqaI5nqdm4Mgf1+DTop6uQ3qv8DkwItOqVSv4+Phg4sSJCAoKUpanpaXhiy++KHQYy9Hx1cRBiUSC11/AnZOTo9Z18+fR5EtPT4ePjw98fHwQHR0NGxsb3LlzBz4+PgVecvkmMpkMMplM7frvm7SMLCTe/Ve5f/v+E/x9/S4s5caoamelw8ioOL5fvRfNG9eEnY0FMl5mY++RCzj/dyKWTBmAuw+eIPbYJXzUsAYs5aZ4+CQFq7cchqGsAj5uUkvXoZMWHD5zFYIAVHe0xa17jzFt6W+o7lgJvT5pquvQ6D0hygQGAGbNmoUGDRrAzc1NWdaoUSNcuXIFrq6uRZ5nY2OD5ORk5f6NGzeQkZGh3M/vYVHnpZTXrl3DkydPMGvWLDg4OAAAzp07p/G9lHcXrt5Bp6H/myj+7cJtAIA+n3yIpWH9dBUWFdOzlDSEL9iEx09fwNTEEK7OlbFkygA0bVgD/z5JxYXLt7Bh+wmkpr2ElYUpGtZ1wU9zhsLKQr2hVyrbUtMyMWv5TiT/+xwWchN09KyPCYM/QQUDfV2H9l6RSAA9rkISF3d3dwQEBKiskJowYQI++ugjhISEYODAgTAxMcGVK1cQGxuL77//HgDQpk0bfP/992jWrBny8vIwYcIEVKjwvwmDtra2MDIywt69e1G1alUYGhrC3LzwZZ2Ojo6QSqVYsmQJhgwZgn/++QfTpk0r2Rt/D7VoXANP/1ii6zBIyyaN/KzIYzbWciwK71+K0VBp69S2ITq1bajrMN57elpIYIp7vq6Icg5MvqlTp0Kh+N9TO+vXr48jR47g+vXraNmyJRo2bIjJkyfD3t5eWWfevHlwcHBAy5Yt4e/vj3HjxsHY2Fh53MDAAIsXL8by5cthb2+Pzp07F3l9GxsbREVFYfPmzahTpw5mzZqF7777rmRuloiIiJQkwusTQtRw7NgxLF++HAkJCdiyZQuqVKmCtWvXwsXFBS1atCiJOMuV1NRUmJub48Hj55DL5boOh0pYwsN0XYdApcje8u2rIUncUlNT4WJvjZSUlBL7Pzz/58TwDecgMy7esGtWRhqW9m5SovGWBI17YH799Vf4+PjAyMgIf/31l/L5JSkpKZg5c6bWAyQiIqLC5Q8hFXcTI40TmOnTpyMyMhIrVqxQmTvy8ccf488//9RqcERERESF0XgSb1xcHFq1alWg3NzcnG9cJiIiKkXl+V1IGvfA2NnZIT4+vkD58ePHUa1aNa0ERURERG9Xnt9GrXECM2jQIIwaNQpnzpyBRCLB/fv3ER0djXHjxmHo0KElESMREREVQk9LmxhpPIT01VdfQaFQoG3btsjIyECrVq0gk8kwbtw4jBgxoiRiJCIiIlKhcQIjkUjwzTff4Msvv0R8fDzS0tJQp04dtV9cSERERNpRnufAvPOTeKVSKerUqaPNWIiIiEgDeij+HBY9iDOD0TiBad269RvfXHnw4MFiBURERET0NhonMA0aNFDZz8nJwYULF/DPP/8gMDBQW3ERERHRW3AISQMLFiwotDw8PBxpaWnFDoiIiIjUw5c5akHfvn3x888/a6s5IiIioiK98yTe1506dQqGhnxJGRERUWmRSFDsSbzlZgipW7duKvuCICA5ORnnzp3DpEmTtBYYERERvRnnwGjA3NxcZV9PTw9ubm6YOnUq2rdvr7XAiIiIiIqiUQKTl5eH/v37w93dHZaWliUVExEREamBk3jVpK+vj/bt2/Ot00RERGWAREt/xEjjVUj16tXDzZs3SyIWIiIi0kB+D0xxNzHSOIGZPn06xo0bh507dyI5ORmpqakqGxEREVFJU3sOzNSpUzF27Fh07NgRANCpUyeVVwoIggCJRIK8vDztR0lEREQFlOc5MGonMFOmTMGQIUNw6NChkoyHiIiI1CSRSN74fkJ12xAjtRMYQRAAAJ6eniUWDBEREZE6NJoDI9YsjYiI6H2ki0m8zs7Oyp6f/27Dhw8HAHh5eRU4NmTIEK3fu0bPgalZs+Zbk5inT58WKyAiIiJSjy6exHv27FmV+a7//PMP2rVrhx49eijLBg0ahKlTpyr3jY2NixdkITRKYKZMmVLgSbxERERUftjY2Kjsz5o1C9WrV1eZYmJsbAw7O7sSjUOjBKZ3796wtbUtqViIiIhIA3oSSbFf5ph//uuPQpHJZJDJZG88Nzs7G+vWrcOYMWNURmiio6Oxbt062NnZwc/PD5MmTdJ6L4zaCQznvxAREZUt2lxG7eDgoFIeFhaG8PDwN54bExOD58+fIygoSFnm7+8PJycn2Nvb49KlS5gwYQLi4uKwdevW4gX6Go1XIREREdH7JykpCXK5XLn/tt4XAFi5ciV8fX1hb2+vLBs8eLDy7+7u7qhcuTLatm2LhIQEVK9eXWvxqp3AKBQKrV2UiIiItEALk3jzX4Ukl8tVEpi3uX37Nvbv3//WnpWmTZsCAOLj43WTwBAREVHZogcJ9Ir5MsZ3PX/VqlWwtbXFJ5988sZ6Fy5cAABUrlz5na5TFCYwREREIqWLZdTAq1GZVatWITAwEAYG/0slEhISsH79enTs2BHW1ta4dOkSQkND0apVK9SvX794gb6GCQwRERFpZP/+/bhz5w4GDBigUi6VSrF//34sXLgQ6enpcHBwQPfu3fHtt99qPQYmMERERCKlq5c5tm/fvtDFPQ4ODjhy5EjxAlITExgiIiKR0uZzYMRGo3chEREREZUF7IEhIiISKV1N4i0LmMAQERGJlB60MIRUzGXYusIhJCIiIhId9sAQERGJFIeQiIiISHT0UPyhFLEOxYg1biIiIirH2ANDREQkUhKJBJJijgEV93xdYQJDREQkUhKg2GuIxJm+MIEhIiISLT6Jl4iIiEhE2ANDREQkYuLsPyk+JjBEREQiVZ6fA8MhJCIiIhId9sAQERGJFJdRExERkejwSbxEREREIsIeGCIiIpHiEBIRERGJTnl+Ei+HkIiIiEh02ANThmmja5DKvqrWRroOgUpR5eajdB0ClTAhL7vUrsUhJCIiIhKd8rwKiQkMERGRSJXnHhixJl5ERERUjrEHhoiISKTK8yokJjBEREQixZc5EhEREYkIe2CIiIhESg8S6BVzEKi45+sKExgiIiKR4hASERERkYiwB4aIiEikJP//p7htiBETGCIiIpHiEBIRERGRiLAHhoiISKQkWliFxCEkIiIiKlXleQiJCQwREZFIlecEhnNgiIiISHTYA0NERCRS5XkZNXtgiIiIREpPop1NXeHh4ZBIJCpbrVq1lMczMzMxfPhwWFtbw9TUFN27d8fDhw9L4M6ZwBAREZEG6tati+TkZOV2/Phx5bHQ0FDs2LEDmzdvxpEjR3D//n1069atROLgEBIREZFI6WIIycDAAHZ2dgXKU1JSsHLlSqxfvx5t2rQBAKxatQq1a9fG6dOn8dFHHxUrztexB4aIiEik8lchFXfTxI0bN2Bvb49q1aohICAAd+7cAQCcP38eOTk58Pb2VtatVasWHB0dcerUKW3eNgD2wBARERGA1NRUlX2ZTAaZTKZS1rRpU0RFRcHNzQ3JycmYMmUKWrZsiX/++QcPHjyAVCqFhYWFyjmVKlXCgwcPtB4vExgiIiKRkqD4q4jyz3ZwcFApDwsLQ3h4uEqZr6+v8u/169dH06ZN4eTkhE2bNsHIyKhYcWiKCQwREZFIabqKqKg2ACApKQlyuVxZ/nrvS2EsLCxQs2ZNxMfHo127dsjOzsbz589VemEePnxY6JyZ4uIcGCIiIoJcLlfZ1Elg0tLSkJCQgMqVK6Nx48aoUKECDhw4oDweFxeHO3fuoFmzZlqPlz0wREREIlXaq5DGjRsHPz8/ODk54f79+wgLC4O+vj769OkDc3NzBAcHY8yYMbCysoJcLseIESPQrFkzra9AApjAEBERiVZpvwvp7t276NOnD548eQIbGxu0aNECp0+fho2NDQBgwYIF0NPTQ/fu3ZGVlQUfHx/88MMPxQuwCExgiIiIREoCFPtFAJqcv2HDhjceNzQ0xNKlS7F06dLiBaUGzoEhIiIi0WEPDBERkUjpQQK9Yo4h6Yn0ZY5MYIiIiESqtIeQyhIOIREREZHosAeGiIhIrMpxFwwTGCIiIpHSxduoywoOIREREZHosAeGiIhIrLTwIDuRdsAwgSEiIhKrcjwFhkNIREREJD7sgSEiIhKrctwFwwSGiIhIpMrzKiQmMERERCJV2m+jLks4B4aIiIhEhz0wREREIlWOp8AwgSEiIhKtcpzBcAiJiIiIRIc9MERERCLFVUhEREQkOlyFRERERCQi7IEhIiISqXI8h5cJDBERkWiV4wyGQ0hEREQkOuyBISIiEimuQiIiIiLRKc+rkJjAEBERiVQ5ngLDOTBEREQkPuyBoTJhxaYjWLLuAB49SUW9GlUw+8seaFzXWddhkRZ90G0K7j54WqA8qFsLRIzroYOI6F01b1gdI/p5w6OWIyrbmCNg3I/YfeSS8riJkRRhIZ3R0bM+rMxNcPv+E/y48QhWbT2urLNgYm94fugGu4rmSH+ZhT8uJSJ8yW+4cfuhLm5JvMpxF0y57YE5fPgwJBIJnj9//sZ6zs7OWLhwYanEVF5t/f08vl24DRMG+uLw2gmoV6MKuo9Yin+fvtB1aKRFe1aOxcUd05TbxkXDAAB+bRroNjDSmLGRDP9cv4cv52ws9Pj00O5o26wOvpi8Bk17TkfkhsOY82UP+LZyV9a5cC0JIVPXoWnP6eg+YikkEgm2fj8cenoi/WmqIxIt/RGjMp/ABAUFQSKRQCKRQCqVwtXVFVOnTkVubm6x2m3evDmSk5Nhbm4OAIiKioKFhUWBemfPnsXgwYOLdS16sx/WH8TnXZojoFMz1KpWGfMn9oaxoRTrtp/SdWikRRUtTWFrLVdusScuw7lKRTRr6Krr0EhD+09ewYzIndh1+FKhx5vWd8Evu87gxJ83kJT8FKu3ncA/N+6hUR0nZZ3V207g5F8JSEp+iktxdzFj2Q5UtbOCY2Xr0roNErkyn8AAQIcOHZCcnIwbN25g7NixCA8Px9y5c4vVplQqhZ2dHSRvmX5tY2MDY2PjYl2Lipadk4sL15Lg9aGbskxPTw+eH7rh7N+JOoyMSlJ2Ti5+3XcOvT9t+tbvQRKfM5cS4dvKHZVtXv2C2KJxDVR3tMWhM1cLrW9sKIW/30e4de8x7j18Vpqhil7+KqTibmIkigRGJpPBzs4OTk5OGDp0KLy9vbF9+3Y8e/YMn3/+OSwtLWFsbAxfX1/cuHFDed7t27fh5+cHS0tLmJiYoG7duti9ezcA1SGkw4cPo3///khJSVH29oSHhwNQHULy9/dHr169VGLLyclBxYoVsWbNGgCAQqFAREQEXFxcYGRkBA8PD2zZsqXkv0gi9eR5GvLyFLCxMlMpt7GS49GTVB1FRSVt79G/kZr2Er06NtV1KFQCJszdjLibD3Bl9ww8OrUIWxYPw5dzNuHkXwkq9YI/a4mkI/Nw79h8eDevg67Dv0dObp6OohYniZY2MRLlJF4jIyM8efIEQUFBuHHjBrZv3w65XI4JEyagY8eOuHLlCipUqIDhw4cjOzsbR48ehYmJCa5cuQJTU9MC7TVv3hwLFy7E5MmTERcXBwCF1gsICECPHj2QlpamPL5v3z5kZGSga9euAICIiAisW7cOkZGRqFGjBo4ePYq+ffvCxsYGnp6ehd5PVlYWsrKylPupqfzBTe+39TtOo81HtWH3/7+h0/tlcC9PNHF3Rp8xkUhKformDV0xd3xPPHicgiN/xCnrbd5zFofOXINdRTlC+npjVcQAdBg4H1nZxZsiQOWDqBIYQRBw4MAB7Nu3D76+voiJicGJEyfQvHlzAEB0dDQcHBwQExODHj164M6dO+jevTvc3V9NHKtWrVqh7UqlUpibm0MikcDOzq7I6/v4+MDExATbtm1Dv379AADr169Hp06dYGZmhqysLMycORP79+9Hs2bNlNc8fvw4li9fXmQCExERgSlTprzz10XMrC1Moa+vV2DC7r9PU2FrLddRVFSSkpKf4ti5OKycGazrUKgEGMoqYNIwP/T7cgV+P3EZAHA5/j7q1ayKkL5tVRKY1PRMpKZn4mbSvzj79y0kHpyDT7088Ovv53UVvvhwFVLZtnPnTpiamsLQ0BC+vr7o1asXgoKCYGBggKZN/9cFbW1tDTc3N1y9+mqcdeTIkZg+fTo+/vhjhIWF4dKlwiecqcvAwAA9e/ZEdHQ0ACA9PR2//fYbAgICAADx8fHIyMhAu3btYGpqqtzWrFmDhISEItudOHEiUlJSlFtSUlKx4hQTaQUDNKjlgCNn//efmkKhwNGz1/GBu4sOI6OSsnHXGVS0NIN38zq6DoVKQAUDfUgrGEAhCCrlCoUCem+YbPG/xRqi+r1a58rzKiRR/Etp3bo1li1bBqlUCnt7exgYGGD79u1vPW/gwIHw8fHBrl278PvvvyMiIgLz5s3DiBEj3jmWgIAAeHp64tGjR4iNjYWRkRE6dOgAAEhLSwMA7Nq1C1WqVFE5TyaTFdmmTCZ74/H33TD/Nhg2ZS0a1nZEo7rOWPbLIaS/zEKA30e6Do20TKFQYMOuM+jp+wEMDPR1HQ69IxMjKVwcbJT7TvbWqFezCp6nZODuw2c4fv4Gpo7sgpeZOUh68BQfN3JFr44f4tuFW1/Vr2KNbu0a4+Dpq3jyLA32lSwwOrA9MjNzEPv/vTZEbyOKBMbExASurqpLLWvXro3c3FycOXNGOYT05MkTxMXFoU6d//1m5+DggCFDhmDIkCGYOHEiVqxYUWgCI5VKkZf39sljzZs3h4ODAzZu3Ig9e/agR48eqFChAgCgTp06kMlkuHPnTpHDRVRQt/aN8fh5GmYu34VHT17AvWYVbFk8nENI76GjZ6/j3sNn6P0pk1Mxa1DbCTuXj1LuzxzTHQCwfudpDJ+yDsHf/IzJwzvjx2mBsJQbI+nBU0xfthM///rqQXZZWblo1qA6hvT2goXcGP8+fYGTf8XDZ+A8PH6WppN7Eiu+C0mEatSogc6dO2PQoEFYvnw5zMzM8NVXX6FKlSro3LkzAGD06NHw9fVFzZo18ezZMxw6dAi1a9cutD1nZ2ekpaXhwIED8PDwgLGxcZHLp/39/REZGYnr16/j0KFDynIzMzOMGzcOoaGhUCgUaNGiBVJSUnDixAnI5XIEBgZq/wvxnhjc0xODezLpe995Na2F5JOLdB0GFdOJP2/A8oOQIo8/evICIVPXFXn8weMU9By9rCRCK3fK8RQYccyBKcqqVavQuHFjfPrpp2jWrBkEQcDu3buVPSJ5eXkYPnw4ateujQ4dOqBmzZr44YcfCm2refPmGDJkCHr16gUbGxvMmTOnyOsGBATgypUrqFKlCj7++GOVY9OmTcOkSZMQERGhvO6uXbvg4sL5HEREpGXleB21RBBem2lFOpeamgpzc3M8fJICuZzDKO+7zBw+96I8qdx81NsrkagJednI+nsFUlJK7v/w/J8T528kw9SseNdIe5GKxjUql2i8JUHUPTBERETlWWmvQoqIiMAHH3wAMzMz2NraokuXLsrnp+Xz8vJSrirL34YMGaLtW2cCQ0REJFraeI2ABkNIR44cwfDhw3H69GnExsYiJycH7du3R3p6ukq9QYMGITk5Wbm9aVrGuxLtJF4iIiIqXXv37lXZj4qKgq2tLc6fP49WrVopy42Njd/4YFhtYA8MERGRSGlzDm9qaqrK9t9X3BQlJSUFAGBlZaVSHh0djYoVK6JevXqYOHEiMjIyinmnBbEHhoiISKy0uI7awcFBpTgsLEz5YuPCKBQKjB49Gh9//DHq1aunLPf394eTkxPs7e1x6dIlTJgwAXFxcdi6dWsxA1XFBIaIiIiQlJSksgrpbU+IHz58OP755x8cP35cpXzw4MHKv7u7u6Ny5cpo27YtEhISUL16da3FywSGiIhIpLTxLqP88+VyudrLqENCQrBz504cPXoUVatWfWPd/HcWxsfHM4EhIiKi0n+VgCAIGDFiBLZt24bDhw+r9ZDWCxcuAAAqV678jhEWjgkMERERqWX48OFYv349fvvtN5iZmeHBgwcAAHNzcxgZGSEhIQHr169Hx44dYW1tjUuXLiE0NBStWrVC/fr1tRoLExgiIiKRKu13IS1b9uodVl5eXirlq1atQlBQEKRSKfbv34+FCxciPT0dDg4O6N69O7799ttiRlkQExgiIiKxKuUM5m1vH3JwcMCRI0eKGZB6mMAQERGJlDYn8YoNH2RHREREosMeGCIiIpGSQAurkLQSSeljAkNERCRSpT2JtyzhEBIRERGJDntgiIiIRKq0H2RXljCBISIiEq3yO4jEISQiIiISHfbAEBERiRSHkIiIiEh0yu8AEoeQiIiISITYA0NERCRSHEIiIiIi0SnP70JiAkNERCRW5XgSDOfAEBERkeiwB4aIiEikynEHDBMYIiIisSrPk3g5hERERESiwx4YIiIikeIqJCIiIhKfcjwJhkNIREREJDrsgSEiIhKpctwBwwSGiIhIrLgKiYiIiEhE2ANDREQkWsVfhSTWQSQmMERERCLFISQiIiIiEWECQ0RERKLDISQiIiKRKs9DSExgiIiIRKo8v0qAQ0hEREQkOuyBISIiEikOIREREZHolOdXCXAIiYiIiESHPTBERERiVY67YJjAEBERiRRXIRERERGJCHtgiIiIRIqrkIiIiEh0yvEUGA4hERERiZZES5uGli5dCmdnZxgaGqJp06b4448/in0rmmICQ0RERGrbuHEjxowZg7CwMPz555/w8PCAj48PHj16VKpxMIEhIiISKYmW/mhi/vz5GDRoEPr37486deogMjISxsbG+Pnnn0voLgvHBIaIiEik8ifxFndTV3Z2Ns6fPw9vb29lmZ6eHry9vXHq1KkSuMOicRJvGSQIAgDgRWqqjiOh0pCZk6frEKgUCXnZug6BSlj+Z5z/f3lJStXCz4n8Nl5vSyaTQSaTqZQ9fvwYeXl5qFSpkkp5pUqVcO3atWLHogkmMGXQixcvAACuLg46joSIiN7VixcvYG5uXiJtS6VS2NnZoYaWfk6YmprCwUG1rbCwMISHh2ul/ZLABKYMsre3R1JSEszMzCAR6wJ9DaWmpsLBwQFJSUmQy+W6DodKGD/v8qM8ftaCIODFixewt7cvsWsYGhoiMTER2dna6dETBKHAz5vXe18AoGLFitDX18fDhw9Vyh8+fAg7OzutxKIuJjBlkJ6eHqpWrarrMHRCLpeXm//kiJ93eVLePuuS6nn5L0NDQxgaGpb4df5LKpWicePGOHDgALp06QIAUCgUOHDgAEJCQko1FiYwREREpLYxY8YgMDAQTZo0wYcffoiFCxciPT0d/fv3L9U4mMAQERGR2nr16oV///0XkydPxoMHD9CgQQPs3bu3wMTeksYEhsoEmUyGsLCwQsdc6f3Dz7v84Gf9fgoJCSn1IaPXSYTSWOdFREREpEV8kB0RERGJDhMYIiIiEh0mMERERCQ6TGBItJydnbFw4UJdh0FlyOHDhyGRSPD8+XNdh1Kuqfs58HuYioMJDBUqKCgIEokEs2bNUimPiYkp9acDR0VFwcLCokD52bNnMXjw4FKNpbworc//1q1bkEgkuHDhgtbaJPXlf84SiQRSqRSurq6YOnUqcnNzi9Vu8+bNkZycrHyYG7+HqSQwgaEiGRoaYvbs2Xj27JmuQymUjY0NjI2NdR3Ge6ssff7aelw6FdShQwckJyfjxo0bGDt2LMLDwzF37txitZn/np63Jbv8HqbiYAJDRfL29oadnR0iIiKKrHP8+HG0bNkSRkZGcHBwwMiRI5Genq48npycjE8++QRGRkZwcXHB+vXrC3Qbz58/H+7u7jAxMYGDgwOGDRuGtLQ0AK+6ovv374+UlBTlb4r5Lxf7bzv+/v7o1auXSmw5OTmoWLEi1qxZA+DV464jIiLg4uICIyMjeHh4YMuWLVr4Sr2ftPH5SyQSxMTEqJxjYWGBqKgoAICLiwsAoGHDhpBIJPDy8gLwqmegS5cumDFjBuzt7eHm5gYAWLt2LZo0aQIzMzPY2dnB398fjx490t5Nl0MymQx2dnZwcnLC0KFD4e3tje3bt+PZs2f4/PPPYWlpCWNjY/j6+uLGjRvK827fvg0/Pz9YWlrCxMQEdevWxe7duwGoDiHxe5hKChMYKpK+vj5mzpyJJUuW4O7duwWOJyQkoEOHDujevTsuXbqEjRs34vjx4yoPN/r8889x//59HD58GL/++it+/PHHAj9w9PT0sHjxYly+fBmrV6/GwYMHMX78eACvuqIXLlwIuVyO5ORkJCcnY9y4cQViCQgIwI4dO5SJDwDs27cPGRkZ6Nq1KwAgIiICa9asQWRkJC5fvozQ0FD07dsXR44c0crX632jjc//bf744w8AwP79+5GcnIytW7cqjx04cABxcXGIjY3Fzp07Abz6gTZt2jRcvHgRMTExuHXrFoKCgop3o6TCyMgI2dnZCAoKwrlz57B9+3acOnUKgiCgY8eOyMnJAQAMHz4cWVlZOHr0KP7++2/Mnj0bpqamBdrj9zCVGIGoEIGBgULnzp0FQRCEjz76SBgwYIAgCIKwbds2If+fTXBwsDB48GCV844dOybo6ekJL1++FK5evSoAEM6ePas8fuPGDQGAsGDBgiKvvXnzZsHa2lq5v2rVKsHc3LxAPScnJ2U7OTk5QsWKFYU1a9Yoj/fp00fo1auXIAiCkJmZKRgbGwsnT55UaSM4OFjo06fPm78Y5ZA2Pn9BEAQAwrZt21TqmJubC6tWrRIEQRASExMFAMJff/1V4PqVKlUSsrKy3hjn2bNnBQDCixcvBEEQhEOHDgkAhGfPnml4x+XTfz9nhUIhxMbGCjKZTOjSpYsAQDhx4oSy7uPHjwUjIyNh06ZNgiAIgru7uxAeHl5ou69/DvweppLAVwnQW82ePRtt2rQp8FvTxYsXcenSJURHRyvLBEGAQqFAYmIirl+/DgMDAzRq1Eh53NXVFZaWlirt7N+/HxEREbh27RpSU1ORm5uLzMxMZGRkqD0+bmBggJ49eyI6Ohr9+vVDeno6fvvtN2zYsAEAEB8fj4yMDLRr107lvOzsbDRs2FCjr0d5866ff+3atYt1XXd3d0ilUpWy8+fPIzw8HBcvXsSzZ8+gUCgAAHfu3EGdOnWKdb3yaufOnTA1NUVOTg4UCgX8/f3RrVs37Ny5E02bNlXWs7a2hpubG65evQoAGDlyJIYOHYrff/8d3t7e6N69O+rXr//OcfB7mDTFBIbeqlWrVvDx8cHEiRNVuuvT0tLwxRdfYOTIkQXOcXR0xPXr19/a9q1bt/Dpp59i6NChmDFjBqysrHD8+HEEBwcjOztbowl+AQEB8PT0xKNHjxAbGwsjIyN06NBBGSsA7Nq1C1WqVFE5j+9oebN3/fyBV3NghNfeVpI/BPE2JiYmKvvp6enw8fGBj48PoqOjYWNjgzt37sDHx4eTfIuhdevWWLZsGaRSKezt7WFgYIDt27e/9byBAwfCx8cHu3btwu+//46IiAjMmzcPI0aMeOdY+D1MmmACQ2qZNWsWGjRooJxMCQCNGjXClStX4OrqWug5bm5uyM3NxV9//YXGjRsDePVb1H9XtZw/fx4KhQLz5s2Dnt6rKVmbNm1SaUcqlSIvL++tMTZv3hwODg7YuHEj9uzZgx49eqBChQoAgDp16kAmk+HOnTvw9PTU7ObpnT5/4NUqk+TkZOX+jRs3kJGRodzP72FR5/O9du0anjx5glmzZsHBwQEAcO7cOY3vhVSZmJgU+Axr166N3NxcnDlzBs2bNwcAPHnyBHFxcSo9XQ4ODhgyZAiGDBmCiRMnYsWKFYUmMPweppLABIbU4u7ujoCAACxevFhZNmHCBHz00UcICQnBwIEDYWJigitXriA2Nhbff/89atWqBW9vbwwePBjLli1DhQoVMHbsWBgZGSmXV7q6uiInJwdLliyBn58fTpw4gcjISJVrOzs7Iy0tDQcOHICHhweMjY2L7Jnx9/dHZGQkrl+/jkOHDinLzczMMG7cOISGhkKhUKBFixZISUnBiRMnIJfLERgYWAJftffHu3z+ANCmTRt8//33aNasGfLy8jBhwgTlDyQAsLW1hZGREfbu3YuqVavC0NBQ+eyQ1zk6OkIqlWLJkiUYMmQI/vnnH0ybNq1kb7ycqlGjBjp37oxBgwZh+fLlMDMzw1dffYUqVaqgc+fOAIDRo0fD19cXNWvWxLNnz3Do0KEihw35PUwlQsdzcKiM+u/kvnyJiYmCVCoV/vvP5o8//hDatWsnmJqaCiYmJkL9+vWFGTNmKI/fv39f8PX1FWQymeDk5CSsX79esLW1FSIjI5V15s+fL1SuXFkwMjISfHx8hDVr1hSYiDlkyBDB2tpaACCEhYUJgqA6ATDflStXBACCk5OToFAoVI4pFAph4cKFgpubm1ChQgXBxsZG8PHxEY4cOVK8L9Z7SFuf/71794T27dsLJiYmQo0aNYTdu3erTOIVBEFYsWKF4ODgIOjp6Qmenp5FXl8QBGH9+vWCs7OzIJPJhGbNmgnbt29XmQTMSbyaKerrLAiC8PTpU6Ffv36Cubm58nvz+vXryuMhISFC9erVBZlMJtjY2Aj9+vUTHj9+LAhC4Z8Dv4dJ2ySC8NoANVEJunv3LhwcHLB//360bdtW1+EQEZFIMYGhEnXw4EGkpaXB3d0dycnJGD9+PO7du4fr16+rDCUQERFpgnNgqETl5OTg66+/xs2bN2FmZobmzZsjOjqayQsRERULe2CIiIhIdPgqASIiIhIdJjBEREQkOkxgiIiISHSYwBAREZHoMIEhokIFBQWhS5cuyn0vLy+MHj261OM4fPgwJBIJnj9/XmQdiUSCmJgYtdsMDw9HgwYNihXXrVu3IJFIcOHChWK1Q0TvhgkMkYgEBQVBIpFAIpFAKpXC1dUVU6dORW5ubolfe+vWrWo/ul+dpIOIqDj4HBgikenQoQNWrVqFrKws7N69G8OHD0eFChUwceLEAnWzs7OVL0wsLisrK620Q0SkDeyBIRIZmUwGOzs7ODk5YejQofD29sb27dsB/G/YZ8aMGbC3t1e+PTopKQk9e/aEhYUFrKys0LlzZ9y6dUvZZl5eHsaMGQMLCwtYW1tj/PjxeP0RUa8PIWVlZWHChAlwcHCATCaDq6srVq5ciVu3bqF169YAAEtLS0gkEgQFBQEAFAoFIiIi4OLiAiMjI3h4eGDLli0q19m9ezdq1qwJIyMjtG7dWiVOdU2YMAE1a9aEsbExqlWrhkmTJiEnJ6dAveXLl8PBwQHGxsbo2bMnUlJSVI7/9NNPqF27NgwNDVGrVi388MMPGsdCRCWDCQyRyBkZGSE7O1u5f+DAAcTFxSE2NhY7d+5ETk4OfHx8YGZmhmPHjuHEiRMwNTVFhw4dlOfNmzcPUVFR+Pnnn3H8+HE8ffoU27Zte+N1P//8c/zyyy9YvHgxrl69iuXLl8PU1BQODg749ddfAQBxcXFITk7GokWLAAARERFYs2YNIiMjcfnyZYSGhqJv3744cuQIgFeJVrdu3eDn54cLFy5g4MCB+OqrrzT+mpiZmSEqKgpXrlzBokWLsGLFCixYsEClTnx8PDZt2oQdO3Zg7969+OuvvzBs2DDl8ejoaEyePBkzZszA1atXMXPmTEyaNAmrV6/WOB4iKgE6fJEkEWnov28PVigUQmxsrCCTyYRx48Ypj1eqVEnIyspSnrN27VrBzc1N5c2+WVlZgpGRkbBv3z5BEAShcuXKwpw5c5THc3JyhKpVq6q8qdjT01MYNWqUIAiCEBcXJwAQYmNjC42zsLcRZ2ZmCsbGxsLJkydV6gYHBwt9+vQRBEEQJk6cKNSpU0fl+IQJE976hmkAwrZt24o8PnfuXKFx48bK/bCwMEFfX1+4e/eusmzPnj2Cnp6ekJycLAiCIFSvXl1Yv369SjvTpk0TmjVrJgjCq7dz4z9vwiai0sU5MEQis3PnTpiamiInJwcKhQL+/v4IDw9XHnd3d1eZ93Lx4kXEx8fDzMxMpZ3MzEwkJCQgJSUFycnJaNq0qfKYgYEBmjRpUmAYKd+FCxegr68PT09PteOOj49HRkYG2rVrp1KenZ2Nhg0bAgCuXr2qEgcANGvWTO1r5Nu4cSMWL16MhIQEpKWlITc3F3K5XKWOo6MjqlSponIdhUKBuLg4mJmZISEhAcHBwRg0aJCyTm5uLszNzTWOh4i0jwkMkci0bt0ay5Ytg1Qqhb29PQwMVL+NTUxMVPbT0tLQuHFjREdHF2jLxsbmnWIwMjLS+Jy0tDQAwK5du1QSB+DVvB5tOXXqFAICAjBlyhT4+PjA3NwcGzZswLx58zSOdcWKFQUSKn19fa3FSkTvjgkMkciYmJjA1dVV7fqNGjXCxo0bYWtrW6AXIl/lypVx5swZtGrVCsCrnobz58+jUaNGhdZ3d3eHQqHAkSNH4O3tXeB4fg9QXl6esqxOnTqQyWS4c+dOkT03tWvXVk5Iznf69Om33+R/nDx5Ek5OTvjmm2+UZbdv3y5Q786dO7h//z7s7e2V19HT04ObmxsqVaoEe3t73Lx5EwEBARpdn4hKByfxEr3nAgICULFiRXTu3BnHjh1DYmIiDh8+jJEjR+Lu3bsAgFGjRmHWrFmIiYnBtWvXMGzYsDc+w8XZ2RmBgYEYMGAAYmJilG1u2rQJAODk5ASJRIKdO3fi33//RVpaGszMzDBu3DiEhoZi9erVSEhIwJ9//oklS5YoJ8YOGTIEN27cwJdffom4uDisX78eUVFRGt1vjRo1cOfOHWzYsAEJCQlYvHhxoROSDQ0NERgYiIsXL+LYsWMYOXIkevbsCTs7OwDAlClTEBERgcWLF+P69ev4+++/sWrVKsyfP1+jeIioZDCBIXrPGRsb4+jRo3B0dES3bt1Qu3ZtBAcHIzMzU9kjM3bsWPTr1w+BgYFo1qwZzMzM0LVr1ze2u2zZMnz22WcYNmwYatWqhUGDBiE9PR0AUKVKFUyZMgVfffUVKlWqhJCQEADAtGnTMGnSJERERKB27dro0KEDdu3aBRcXFwCv5qX8+uuviImJgYeHByIjIzFz5kyN7rdTp04IDQ1FSEgIGjRogJMnT2LSpEkF6rm6uqJbt27o2LEj2rdvj/r166sskx44cCB++uknrFq1Cu7u7vD09ERUVJQyViLSLYlQ1Cw9IiIiojKKPTBEREQkOkxgiIiISHSYwBAREZHoMIEhIiIi0WECQ0RERKLDBIaIiIhEhwkMERERiQ4TGCIiIhIdJjBEREQkOkxgiIiISHSYwBAREZHoMIEhIiIi0fk/KoEzeT34VxQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"# test.py\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "from data_processing import dataset_ATM, dataset_ABSA\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "def load_model_pkl(model, path):\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "    return model\n",
    "\n",
    "def test_ate(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=2)\n",
    "            for pred, truth in zip(preds, tags_tensors):\n",
    "                predictions.extend(pred.cpu().tolist())\n",
    "                truths.extend(truth.cpu().tolist())\n",
    "    return truths, predictions\n",
    "\n",
    "def test_absa(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                segments_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "                label_ids.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            truths.extend(label_ids.cpu().tolist())\n",
    "    return truths, predictions\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    return ids_tensors, tags_tensors, masks_tensors\n",
    "\n",
    "def create_mini_batch_absa(samples):\n",
    "    max_len = max([len(s[1]) for s in samples])\n",
    "    ids_tensors = [torch.cat([s[1], torch.zeros(max_len - len(s[1]), dtype=torch.long)]) for s in samples]\n",
    "    ids_tensors = torch.stack(ids_tensors)\n",
    "    segments_tensors = [torch.cat([s[2], torch.zeros(max_len - len(s[2]), dtype=torch.long)]) for s in samples]\n",
    "    segments_tensors = torch.stack(segments_tensors)\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "# Load data and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "ate_test_ds = dataset_ATM(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "absa_test_ds = dataset_ABSA(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "\n",
    "ate_test_loader = DataLoader(ate_test_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=False)\n",
    "absa_test_loader = DataLoader(absa_test_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=False)\n",
    "    \n",
    "# Load model architectures\n",
    "ate_model = bert_ATE.from_pretrained(\"bert-base-uncased\", num_labels=3).to(DEVICE)\n",
    "absa_model = bert_ABSA.from_pretrained(\"bert-base-uncased\", num_labels=3).to(DEVICE)\n",
    "\n",
    "# Load saved weights\n",
    "ate_model = load_model_pkl(ate_model, \"ate_model_v1.pkl\")\n",
    "absa_model = load_model_pkl(absa_model, \"absa_model_v1.pkl\")\n",
    "\n",
    "# ATE Testing\n",
    "print(\"\\n=== ATE Classification Report ===\")\n",
    "truths_ate, preds_ate = test_ate(ate_test_loader, ate_model)\n",
    "print(\"True labels:\", set(truths_ate))\n",
    "print(\"Predicted labels:\", set(preds_ate))\n",
    "print(classification_report(truths_ate, preds_ate, target_names=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]))\n",
    "\n",
    "cm_ate = confusion_matrix(truths_ate, preds_ate, labels=[0, 1, 2])\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_ate, display_labels=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]).plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"ATE Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ABSA Testing\n",
    "print(\"\\n=== ABSA Classification Report ===\")\n",
    "truths_absa, preds_absa = test_absa(absa_test_loader, absa_model)\n",
    "print(\"True labels:\", set(truths_absa))\n",
    "print(\"Predicted labels:\", set(preds_absa))\n",
    "print(classification_report(truths_absa, preds_absa, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm_absa = confusion_matrix(truths_absa, preds_absa, labels=[0, 1, 2])\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_absa, display_labels=[\"Negative\", \"Neutral\", \"Positive\"]).plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"ABSA Confusion Matrix\")\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# test.py\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "from data_processing import dataset_ATM, dataset_ABSA\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "def load_model_pkl(model, path):\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "    return model\n",
    "\n",
    "def test_ate(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, tags_tensors, masks_tensors in loader:\n",
    "            ids_tensors, tags_tensors, masks_tensors = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                tags_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=2)\n",
    "            for pred, truth in zip(preds, tags_tensors):\n",
    "                predictions.extend(pred.cpu().tolist())\n",
    "                truths.extend(truth.cpu().tolist())\n",
    "    return truths, predictions\n",
    "\n",
    "def test_absa(loader, model):\n",
    "    model.eval()\n",
    "    predictions, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_tensors, segments_tensors, masks_tensors, label_ids in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = (\n",
    "                ids_tensors.to(DEVICE),\n",
    "                segments_tensors.to(DEVICE),\n",
    "                masks_tensors.to(DEVICE),\n",
    "                label_ids.to(DEVICE),\n",
    "            )\n",
    "            outputs = model(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors)\n",
    "            logits = outputs[\"logits\"]\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            truths.extend(label_ids.cpu().tolist())\n",
    "    return truths, predictions\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    return ids_tensors, tags_tensors, masks_tensors\n",
    "\n",
    "def create_mini_batch_absa(samples):\n",
    "    max_len = max([len(s[1]) for s in samples])\n",
    "    ids_tensors = [torch.cat([s[1], torch.zeros(max_len - len(s[1]), dtype=torch.long)]) for s in samples]\n",
    "    ids_tensors = torch.stack(ids_tensors)\n",
    "    segments_tensors = [torch.cat([s[2], torch.zeros(max_len - len(s[2]), dtype=torch.long)]) for s in samples]\n",
    "    segments_tensors = torch.stack(segments_tensors)\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "# Load data and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "ate_test_ds = dataset_ATM(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "absa_test_ds = dataset_ABSA(pd.read_csv(\"mrt_test.csv\"), tokenizer)\n",
    "\n",
    "ate_test_loader = DataLoader(ate_test_ds, batch_size=8, collate_fn=create_mini_batch, shuffle=False)\n",
    "absa_test_loader = DataLoader(absa_test_ds, batch_size=16, collate_fn=create_mini_batch_absa, shuffle=False)\n",
    "\n",
    "# Load model architectures and weights from pkl without loading base encoder twice\n",
    "ate_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "ate_model = bert_ATE(ate_config).to(DEVICE)\n",
    "ate_model = load_model_pkl(ate_model, \"ate_model_v1.pkl\")\n",
    "\n",
    "absa_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "absa_model = bert_ABSA(absa_config).to(DEVICE)\n",
    "absa_model = load_model_pkl(absa_model, \"absa_model_v1.pkl\")\n",
    "\n",
    "# ATE Testing\n",
    "print(\"\\n=== ATE Classification Report ===\")\n",
    "truths_ate, preds_ate = test_ate(ate_test_loader, ate_model)\n",
    "print(\"True labels:\", set(truths_ate))\n",
    "print(\"Predicted labels:\", set(preds_ate))\n",
    "print(classification_report(truths_ate, preds_ate, target_names=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]))\n",
    "\n",
    "cm_ate = confusion_matrix(truths_ate, preds_ate, labels=[0, 1, 2])\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_ate, display_labels=[\"Non-Aspect\", \"B-Term\", \"I-Term\"]).plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"ATE Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ABSA Testing\n",
    "print(\"\\n=== ABSA Classification Report ===\")\n",
    "truths_absa, preds_absa = test_absa(absa_test_loader, absa_model)\n",
    "print(\"True labels:\", set(truths_absa))\n",
    "print(\"Predicted labels:\", set(preds_absa))\n",
    "print(classification_report(truths_absa, preds_absa, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm_absa = confusion_matrix(truths_absa, preds_absa, labels=[0, 1, 2])\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_absa, display_labels=[\"Negative\", \"Neutral\", \"Positive\"]).plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"ABSA Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da802b0a-6a04-4c48-809b-8e12252e685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Model Loading Configuration:\n",
      "================================================================\n",
      "Found user-trained models. Will load: ate_model_v1.pkl and absa_model_v1.pkl\n",
      "================================================================\n",
      "\n",
      "Loading ATE model from: ate_model_v1.pkl...\n",
      "ATE model loaded successfully.\n",
      "\n",
      "Loading ABSA model from: absa_model_v1.pkl...\n",
      "\n",
      "An error occurred loading the models: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.16 GiB is allocated by PyTorch, and 273.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "--- Batch Prediction from CSV (Hardcoded Filenames) ---\n",
      "Attempting to read sentences from: 'mrt_reviews_batch2_sentences.csv'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the exact name of the column in your CSV containing the sentences:  Sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully read 100 sentences from column 'Sentence' in 'mrt_reviews_batch2_sentences.csv'.\n",
      "\n",
      "Processing 100 sentences...\n",
      "  Processed 50/100...\n",
      "  Processed 100/100...\n",
      "Finished processing all sentences.\n",
      "\n",
      "Results successfully saved to 'output_predictions.json'\n",
      "================================================================\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# predict_from_csv_hardcoded.py\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import BertTokenizer, logging\n",
    "# Assuming bert_ate_absa_models.py is in the same directory or Python path\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# --- Device Setup ---\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "pretrain_model_name = \"bert-base-uncased\"\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer '{pretrain_model_name}': {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Model File Selection ---\n",
    "print(\"\\nModel Loading Configuration:\")\n",
    "print(\"================================================================\")\n",
    "# Prioritize user-trained models\n",
    "ate_model_file_v1 = \"ate_model_v1.pkl\"\n",
    "absa_model_file_v1 = \"absa_model_v1.pkl\"\n",
    "user_models_v1_exist = os.path.exists(ate_model_file_v1) and os.path.exists(absa_model_file_v1)\n",
    "\n",
    "# Fallback to default models\n",
    "default_ate_file = \"ate_model.pkl\"\n",
    "default_absa_file = \"absa_model.pkl\"\n",
    "default_models_exist = os.path.exists(default_ate_file) and os.path.exists(default_absa_file)\n",
    "\n",
    "final_ate_model_file = None\n",
    "final_absa_model_file = None\n",
    "\n",
    "if user_models_v1_exist:\n",
    "    print(f\"Found user-trained models. Will load: {ate_model_file_v1} and {absa_model_file_v1}\")\n",
    "    final_ate_model_file = ate_model_file_v1\n",
    "    final_absa_model_file = absa_model_file_v1\n",
    "elif default_models_exist:\n",
    "    print(f\"User-trained models ('{ate_model_file_v1}', '{absa_model_file_v1}') not found.\")\n",
    "    print(f\"Falling back to default models: {default_ate_file} and {default_absa_file}.\")\n",
    "    final_ate_model_file = default_ate_file\n",
    "    final_absa_model_file = default_absa_file\n",
    "else:\n",
    "    print(f\"Critical Error: Neither user-trained models ('{ate_model_file_v1}', '{absa_model_file_v1}')\")\n",
    "    print(f\"nor default models ('{default_ate_file}', '{default_absa_file}') were found.\")\n",
    "    print(\"Cannot proceed without model files. Please ensure model files are in the script's directory.\")\n",
    "    exit()\n",
    "print(\"================================================================\")\n",
    "\n",
    "\n",
    "# --- Load Trained Models ---\n",
    "try:\n",
    "    print(f\"\\nLoading ATE model from: {final_ate_model_file}...\")\n",
    "    ate_model = bert_ATE.from_pretrained(pretrain_model_name, num_labels=3).to(DEVICE)\n",
    "    ate_model.load_state_dict(torch.load(final_ate_model_file, map_location=DEVICE))\n",
    "    ate_model.eval()\n",
    "    print(\"ATE model loaded successfully.\")\n",
    "\n",
    "    print(f\"\\nLoading ABSA model from: {final_absa_model_file}...\")\n",
    "    absa_model = bert_ABSA.from_pretrained(pretrain_model_name, num_labels=3).to(DEVICE)\n",
    "    absa_model.load_state_dict(torch.load(final_absa_model_file, map_location=DEVICE))\n",
    "    absa_model.eval()\n",
    "    print(\"ABSA model loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # This case should ideally be caught by the checks above, but as a safeguard:\n",
    "    print(f\"\\nERROR: Could not find the required model file(s) ({final_ate_model_file} or {final_absa_model_file}).\")\n",
    "    print(\"This should have been caught earlier. Please check file paths and existence.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred loading the models: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Model Inference Functions (Adapted from your result_single.py and previous versions) ---\n",
    "\n",
    "def extract_aspect_terms(sentence):\n",
    "    \"\"\"Extracts aspect terms from a sentence using the ATE model.\"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if not tokens:\n",
    "        return []\n",
    "\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "    if input_ids.shape[1] > tokenizer.model_max_length:\n",
    "         # print(f\"Warning: Input sentence too long ({input_ids.shape[1]} tokens), truncating to {tokenizer.model_max_length} for ATE.\")\n",
    "         input_ids = input_ids[:, :tokenizer.model_max_length]\n",
    "         # Adjust tokens list to reflect truncation for accurate zipping later\n",
    "         # This gets tricky if truncation happens mid-wordpiece.\n",
    "         # A simpler approach is to rely on the length of predictions relative to original tokens.\n",
    "         # For now, we'll let the prediction slicing handle it, but it's an area for refinement.\n",
    "         # Re-tokenize the truncated IDs to get a token list that matches the model input length\n",
    "         tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    attention_mask = (input_ids != 0).long().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = ate_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "            # .squeeze(0) assumes batch size of 1, which is true for single sentence processing\n",
    "            predictions = torch.argmax(logits, dim=2).squeeze(0).cpu().tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ATE model inference for sentence: '{sentence}'. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    if not isinstance(predictions, list):\n",
    "        predictions = [predictions]\n",
    "\n",
    "    # Align predictions with the (potentially truncated) tokens\n",
    "    # `predictions` from BERT will correspond to the length of `input_ids`\n",
    "    # `tokens` list (potentially re-tokenized after truncation) is what we iterate over\n",
    "    num_actual_tokens = len(tokens) # Number of tokens after encode and potential truncation, excluding special tokens\n",
    "    \n",
    "    # Predictions from BERT include [CLS] and [SEP]\n",
    "    # We expect predictions for [CLS], token_1, ..., token_n, [SEP], [PAD]...\n",
    "    # So, we slice from index 1 up to 1 + num_actual_tokens\n",
    "    if len(predictions) >= num_actual_tokens + 1: # Need at least predictions for [CLS] + actual_tokens\n",
    "        pred_slice = predictions[1 : num_actual_tokens + 1]\n",
    "    else:\n",
    "        # This indicates a significant mismatch, possibly due to extreme truncation or error\n",
    "        # print(f\"Warning: ATE prediction length ({len(predictions)}) mismatch with token count ({num_actual_tokens}). Using available predictions.\")\n",
    "        available_preds_after_cls = len(predictions) - 1\n",
    "        slice_len = min(num_actual_tokens, available_preds_after_cls)\n",
    "        pred_slice = predictions[1 : 1 + slice_len]\n",
    "        tokens = tokens[:slice_len] # Adjust tokens to match the slice for zipping\n",
    "\n",
    "    aspect_terms = []\n",
    "    current_term = \"\"\n",
    "    for token, label in zip(tokens, pred_slice):\n",
    "        temp_label = label\n",
    "        if token.startswith(\"##\") and temp_label == 1:\n",
    "            temp_label = 2 # Treat as I-Term if it's a subword continuation\n",
    "\n",
    "        if temp_label == 1:  # B-Term\n",
    "            if current_term: # Finalize previous term\n",
    "                aspect_terms.append(current_term)\n",
    "            current_term = token.lstrip(\"##\")\n",
    "        elif temp_label == 2:  # I-Term\n",
    "            if current_term: # Continue existing term\n",
    "                current_term += token.lstrip(\"##\")\n",
    "            else: # If I-Term appears without a preceding B-Term, treat it as a new B-Term\n",
    "                current_term = token.lstrip(\"##\")\n",
    "        else:  # O-Tag (label 0)\n",
    "            if current_term: # Finalize previous term\n",
    "                aspect_terms.append(current_term)\n",
    "                current_term = \"\"\n",
    "\n",
    "    if current_term: # Append any term remaining after the loop\n",
    "        aspect_terms.append(current_term)\n",
    "    return aspect_terms\n",
    "\n",
    "def determine_polarity(sentence, aspect_terms):\n",
    "    \"\"\"Determines polarity for each aspect term in the context of the sentence.\"\"\"\n",
    "    polarities = {}\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "    for term in aspect_terms:\n",
    "        # Format for ABSA: [CLS] sentence [SEP] aspect_term [SEP]\n",
    "        term_with_context = f\"[CLS] {sentence} [SEP] {term} [SEP]\"\n",
    "\n",
    "        try:\n",
    "            encoded_input = tokenizer(term_with_context, return_tensors=\"pt\", truncation=True, max_length=tokenizer.model_max_length)\n",
    "            input_ids = encoded_input['input_ids'].to(DEVICE)\n",
    "            attention_mask = encoded_input['attention_mask'].to(DEVICE)\n",
    "\n",
    "            # Determine token_type_ids (segment IDs)\n",
    "            sep_indices = (input_ids[0] == tokenizer.sep_token_id).nonzero(as_tuple=True)[0]\n",
    "            token_type_ids = None # Default to None if model doesn't strictly need them or if format is unexpected\n",
    "            if len(sep_indices) >= 1: # At least one SEP token (after sentence part)\n",
    "                first_sep_index = sep_indices[0]\n",
    "                token_type_ids = torch.zeros_like(input_ids)\n",
    "                # Segment B (aspect term + second SEP) starts after the first SEP\n",
    "                if input_ids.shape[1] > first_sep_index + 1 : # Check if there are tokens after the first SEP\n",
    "                    token_type_ids[0, first_sep_index + 1:] = 1\n",
    "                token_type_ids = token_type_ids.to(DEVICE)\n",
    "            # else:\n",
    "                # print(f\"Warning: Could not reliably determine token_type_ids for ABSA input for term '{term}'.\")\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = absa_model(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   token_type_ids=token_type_ids)\n",
    "                logits = outputs[\"logits\"]\n",
    "                polarity_idx = torch.argmax(logits, dim=1).item()\n",
    "            polarities[term] = label_map.get(polarity_idx, \"Unknown\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ABSA model inference for term '{term}' in sentence: '{sentence}'. Error: {e}\")\n",
    "            polarities[term] = \"Error\"\n",
    "    return polarities\n",
    "\n",
    "def predict_outputs_for_sentence(sentence_text):\n",
    "    \"\"\"Combines ATE and ABSA for a single sentence text.\"\"\"\n",
    "    if not isinstance(sentence_text, str) or not sentence_text.strip():\n",
    "         # print(f\"Warning: Skipping empty or invalid sentence: {sentence_text}\")\n",
    "         return [] # Return empty list for invalid input\n",
    "    extracted_aspects = extract_aspect_terms(sentence_text)\n",
    "    if extracted_aspects:\n",
    "        polarities_map = determine_polarity(sentence_text, extracted_aspects)\n",
    "        return [{\"aspect\": aspect, \"polarity\": polarities_map.get(aspect, \"Unknown\")} for aspect in extracted_aspects]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# --- CSV Reading Function ---\n",
    "def read_sentences_from_csv(file_path, sentence_column_name):\n",
    "    \"\"\"Reads sentences from a specified column in a CSV file.\"\"\"\n",
    "    sentences = []\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if sentence_column_name not in df.columns:\n",
    "            print(f\"Error: Column '{sentence_column_name}' not found in CSV file '{file_path}'.\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return None\n",
    "        df.dropna(subset=[sentence_column_name], inplace=True) # Remove rows where sentence is NaN\n",
    "        sentences = df[sentence_column_name].astype(str).tolist() # Ensure all are strings\n",
    "        print(f\"\\nSuccessfully read {len(sentences)} sentences from column '{sentence_column_name}' in '{file_path}'.\")\n",
    "        return sentences\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input CSV file not found at '{file_path}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Batch Prediction from CSV (Hardcoded Filenames) ---\")\n",
    "\n",
    "    # Hardcoded input CSV file path and output JSON file path\n",
    "    input_csv_file = \"mrt_reviews_batch2_sentences.csv\" # Hardcoded input CSV name\n",
    "    output_json_file = \"output_predictions.json\" # Hardcoded output JSON name\n",
    "\n",
    "    print(f\"Attempting to read sentences from: '{input_csv_file}'\")\n",
    "\n",
    "    # Get sentence column name from user\n",
    "    sentence_column = input(\"Enter the exact name of the column in your CSV containing the sentences: \").strip()\n",
    "\n",
    "    # Read sentences\n",
    "    sentences_to_process = read_sentences_from_csv(input_csv_file, sentence_column)\n",
    "\n",
    "    if sentences_to_process is None: # Error during reading\n",
    "        print(\"Exiting due to CSV reading error.\")\n",
    "        exit()\n",
    "    if not sentences_to_process: # No sentences found\n",
    "        print(f\"No sentences found in column '{sentence_column}' of '{input_csv_file}'. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Process sentences and store results\n",
    "    all_results = []\n",
    "    total_sentences = len(sentences_to_process)\n",
    "    print(f\"\\nProcessing {total_sentences} sentences...\")\n",
    "\n",
    "    for i, sentence_text in enumerate(sentences_to_process):\n",
    "        if (i + 1) % 50 == 0 or (i + 1) == total_sentences : # Print progress\n",
    "            print(f\"  Processed {i+1}/{total_sentences}...\")\n",
    "        \n",
    "        cleaned_sentence = sentence_text.strip()\n",
    "        if not cleaned_sentence: # Skip if sentence is empty after stripping\n",
    "            # print(f\"  Skipping empty sentence at original index {i}.\")\n",
    "            all_results.append({\n",
    "                \"original_sentence\": sentence_text, # Keep original for reference\n",
    "                \"predicted_outputs\": [] # No predictions for empty\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        predicted_data = predict_outputs_for_sentence(cleaned_sentence)\n",
    "        all_results.append({\n",
    "            \"original_sentence\": cleaned_sentence, # Store the cleaned sentence\n",
    "            \"predicted_outputs\": predicted_data\n",
    "        })\n",
    "\n",
    "    print(f\"Finished processing all sentences.\")\n",
    "\n",
    "    # Save results to JSON\n",
    "    try:\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as f_out:\n",
    "            json.dump(all_results, f_out, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nResults successfully saved to '{output_json_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results to JSON file '{output_json_file}': {e}\")\n",
    "\n",
    "    print(\"================================================================\")\n",
    "    print(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2822dcc-c819-4a69-96dc-1959cb17fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== MENU ===========================\n",
      "Choose which model files to use:\n",
      "\n",
      "1) Models fine-tuned by us --> (ate_model.pkl and absa_model.pkl)\n",
      "\n",
      "2) Models trained by user if he/she run the training files before running this --> (ate_model_v1.pkl and absa_model_v1.pkl)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1) Models fine-tuned by us --> (ate_model.pkl and absa_model.pkl)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2) Models trained by user if he/she run the training files before running this --> (ate_model_v1.pkl and absa_model_v1.pkl)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter 1 or 2: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading ate_model_v1.pkl and absa_model_v1.pkl models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# result_single.py\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, logging\n",
    "from bert_ate_absa_models import bert_ATE, bert_ABSA\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Load device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models and tokenizer\n",
    "pretrain_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "\n",
    "# User's choice for model selection\n",
    "print(\"=========================== MENU ===========================\")\n",
    "print(\"Choose which model files to use:\\n\")\n",
    "print(\"1) Models fine-tuned by us --> (ate_model.pkl and absa_model.pkl)\\n\")\n",
    "print(\"2) Models trained by user if he/she run the training files before running this --> (ate_model_v1.pkl and absa_model_v1.pkl)\\n\")\n",
    "\n",
    "choice = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "if choice == \"2\":\n",
    "    print(\"Loading ate_model_v1.pkl and absa_model_v1.pkl models.\")\n",
    "    print(\"================================================================\")\n",
    "    ate_model_file = \"ate_model_v1.pkl\"\n",
    "    absa_model_file = \"absa_model_v1.pkl\"\n",
    "    if not (os.path.exists(ate_model_file) and os.path.exists(absa_model_file)):\n",
    "        print(f\"Files not found. Falling back to pre-trained models --> (ate_model.pkl and absa_model.pkl).\\n\")\n",
    "        print(\"================================================================\")\n",
    "        ate_model_file = \"ate_model.pkl\"\n",
    "        absa_model_file = \"absa_model.pkl\"\n",
    "else:\n",
    "    print(\"Loading ate_model.pkl and absa_model.pkl models.\")\n",
    "    print(\"================================================================\")\n",
    "    ate_model_file = \"ate_model.pkl\"\n",
    "    absa_model_file = \"absa_model.pkl\"\n",
    "\n",
    "# Load trained models\n",
    "ate_model = bert_ATE.from_pretrained(pretrain_model_name, num_labels=3).to(DEVICE)\n",
    "ate_model.load_state_dict(torch.load(ate_model_file))\n",
    "ate_model.eval()\n",
    "\n",
    "absa_model = bert_ABSA.from_pretrained(pretrain_model_name, num_labels=3).to(DEVICE)\n",
    "absa_model.load_state_dict(torch.load(absa_model_file))\n",
    "absa_model.eval()\n",
    "\n",
    "# ==========================\n",
    "# Function: Extract Aspects\n",
    "# ==========================\n",
    "def extract_aspect_terms(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = ate_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        predictions = torch.argmax(logits, dim=2).squeeze().cpu().tolist()\n",
    "\n",
    "    aspect_terms = []\n",
    "    current_term = \"\"\n",
    "    for token, label in zip(tokens, predictions[1:-1]):  # Exclude [CLS] and [SEP]\n",
    "        if token.startswith(\"##\") and label == 1:\n",
    "            label = 2  # treat as I-Term\n",
    "\n",
    "        if label == 1:  # B-Term\n",
    "            if current_term:\n",
    "                aspect_terms.append(current_term.strip())\n",
    "            current_term = token.lstrip(\"##\")\n",
    "        elif label == 2:  # I-Term\n",
    "            current_term += \" \" + token.lstrip(\"##\")  # preserve space\n",
    "        else:  # Non-aspect\n",
    "            if current_term:\n",
    "                aspect_terms.append(current_term.strip())\n",
    "                current_term = \"\"\n",
    "\n",
    "    # Append any remaining term\n",
    "    if current_term:\n",
    "        aspect_terms.append(current_term.strip())\n",
    "\n",
    "    return aspect_terms\n",
    "\n",
    "# ===============================\n",
    "# Function: Determine Polarities\n",
    "# ===============================\n",
    "def determine_polarity(sentence, aspect_terms):\n",
    "    polarities = {}\n",
    "    for term in aspect_terms:\n",
    "        inputs = tokenizer(sentence, term, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = absa_model(**inputs)  # includes token_type_ids automatically\n",
    "            logits = outputs[\"logits\"]\n",
    "            polarity = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "            # Debug output (optional)\n",
    "            print(\"Raw logits:\", logits)\n",
    "            print(\"Predicted polarity index:\", polarity)\n",
    "\n",
    "            polarity_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "            polarities[term] = polarity_label[polarity]\n",
    "\n",
    "    return polarities\n",
    "\n",
    "# =====================\n",
    "# Main Execution Block\n",
    "# =====================\n",
    "def main_pipeline(sentence):\n",
    "    print(\"\\n=========================== Input Sentence ===========================\")\n",
    "    print(f\"Input Sentence: {sentence}\")\n",
    "    aspect_terms = extract_aspect_terms(sentence)\n",
    "    print(f\"Extracted Aspect Terms: {aspect_terms}\")\n",
    "\n",
    "    if aspect_terms:\n",
    "        polarities = determine_polarity(sentence, aspect_terms)\n",
    "        print(\"\\n================================================================\")\n",
    "        print(\"Aspect Terms and Their Polarities:\")\n",
    "        for term, polarity in polarities.items():\n",
    "            print(f\"  {term}: {polarity}\")\n",
    "    else:\n",
    "        print(\"No aspect terms found.\")\n",
    "        print(\"================================================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"\\n================================================================\")\n",
    "        sentence = input(\"Enter a sentence (or type 'q' to quit): \")\n",
    "        if sentence.lower() in [\"q\", \"quit\"]:\n",
    "            print(\"Exiting...\")\n",
    "            print(\"================================================================\")\n",
    "            break\n",
    "        main_pipeline(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebf0d9-27b2-4303-80b1-b84fde2aa941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE model 'ate_model_v1.pkl' loaded successfully on cpu (with strict=False)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSA model 'absa_model_v1.pkl' loaded successfully on cpu!\n",
      "\n",
      "--- MRT Review Aspect-Based Sentiment Analysis ---\n",
      "Enter your MRT review and press Enter. Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your MRT Review:  the station is clean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified Aspects and Sentiments:\n",
      "- Aspect: 'station', Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "#RESULT SINGLE \n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
    "import re\n",
    "\n",
    "# --- 1. Define Model Configurations and Mappings ---\n",
    "# These must match how you trained your models!\n",
    "\n",
    "# ATE Model (Aspect Term Extraction)\n",
    "# Your labels: 0 non-aspect, 1 b-term, 2 i-term\n",
    "ATE_LABELS = ['non-aspect', 'b-term', 'i-term']\n",
    "ATE_ID2LABEL = {i: label for i, label in enumerate(ATE_LABELS)}\n",
    "ATE_LABEL2ID = {label: i for i, label in enumerate(ATE_LABELS)}\n",
    "NUM_ATE_LABELS = len(ATE_LABELS)\n",
    "\n",
    "# ABSA Model (Aspect-Based Sentiment Analysis)\n",
    "# Your polarities: 0 negative, 1 neutral, 2 positive\n",
    "ABSA_LABELS = ['Negative', 'Neutral', 'Positive']\n",
    "ABSA_ID2LABEL = {i: label for i, label in enumerate(ABSA_LABELS)}\n",
    "ABSA_LABEL2ID = {label: i for i, label in enumerate(ABSA_LABELS)}\n",
    "NUM_ABSA_LABELS = len(ABSA_LABELS)\n",
    "\n",
    "# --- 2. Load Tokenizer and Models ---\n",
    "# The tokenizer should be the same as what you used for training 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load ATE Model\n",
    "try:\n",
    "    ate_model = BertForTokenClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=NUM_ATE_LABELS,\n",
    "        id2label=ATE_ID2LABEL,\n",
    "        label2id=ATE_LABEL2ID,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    ate_model.load_state_dict(torch.load('ate_model_v1.pkl', map_location=device), strict=False)\n",
    "    ate_model.to(device)\n",
    "    ate_model.eval() # Set model to evaluation mode\n",
    "    print(f\"ATE model 'ate_model_v1.pkl' loaded successfully on {device} (with strict=False)!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ate_model_v1.pkl' not found. Please ensure the path is correct.\")\n",
    "    print(\"Make sure this .pkl file contains the model's state_dict.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ATE model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load ABSA Model\n",
    "try:\n",
    "    absa_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=NUM_ABSA_LABELS,\n",
    "        id2label=ABSA_ID2LABEL,\n",
    "        label2id=ABSA_LABEL2ID\n",
    "    )\n",
    "    absa_model.load_state_dict(torch.load('absa_model_v1.pkl', map_location=device))\n",
    "    absa_model.to(device)\n",
    "    absa_model.eval() # Set model to evaluation mode\n",
    "    print(f\"ABSA model 'absa_model_v1.pkl' loaded successfully on {device}!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'absa_model_v1.pkl' not found. Please ensure the path is correct.\")\n",
    "    print(\"Make sure this .pkl file contains the model's state_dict.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ABSA model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Text Preprocessing (Minimal) ---\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 4. Aspect Term Extraction (ATE) Function ---\n",
    "def extract_aspect_terms(review_text, tokenizer, ate_model, device, max_len=128):\n",
    "    preprocessed_text = preprocess_text(review_text)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        preprocessed_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True # Explicitly request token_type_ids\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding['token_type_ids'].to(device) # Explicitly move token_type_ids to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass token_type_ids to the model\n",
    "        outputs = ate_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().cpu().numpy()\n",
    "\n",
    "    extracted_aspects = []\n",
    "    current_aspect_tokens = []\n",
    "\n",
    "    original_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())\n",
    "    valid_length = (attention_mask.squeeze() == 1).sum().item() \n",
    "    \n",
    "    for i in range(1, valid_length - 1):\n",
    "        token = original_tokens[i]\n",
    "        predicted_label_id = predictions[i]\n",
    "        predicted_label = ATE_ID2LABEL[predicted_label_id]\n",
    "\n",
    "        if predicted_label == 'b-term':\n",
    "            if current_aspect_tokens:\n",
    "                extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "            current_aspect_tokens = [token]\n",
    "        elif predicted_label == 'i-term':\n",
    "            if current_aspect_tokens:\n",
    "                current_aspect_tokens.append(token)\n",
    "            else:\n",
    "                pass \n",
    "        else: # 'non-aspect'\n",
    "            if current_aspect_tokens:\n",
    "                extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "                current_aspect_tokens = []\n",
    "    \n",
    "    if current_aspect_tokens:\n",
    "        extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "\n",
    "    return list(set([aspect.strip() for aspect in extracted_aspects if aspect.strip()]))\n",
    "\n",
    "# --- 5. Aspect-Based Sentiment Analysis (ABSA) Function ---\n",
    "def analyze_aspect_sentiment(review_text, aspect_term, tokenizer, absa_model, device, max_len=128):\n",
    "    preprocessed_review = preprocess_text(review_text)\n",
    "    preprocessed_aspect = preprocess_text(aspect_term)\n",
    "\n",
    "    # For ABSA, token_type_ids are crucial for distinguishing review and aspect segments\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        preprocessed_review,\n",
    "        preprocessed_aspect,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True # Explicitly request token_type_ids\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device) # Explicitly move token_type_ids to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass token_type_ids to the model\n",
    "        outputs = absa_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "    sentiment = ABSA_ID2LABEL.get(predicted_class_id, \"Unknown Sentiment\")\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# --- 6. Main Interaction Loop ---\n",
    "print(\"\\n--- MRT Review Aspect-Based Sentiment Analysis ---\")\n",
    "print(\"Enter your MRT review and press Enter. Type 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    user_review = input(\"\\nYour MRT Review: \")\n",
    "    if user_review.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    if not user_review.strip():\n",
    "        print(\"Please enter a review.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Extract Aspect Terms\n",
    "    aspect_terms = extract_aspect_terms(user_review, tokenizer, ate_model, device)\n",
    "\n",
    "    if aspect_terms:\n",
    "        print(\"\\nIdentified Aspects and Sentiments:\")\n",
    "        for aspect in aspect_terms:\n",
    "            # Step 2: Analyze Sentiment for each identified Aspect\n",
    "            sentiment = analyze_aspect_sentiment(user_review, aspect, tokenizer, absa_model, device)\n",
    "            print(f\"- Aspect: '{aspect}', Sentiment: {sentiment}\")\n",
    "    else:\n",
    "        print(\"No specific aspects identified in this review.\")\n",
    "\n",
    "print(\"\\nThank you for using the MRT Review Analyzer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c921ef-0d6d-41ee-b377-11b42815aa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ATE model 'ate_model_v1.pkl' loaded successfully on cuda (with strict=False)!\n",
      "ABSA model 'absa_model_v1.pkl' loaded successfully on cuda!\n",
      "Aspect dictionary 'aspect_dictionary.csv' loaded successfully with 1065 terms.\n",
      "\n",
      "--- MRT Review Aspect-Based Sentiment Analysis ---\n",
      "Enter your MRT review and press Enter. Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your MRT Review:  the station is clean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified Aspects and Sentiments:\n",
      "- Term: 'station', Category: 'Uncategorized', Sentiment: Positive\n",
      "- Term: 'clean', Category: 'cleanliness', Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "#find category\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
    "import re\n",
    "import pandas as pd # New import for loading aspect_dictionary.csv\n",
    "\n",
    "# --- 1. Define Model Configurations and Mappings ---\n",
    "# These must match how you trained your models!\n",
    "\n",
    "# ATE Model (Aspect Term Extraction)\n",
    "# Your labels: 0 non-aspect, 1 b-term, 2 i-term\n",
    "ATE_LABELS = ['non-aspect', 'b-term', 'i-term']\n",
    "ATE_ID2LABEL = {i: label for i, label in enumerate(ATE_LABELS)}\n",
    "ATE_LABEL2ID = {label: i for i, label in enumerate(ATE_LABELS)}\n",
    "NUM_ATE_LABELS = len(ATE_LABELS)\n",
    "\n",
    "# ABSA Model (Aspect-Based Sentiment Analysis)\n",
    "# Your polarities: 0 negative, 1 neutral, 2 positive\n",
    "ABSA_LABELS = ['Negative', 'Neutral', 'Positive']\n",
    "ABSA_ID2LABEL = {i: label for i, label in enumerate(ABSA_LABELS)}\n",
    "ABSA_LABEL2ID = {label: i for i, label in enumerate(ABSA_LABELS)}\n",
    "NUM_ABSA_LABELS = len(ABSA_LABELS)\n",
    "\n",
    "# --- 2. Load Tokenizer and Models ---\n",
    "# The tokenizer should be the same as what you used for training 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load ATE Model\n",
    "try:\n",
    "    ate_model = BertForTokenClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=NUM_ATE_LABELS,\n",
    "        id2label=ATE_ID2LABEL,\n",
    "        label2id=ATE_LABEL2ID,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    ate_model.load_state_dict(torch.load('ate_model_v1.pkl', map_location=device), strict=False)\n",
    "    ate_model.to(device)\n",
    "    ate_model.eval() # Set model to evaluation mode\n",
    "    print(f\"ATE model 'ate_model_v1.pkl' loaded successfully on {device} (with strict=False)!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ate_model_v1.pkl' not found. Please ensure the path is correct.\")\n",
    "    print(\"Make sure this .pkl file contains the model's state_dict.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ATE model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load ABSA Model\n",
    "try:\n",
    "    absa_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=NUM_ABSA_LABELS,\n",
    "        id2label=ABSA_ID2LABEL,\n",
    "        label2id=ABSA_LABEL2ID\n",
    "    )\n",
    "    absa_model.load_state_dict(torch.load('absa_model_v1.pkl', map_location=device))\n",
    "    absa_model.to(device)\n",
    "    absa_model.eval() # Set model to evaluation mode\n",
    "    print(f\"ABSA model 'absa_model_v1.pkl' loaded successfully on {device}!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'absa_model_v1.pkl' not found. Please ensure the path is correct.\")\n",
    "    print(\"Make sure this .pkl file contains the model's state_dict.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ABSA model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- NEW: Load Aspect Dictionary ---\n",
    "aspect_dictionary = {}\n",
    "ASPECT_DICT_PATH = 'aspect_dictionary.csv' # Make sure this file is in the same directory\n",
    "\n",
    "try:\n",
    "    aspect_dict_df = pd.read_csv(ASPECT_DICT_PATH)\n",
    "    if 'term' in aspect_dict_df.columns and 'category' in aspect_dict_df.columns:\n",
    "        for index, row in aspect_dict_df.iterrows():\n",
    "            term = str(row['term']).strip().lower()\n",
    "            category = str(row['category']).strip().lower()\n",
    "            if term and category:\n",
    "                # Store term -> list of categories (in case one term maps to multiple, though unlikely in your file)\n",
    "                if term not in aspect_dictionary:\n",
    "                    aspect_dictionary[term] = []\n",
    "                if category not in aspect_dictionary[term]: # Avoid duplicate categories for a single term\n",
    "                    aspect_dictionary[term].append(category)\n",
    "        print(f\"Aspect dictionary '{ASPECT_DICT_PATH}' loaded successfully with {len(aspect_dictionary)} terms.\")\n",
    "    else:\n",
    "        print(f\"Warning: '{ASPECT_DICT_PATH}' must contain 'term' and 'category' columns. Category lookup will not work.\")\n",
    "        aspect_dictionary = {} # Empty the dict if columns are missing\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Aspect dictionary file '{ASPECT_DICT_PATH}' not found. Category lookup will not work.\")\n",
    "    aspect_dictionary = {} # Empty the dict on error\n",
    "except Exception as e:\n",
    "    print(f\"Error loading aspect dictionary: {e}. Category lookup will not work.\")\n",
    "    aspect_dictionary = {} # Empty the dict on error\n",
    "\n",
    "\n",
    "# --- 3. Text Preprocessing (Minimal) ---\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): # Added check for non-string input\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 4. Aspect Term Extraction (ATE) Function (BERT-based, renamed for clarity) ---\n",
    "def extract_aspect_terms_bert(review_text, tokenizer, ate_model, device, max_len=128):\n",
    "    preprocessed_text = preprocess_text(review_text)\n",
    "    if not preprocessed_text: # Handle empty preprocessed text\n",
    "        return []\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        preprocessed_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding['token_type_ids'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = ate_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().cpu().numpy()\n",
    "\n",
    "    extracted_aspects = []\n",
    "    current_aspect_tokens = []\n",
    "\n",
    "    original_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())\n",
    "    valid_length = (attention_mask.squeeze() == 1).sum().item()\n",
    "    \n",
    "    for i in range(1, valid_length - 1): # Exclude [CLS] and [SEP]\n",
    "        token = original_tokens[i]\n",
    "        predicted_label_id = predictions[i]\n",
    "        predicted_label = ATE_ID2LABEL[predicted_label_id]\n",
    "\n",
    "        if predicted_label == 'b-term':\n",
    "            if current_aspect_tokens:\n",
    "                extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "            current_aspect_tokens = [token]\n",
    "        elif predicted_label == 'i-term':\n",
    "            if current_aspect_tokens:\n",
    "                current_aspect_tokens.append(token)\n",
    "            else:\n",
    "                pass # Skip i-term if not part of an ongoing aspect\n",
    "        else: # 'non-aspect'\n",
    "            if current_aspect_tokens:\n",
    "                extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "                current_aspect_tokens = []\n",
    "    \n",
    "    if current_aspect_tokens: # Add the last aspect if loop ends with one\n",
    "        extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "\n",
    "    return list(set([aspect.strip() for aspect in extracted_aspects if aspect.strip()]))\n",
    "\n",
    "# --- 5. Aspect-Based Sentiment Analysis (ABSA) Function (renamed for clarity) ---\n",
    "def analyze_sentiment_for_term(review_text, aspect_term, tokenizer, absa_model, device, max_len=128):\n",
    "    preprocessed_review = preprocess_text(review_text)\n",
    "    preprocessed_aspect = preprocess_text(aspect_term)\n",
    "\n",
    "    if not preprocessed_review or not preprocessed_aspect: # Skip if either is empty\n",
    "        return \"N/A\"\n",
    "\n",
    "    # For ABSA, token_type_ids are crucial for distinguishing review and aspect segments\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        preprocessed_review,\n",
    "        preprocessed_aspect,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = absa_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "    sentiment = ABSA_ID2LABEL.get(predicted_class_id, \"Unknown Sentiment\")\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# --- NEW: Function to get category from dictionary (used for all terms) ---\n",
    "def get_category_for_term(term, aspect_dict):\n",
    "    preprocessed_term = preprocess_text(term)\n",
    "    # Check for direct match in dictionary\n",
    "    categories = aspect_dict.get(preprocessed_term, [])\n",
    "    if categories:\n",
    "        return categories[0] # Return the first category if multiple exist for the term\n",
    "    else:\n",
    "        return \"Uncategorized\"\n",
    "\n",
    "# --- NEW: Function to identify terms directly from the dictionary in the review text ---\n",
    "def identify_dictionary_terms(review_text, aspect_dict):\n",
    "    found_terms_and_categories = []\n",
    "    preprocessed_review = preprocess_text(review_text)\n",
    "    \n",
    "    # Sort terms by length (descending) to match longer phrases first\n",
    "    sorted_dict_terms = sorted(aspect_dict.keys(), key=len, reverse=True)\n",
    "\n",
    "    for term in sorted_dict_terms:\n",
    "        # Use regex to find whole word matches for the term\n",
    "        # r'\\b' ensures whole word matching. re.escape handles special characters in term.\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', preprocessed_review):\n",
    "            category = get_category_for_term(term, aspect_dict)\n",
    "            if category != \"Uncategorized\": # Only add if it maps to a known category\n",
    "                found_terms_and_categories.append({'term': term, 'category': category})\n",
    "    \n",
    "    # Remove duplicates based on term (if multiple dict terms resolve to same exact term, e.g. \"clean\" and \"clean\")\n",
    "    unique_terms = {item['term']: item for item in found_terms_and_categories}.values()\n",
    "    return list(unique_terms)\n",
    "\n",
    "\n",
    "# --- 6. Main Interaction Loop (MODIFIED for Hybrid Approach) ---\n",
    "print(\"\\n--- MRT Review Aspect-Based Sentiment Analysis ---\")\n",
    "print(\"Enter your MRT review and press Enter. Type 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    user_review = input(\"\\nYour MRT Review: \")\n",
    "    if user_review.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    if not user_review.strip():\n",
    "        print(\"Please enter a review.\")\n",
    "        continue\n",
    "\n",
    "    processed_terms = []\n",
    "    processed_term_texts = set() # To keep track of terms already analyzed/printed to avoid duplicates\n",
    "\n",
    "    # Approach 1: Terms identified by BERT ATE model\n",
    "    bert_extracted_terms = extract_aspect_terms_bert(user_review, tokenizer, ate_model, device)\n",
    "    for term in bert_extracted_terms:\n",
    "        # Only process if this specific term text hasn't been handled yet\n",
    "        if preprocess_text(term) not in processed_term_texts:\n",
    "            category = get_category_for_term(term, aspect_dictionary)\n",
    "            sentiment = analyze_sentiment_for_term(user_review, term, tokenizer, absa_model, device)\n",
    "            processed_terms.append({'term': term, 'category': category, 'sentiment': sentiment})\n",
    "            processed_term_texts.add(preprocess_text(term)) # Add to set after processing\n",
    "\n",
    "    # Approach 2: Terms explicitly found from the dictionary within the review\n",
    "    # This captures terms like \"clean\" even if BERT ATE didn't extract them as primary aspects.\n",
    "    dictionary_found_terms_info = identify_dictionary_terms(user_review, aspect_dictionary)\n",
    "    for item in dictionary_found_terms_info:\n",
    "        term = item['term']\n",
    "        category = item['category'] # Category is directly from dictionary lookup\n",
    "        \n",
    "        # Only process if this specific term text hasn't been handled by BERT ATE or a prior dict match\n",
    "        if preprocess_text(term) not in processed_term_texts:\n",
    "            sentiment = analyze_sentiment_for_term(user_review, term, tokenizer, absa_model, device)\n",
    "            processed_terms.append({'term': term, 'category': category, 'sentiment': sentiment})\n",
    "            processed_term_texts.add(preprocess_text(term))\n",
    "\n",
    "    if processed_terms:\n",
    "        print(\"\\nIdentified Aspects and Sentiments:\")\n",
    "        # Sort for consistent output (e.g., by category then by term)\n",
    "        processed_terms.sort(key=lambda x: (x['category'], x['term'])) \n",
    "        for result in processed_terms:\n",
    "            print(f\"- Term: '{result['term']}', Category: '{result['category']}', Sentiment: {result['sentiment']}\")\n",
    "    else:\n",
    "        print(\"No specific aspects identified in this review.\")\n",
    "\n",
    "print(\"\\nThank you for using the MRT Review Analyzer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c198a2cd-83be-4112-b2e0-3b94010db4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 'punkt' tokenizer data found.\n",
      "NLTK 'punkt_tab' tokenizer data found.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE model 'ate_model_v1.pkl' loaded successfully on cuda (with strict=False)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSA model 'absa_model_v1.pkl' loaded successfully on cuda!\n",
      "Aspect dictionary 'aspect_dictionary.csv' loaded successfully with 1067 terms.\n",
      "\n",
      "--- MRT Review Aspect-Based Sentiment Analysis (Database Batch Mode) ---\n",
      "Attempting to load aspect dictionary from: aspect_dictionary.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the review text column in your 'reviews' table (default is 'raw_reviews'):  raw_reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connected. AspectSentiments table ensured.\n",
      "Successfully loaded 5929 reviews from the database.\n",
      "Processing review 1/5929 (Review ID: 1, Station: KG04 KWASA DAMANSARA): 'always clean n well managed msia had superb facilities only a few placesspot leaks during raining n ...'\n",
      "Processing review 2/5929 (Review ID: 2, Station: KG04 KWASA DAMANSARA): 'the last point of your life end of the train line it is very spacious compared to other stations but...'\n",
      "Processing review 3/5929 (Review ID: 3, Station: KG04 KWASA DAMANSARA): 'nice mrt location wide and with ample parking space and convenient it is convenient to walk to the n...'\n",
      "Processing review 4/5929 (Review ID: 4, Station: KG04 KWASA DAMANSARA): 'the prime minister recently announced that there is a free ride on the mrt putrajaya line from kwasa...'\n",
      "Processing review 5/5929 (Review ID: 5, Station: KG04 KWASA DAMANSARA): 'lots of parking and spacious roadways the covered waiting area is large so it is easy to find places...'\n",
      "Processing review 6/5929 (Review ID: 6, Station: KG04 KWASA DAMANSARA): 'mrt delay stall from am till am on rd february complaint on their website and no response so far who...'\n",
      "Processing review 7/5929 (Review ID: 7, Station: KG04 KWASA DAMANSARA): 'the last station for both putrajaya kajang mrt lines switching between lines is a simple walk either...'\n",
      "Processing review 8/5929 (Review ID: 8, Station: KG04 KWASA DAMANSARA): 'everything just okey if you just came for take mrt not much even i didnt usually see any rapid bus a...'\n",
      "Processing review 9/5929 (Review ID: 9, Station: KG04 KWASA DAMANSARA): 'the female staff at the counter was helpful and professional i arrived around pm and there wasnt man...'\n",
      "Processing review 10/5929 (Review ID: 10, Station: KG04 KWASA DAMANSARA): 'ample parking that what matters a person can easily switch mode of transportation by parking here to...'\n",
      "Processing review 11/5929 (Review ID: 11, Station: KG04 KWASA DAMANSARA): 'its easily access parking is available ckean...'\n",
      "Processing review 12/5929 (Review ID: 12, Station: KG04 KWASA DAMANSARA): 'ample parking space convenient station as it connects separate mrt route theres surau and toilet the...'\n",
      "Processing review 13/5929 (Review ID: 13, Station: KG04 KWASA DAMANSARA): 'disembark from kajang line then just have to take platform putrajaya linewhich is just side by side ...'\n",
      "Processing review 14/5929 (Review ID: 14, Station: KG04 KWASA DAMANSARA): 'nice mrt station but car parking need to improve if raining difficult to the carpark...'\n",
      "Processing review 15/5929 (Review ID: 15, Station: KG04 KWASA DAMANSARA): 'everything was ok but bad st experience with not friendly staff system down and token machine is not...'\n",
      "Processing review 16/5929 (Review ID: 16, Station: KG04 KWASA DAMANSARA): 'wednesday am wanted to top up for one day pass before touching out the lady a little bit chubby wear...'\n",
      "Processing review 17/5929 (Review ID: 17, Station: KG04 KWASA DAMANSARA): 'very disappointing experience at the end station ie putra jaya sentral thinking that i could take lu...'\n",
      "Processing review 18/5929 (Review ID: 18, Station: KG04 KWASA DAMANSARA): 'my train ready to take us at kwasa damansara towards kampung batu on thursday th june at pm new type...'\n",
      "Processing review 19/5929 (Review ID: 19, Station: KG04 KWASA DAMANSARA): 'i actually love the smooth transition between pyl and kgl at this station wait time most of the time...'\n",
      "Processing review 20/5929 (Review ID: 20, Station: KG04 KWASA DAMANSARA): 'huge parking place clean environment...'\n",
      "Processing review 21/5929 (Review ID: 21, Station: KG04 KWASA DAMANSARA): 'a nice new way to travel into the city plenty of car park spaces the place is very clean and modern ...'\n",
      "Processing review 22/5929 (Review ID: 22, Station: KG04 KWASA DAMANSARA): 'nice terminal took the ride to putrjaya quite smooth the ride...'\n",
      "Processing review 23/5929 (Review ID: 23, Station: KG04 KWASA DAMANSARA): 'very quiet easy access to kl praying rooms and toilets are also provided security guards are observa...'\n",
      "Processing review 24/5929 (Review ID: 24, Station: KG04 KWASA DAMANSARA): 'please improve parking services i used tng acess rmday but when out its deduct me rm same card ‍...'\n",
      "Processing review 25/5929 (Review ID: 25, Station: KG04 KWASA DAMANSARA): 'first time from kajang end to kwasa damansara end to try out putrajaya line before the complete rout...'\n",
      "Processing review 26/5929 (Review ID: 26, Station: KG04 KWASA DAMANSARA): 'toilet surau not maintained seriously cleanliness is not up to the standard...'\n",
      "Processing review 27/5929 (Review ID: 27, Station: KG04 KWASA DAMANSARA): 'lots of parking spaces even at am only rmday if you use the same tng card to enter the parking and u...'\n",
      "Processing review 28/5929 (Review ID: 28, Station: KG04 KWASA DAMANSARA): 'now seem more people coming here due to overcrowded at kwasa sentral staff is ok helpful...'\n",
      "Processing review 29/5929 (Review ID: 29, Station: KG04 KWASA DAMANSARA): 'good facilities for the station train service on ssp line stopped short to kwasa sentral we had to w...'\n",
      "Processing review 30/5929 (Review ID: 30, Station: KG04 KWASA DAMANSARA): 'final stop for the kajangkwasa damansara mrt line st time here and its nice and breezy with lots of ...'\n",
      "Processing review 31/5929 (Review ID: 31, Station: KG04 KWASA DAMANSARA): 'nice station easy to exchange to sg buloh lane as the platform are placed next to each other clean s...'\n",
      "Processing review 32/5929 (Review ID: 32, Station: KG04 KWASA DAMANSARA): 'facility in the station is superb unfortunately the surrounding can further be improved lighting for...'\n",
      "Processing review 33/5929 (Review ID: 33, Station: KG04 KWASA DAMANSARA): 'best to have this mrt line that also links to yellow line putrajaya line...'\n",
      "Processing review 34/5929 (Review ID: 34, Station: KG04 KWASA DAMANSARA): 'it was nice but instead of happiness just...'\n",
      "Processing review 35/5929 (Review ID: 35, Station: KG04 KWASA DAMANSARA): 'a relatively quiet mrt station not many people are fully utilizing this station at the moment but th...'\n",
      "Processing review 36/5929 (Review ID: 36, Station: KG04 KWASA DAMANSARA): 'im not used to trains yet but using it recently was fun for the most part it was smooth and no crazy...'\n",
      "Processing review 37/5929 (Review ID: 37, Station: KG04 KWASA DAMANSARA): 'easy and convenient for people go to kl city...'\n",
      "Processing review 38/5929 (Review ID: 38, Station: KG04 KWASA DAMANSARA): 'unlimited parking space super cheap parking fee it was just rm per entry not crowded and less waitin...'\n",
      "Processing review 39/5929 (Review ID: 39, Station: KG04 KWASA DAMANSARA): 'a great place to avoid all the congestions at kwasa sentral mrt station...'\n",
      "Processing review 40/5929 (Review ID: 40, Station: KG04 KWASA DAMANSARA): 'good and modern transportation station mrt trains come on time helpful staff to help you with ticket...'\n",
      "Processing review 41/5929 (Review ID: 41, Station: KG04 KWASA DAMANSARA): 'very good very convenient clean toilet and surrounding friendly staffs and information friendly...'\n",
      "Processing review 42/5929 (Review ID: 42, Station: KG04 KWASA DAMANSARA): 'kwasa damansara is the transit station for you to get to kajang line it was the last station for mrt...'\n",
      "Processing review 43/5929 (Review ID: 43, Station: KG04 KWASA DAMANSARA): 'overall ok main issue is the male toiletomg so bad pls do something about it...'\n",
      "Processing review 44/5929 (Review ID: 44, Station: KG04 KWASA DAMANSARA): 'this morning was my first time patronising this mrt station as previously on numerous occasion whene...'\n",
      "Processing review 45/5929 (Review ID: 45, Station: KG04 KWASA DAMANSARA): 'interchange for kajang and putrajaya line if only the timing of trains can be better so that passeng...'\n",
      "Processing review 46/5929 (Review ID: 46, Station: KG04 KWASA DAMANSARA): 'good ride but male toilet needs to be done up properly...'\n",
      "Processing review 47/5929 (Review ID: 47, Station: KG04 KWASA DAMANSARA): 'have ample parking space for anyone making a trip to putrajaya line and kajang line the farthest poi...'\n",
      "Processing review 48/5929 (Review ID: 48, Station: KG04 KWASA DAMANSARA): 'this station is spacious and comfortable washrooms are clean prayer room is available not big but co...'\n",
      "Processing review 49/5929 (Review ID: 49, Station: KG04 KWASA DAMANSARA): 'at night it very dangerous to walk alone to parking area no security officer at sight...'\n",
      "Processing review 50/5929 (Review ID: 50, Station: KG04 KWASA DAMANSARA): 'newly opened putrajaya mrt line at the last stop before we can transit to the next kajang mrt line r...'\n",
      "Processing review 51/5929 (Review ID: 51, Station: KG04 KWASA DAMANSARA): 'great place and clean however most of the toilet was out of order many mrt station toilets was out o...'\n",
      "Processing review 52/5929 (Review ID: 52, Station: KG04 KWASA DAMANSARA): 'very clean well kept station with ample parkingconvrniently located...'\n",
      "Processing review 53/5929 (Review ID: 53, Station: KG04 KWASA DAMANSARA): 'toilet clean plenty parking space...'\n",
      "Processing review 54/5929 (Review ID: 54, Station: KG04 KWASA DAMANSARA): 'mrt station looks good but a male workers attitude is so bad and emotional that make i choose to lea...'\n",
      "Processing review 55/5929 (Review ID: 55, Station: KG04 KWASA DAMANSARA): 'the end of mrt line here and exchange of putrajaya line on the opposite side usually mrt to putrajay...'\n",
      "Processing review 56/5929 (Review ID: 56, Station: KG04 KWASA DAMANSARA): 'the most deserted train station i ever been i was the only person who took the ride from the station...'\n",
      "Processing review 57/5929 (Review ID: 57, Station: KG04 KWASA DAMANSARA): 'nice public transport place...'\n",
      "Processing review 58/5929 (Review ID: 58, Station: KG04 KWASA DAMANSARA): 'everything is easy to find and receptionist are really friendly and willing to help you out there wa...'\n",
      "Processing review 59/5929 (Review ID: 59, Station: KG04 KWASA DAMANSARA): 'if youre doing a transfer make sure you aware of the last train or youre ending up getting a grab wh...'\n",
      "Processing review 60/5929 (Review ID: 60, Station: KG04 KWASA DAMANSARA): 'my most favorite starting mrt station i like it here more than sg buloh sbk station as there is less...'\n",
      "Processing review 61/5929 (Review ID: 61, Station: KG04 KWASA DAMANSARA): 'need to have more shops overall is ok and clean...'\n",
      "Processing review 62/5929 (Review ID: 62, Station: KG04 KWASA DAMANSARA): 'still not fully utilized big parking space not crowded big building if it have provided floor convey...'\n",
      "Processing review 63/5929 (Review ID: 63, Station: KG04 KWASA DAMANSARA): 'clean but lack of convenient store and lack of road signs eg parking entrance...'\n",
      "Processing review 64/5929 (Review ID: 64, Station: KG04 KWASA DAMANSARA): 'always use mrt at kwasa damansara because it near to my house they have not lot people at here compa...'\n",
      "Processing review 65/5929 (Review ID: 65, Station: KG04 KWASA DAMANSARA): 'very well connected to other mrtlrtktm etc avoid all morningevening jammingvery clean indeed...'\n",
      "Processing review 66/5929 (Review ID: 66, Station: KG04 KWASA DAMANSARA): 'schedule screen at platform level should be at an angle for more passenger to notice...'\n",
      "Processing review 67/5929 (Review ID: 67, Station: KG04 KWASA DAMANSARA): 'superb and clean efficient service...'\n",
      "Processing review 68/5929 (Review ID: 68, Station: KG04 KWASA DAMANSARA): 'those who set their destination to seksyen kota damansara and want to use fully public transportplea...'\n",
      "Processing review 69/5929 (Review ID: 69, Station: KG04 KWASA DAMANSARA): 'too silent too wide open space there when night time sun havent out yet alone quite dangerous especi...'\n",
      "Processing review 70/5929 (Review ID: 70, Station: KG04 KWASA DAMANSARA): 'dead place as it just a transfer station...'\n",
      "Processing review 71/5929 (Review ID: 71, Station: KG04 KWASA DAMANSARA): 'it is convenienton kwasa damansara cz u arrived it as last stop then get off n walk opposite just to...'\n",
      "Processing review 72/5929 (Review ID: 72, Station: KG04 KWASA DAMANSARA): 'needs a kiosk or vending machine...'\n",
      "Processing review 73/5929 (Review ID: 73, Station: KG04 KWASA DAMANSARA): 'the mrt is okay but one thing i dont like about there is they sometimes close and didnt give a notic...'\n",
      "Processing review 74/5929 (Review ID: 74, Station: KG04 KWASA DAMANSARA): 'packed in the morningbut it was safeu can see police bantuan on board...'\n",
      "Processing review 75/5929 (Review ID: 75, Station: KG04 KWASA DAMANSARA): 'more people travel on mrt mainly covid cases much lesser and people feel more secure now...'\n",
      "Processing review 76/5929 (Review ID: 76, Station: KG04 KWASA DAMANSARA): 'a lot of parking here...'\n",
      "Processing review 77/5929 (Review ID: 77, Station: KG04 KWASA DAMANSARA): 'no official parking here but you can park at the shop lots clean toilet and surau...'\n",
      "Processing review 78/5929 (Review ID: 78, Station: KG04 KWASA DAMANSARA): 'nice metro stop but it needs to be connected better by foot to the other side of the parallel highwa...'\n",
      "Processing review 79/5929 (Review ID: 79, Station: KG04 KWASA DAMANSARA): 'very convenient to park car on the way to kl from the north then take the mrt rest of the way into t...'\n",
      "Processing review 80/5929 (Review ID: 80, Station: KG04 KWASA DAMANSARA): 'the interchange station between kajang and putrajaya lines well maintained with clear signboard dire...'\n",
      "Processing review 81/5929 (Review ID: 81, Station: KG04 KWASA DAMANSARA): 'transit station with a good facility and very convenient for every passenger...'\n",
      "Processing review 82/5929 (Review ID: 82, Station: KG04 KWASA DAMANSARA): 'newly built and very nice trains highly recommended to get around the city...'\n",
      "Processing review 83/5929 (Review ID: 83, Station: KG04 KWASA DAMANSARA): 'clean train on time...'\n",
      "Processing review 84/5929 (Review ID: 84, Station: KG04 KWASA DAMANSARA): 'very nice and clean...'\n",
      "Processing review 85/5929 (Review ID: 85, Station: KG04 KWASA DAMANSARA): 'great station for those who use public transport and its cheaper parking space for daylong...'\n",
      "Processing review 86/5929 (Review ID: 86, Station: KG04 KWASA DAMANSARA): 'new station male toilet not cleaned regularly no tissue sink pipe some not working need a repair...'\n",
      "Processing review 87/5929 (Review ID: 87, Station: KG04 KWASA DAMANSARA): 'nice and clean station really easy to get around...'\n",
      "Processing review 88/5929 (Review ID: 88, Station: KG04 KWASA DAMANSARA): 'parking there cannot accept ic touch n go system not ready...'\n",
      "Processing review 89/5929 (Review ID: 89, Station: KG04 KWASA DAMANSARA): 'fast and time right and peaceful...'\n",
      "Processing review 90/5929 (Review ID: 90, Station: KG04 KWASA DAMANSARA): 'ticket machine can only accept cash at the moment hmmm...'\n",
      "Processing review 91/5929 (Review ID: 91, Station: KG04 KWASA DAMANSARA): 'experience our local pubilic transport facilitiesnice...'\n",
      "Processing review 92/5929 (Review ID: 92, Station: KG04 KWASA DAMANSARA): 'spacious parking area per entry is rm...'\n",
      "Processing review 93/5929 (Review ID: 93, Station: KG04 KWASA DAMANSARA): 'this is exchange station between putrajaya line and kajang line easy exchange...'\n",
      "Processing review 94/5929 (Review ID: 94, Station: KG04 KWASA DAMANSARA): 'good...'\n",
      "  Warning: No valid segments found for review 94. Processing original review as one unit.\n",
      "Processing review 95/5929 (Review ID: 95, Station: KG04 KWASA DAMANSARA): 'beautiful newly opened...'\n",
      "Processing review 96/5929 (Review ID: 96, Station: KG04 KWASA DAMANSARA): 'interchange for mrt kajang line...'\n",
      "Processing review 97/5929 (Review ID: 97, Station: KG04 KWASA DAMANSARA): 'fast...'\n",
      "  Warning: No valid segments found for review 97. Processing original review as one unit.\n",
      "Processing review 98/5929 (Review ID: 98, Station: KG04 KWASA DAMANSARA): 'interchange terminal for mrt kajang line putrajaya line soon to be opened...'\n",
      "Processing review 99/5929 (Review ID: 99, Station: KG04 KWASA DAMANSARA): 'great service...'\n",
      "Processing review 100/5929 (Review ID: 100, Station: KG04 KWASA DAMANSARA): 'well maintained and clean environment...'\n",
      "Processing review 101/5929 (Review ID: 101, Station: KG04 KWASA DAMANSARA): 'good place...'\n",
      "Processing review 102/5929 (Review ID: 102, Station: KG04 KWASA DAMANSARA): 'what a lovely joy ride...'\n",
      "Processing review 103/5929 (Review ID: 103, Station: KG04 KWASA DAMANSARA): 'excellent...'\n",
      "  Warning: No valid segments found for review 103. Processing original review as one unit.\n",
      "Processing review 104/5929 (Review ID: 104, Station: KG04 KWASA DAMANSARA): 'nice...'\n",
      "  Warning: No valid segments found for review 104. Processing original review as one unit.\n",
      "Processing review 105/5929 (Review ID: 105, Station: KG04 KWASA DAMANSARA): 'suddenly stopped the service without notice really disappointed...'\n",
      "Processing review 106/5929 (Review ID: 106, Station: KG04 KWASA DAMANSARA): 'it served what it needed to an mrt station...'\n",
      "Processing review 107/5929 (Review ID: 107, Station: KG04 KWASA DAMANSARA): 'now it is the terminal station for the mrt kajang line...'\n",
      "Processing review 108/5929 (Review ID: 108, Station: KG04 KWASA DAMANSARA): 'crowded because next connecting train was to putrajaya...'\n",
      "Processing review 109/5929 (Review ID: 109, Station: KG04 KWASA DAMANSARA): 'good stesen i connected to the new putrajaya line...'\n",
      "Processing review 110/5929 (Review ID: 110, Station: KG04 KWASA DAMANSARA): 'nice and clean train came on time...'\n",
      "Processing review 111/5929 (Review ID: 111, Station: KG04 KWASA DAMANSARA): 'very clean place and everything looks really futuristic nice...'\n",
      "Processing review 112/5929 (Review ID: 112, Station: KG04 KWASA DAMANSARA): 'last station and transit station to putrajaya lane opening soon...'\n",
      "Processing review 113/5929 (Review ID: 113, Station: KG04 KWASA DAMANSARA): 'best can travel to damansara or kepong fast...'\n",
      "Processing review 114/5929 (Review ID: 114, Station: KG04 KWASA DAMANSARA): 'very clean mrt station...'\n",
      "Processing review 115/5929 (Review ID: 115, Station: KG04 KWASA DAMANSARA): 'niceee...'\n",
      "  Warning: No valid segments found for review 115. Processing original review as one unit.\n",
      "Processing review 116/5929 (Review ID: 116, Station: KG04 KWASA DAMANSARA): 'nice n clean...'\n",
      "Processing review 117/5929 (Review ID: 117, Station: KG04 KWASA DAMANSARA): 'this its the morning time and neutral beautiful place...'\n",
      "Processing review 118/5929 (Review ID: 118, Station: KG04 KWASA DAMANSARA): 'modern public transportation with awesome facilities and clean...'\n",
      "Processing review 119/5929 (Review ID: 119, Station: KG04 KWASA DAMANSARA): 'convenient...'\n",
      "  Warning: No valid segments found for review 119. Processing original review as one unit.\n",
      "Processing review 120/5929 (Review ID: 120, Station: KG04 KWASA DAMANSARA): 'convenient lots of parking space...'\n",
      "Processing review 121/5929 (Review ID: 121, Station: KG04 KWASA DAMANSARA): 'nice n the views was awesome...'\n",
      "Processing review 122/5929 (Review ID: 123, Station: KG04 KWASA DAMANSARA): 'i liked it not that popular with the public...'\n",
      "Processing review 123/5929 (Review ID: 124, Station: KG04 KWASA DAMANSARA): 'not so easy due traffic jam...'\n",
      "Processing review 124/5929 (Review ID: 125, Station: KG04 KWASA DAMANSARA): 'easy travel companion...'\n",
      "Processing review 125/5929 (Review ID: 126, Station: KG04 KWASA DAMANSARA): 'sometimes the touch n go for carpark had problem...'\n",
      "Processing review 126/5929 (Review ID: 127, Station: KG04 KWASA DAMANSARA): 'you also get mrt bus out of mrt train...'\n",
      "Processing review 127/5929 (Review ID: 128, Station: KG04 KWASA DAMANSARA): 'the staff were extremely helpful...'\n",
      "Processing review 128/5929 (Review ID: 129, Station: KG04 KWASA DAMANSARA): 'nearest mrt station to elmina...'\n",
      "Processing review 129/5929 (Review ID: 130, Station: KG04 KWASA DAMANSARA): 'spacious and got parking...'\n",
      "Processing review 130/5929 (Review ID: 131, Station: KG04 KWASA DAMANSARA): 'nice place no noise...'\n",
      "Processing review 131/5929 (Review ID: 132, Station: KG04 KWASA DAMANSARA): 'new place and very nice...'\n",
      "Processing review 132/5929 (Review ID: 133, Station: KG04 KWASA DAMANSARA): 'its new with parking...'\n",
      "Processing review 133/5929 (Review ID: 134, Station: KG04 KWASA DAMANSARA): 'facilities condition is satisfied...'\n",
      "Processing review 134/5929 (Review ID: 135, Station: KG04 KWASA DAMANSARA): 'beautiful train station...'\n",
      "Processing review 135/5929 (Review ID: 136, Station: KG04 KWASA DAMANSARA): 'convenient transit station...'\n",
      "Processing review 136/5929 (Review ID: 137, Station: KG04 KWASA DAMANSARA): 'clean...'\n",
      "  Warning: No valid segments found for review 137. Processing original review as one unit.\n",
      "Processing review 137/5929 (Review ID: 139, Station: KG04 KWASA DAMANSARA): 'i loved...'\n",
      "Processing review 138/5929 (Review ID: 140, Station: KG04 KWASA DAMANSARA): 'ok...'\n",
      "  Warning: No valid segments found for review 140. Processing original review as one unit.\n",
      "Processing review 139/5929 (Review ID: 141, Station: KG04 KWASA DAMANSARA): 'looks like undeveloped...'\n",
      "Processing review 140/5929 (Review ID: 142, Station: KG04 KWASA DAMANSARA): 'nice and clean...'\n",
      "  Warning: No valid segments found for review 142. Processing original review as one unit.\n",
      "Processing review 141/5929 (Review ID: 143, Station: KG04 KWASA DAMANSARA): 'good travel...'\n",
      "Processing review 142/5929 (Review ID: 144, Station: KG04 KWASA DAMANSARA): 'inter home commuters...'\n",
      "Processing review 143/5929 (Review ID: 145, Station: KG04 KWASA DAMANSARA): 'very clean...'\n",
      "Processing review 144/5929 (Review ID: 146, Station: KG04 KWASA DAMANSARA): 'clean and comfortable...'\n",
      "  Warning: No valid segments found for review 146. Processing original review as one unit.\n",
      "Processing review 145/5929 (Review ID: 147, Station: KG04 KWASA DAMANSARA): 'lot of parking...'\n",
      "Processing review 146/5929 (Review ID: 148, Station: KG04 KWASA DAMANSARA): 'nice place...'\n",
      "Processing review 147/5929 (Review ID: 154, Station: KG04 KWASA DAMANSARA): 'if you want to go to the kwsp building using public transport the easiest way is to take the kwasa d...'\n",
      "Processing review 148/5929 (Review ID: 155, Station: KG04 KWASA DAMANSARA): 'big station and quite remote position it is hoped to provide police assistance to monitor this area ...'\n",
      "Processing review 149/5929 (Review ID: 156, Station: KG04 KWASA DAMANSARA): 'please help if you finish work at night and want to return at from kwasa damansara to metro perimay ...'\n",
      "Processing review 150/5929 (Review ID: 157, Station: KG04 KWASA DAMANSARA): 'impressed with the mrt service great children enjoy riding the mrt...'\n",
      "Processing review 151/5929 (Review ID: 158, Station: KG04 KWASA DAMANSARA): 'please look at the public utility toilets there are so many that are not used or broken you have to ...'\n",
      "Processing review 152/5929 (Review ID: 159, Station: KG04 KWASA DAMANSARA): 'beautiful and clean there are levels of track level and level make sure you wait on the right track ...'\n",
      "Processing review 153/5929 (Review ID: 160, Station: KG04 KWASA DAMANSARA): 'best first time trail here...'\n",
      "Processing review 154/5929 (Review ID: 161, Station: KG04 KWASA DAMANSARA): 'for those who get off at mrt kwasa damansara for the first time and want to go to the epf building n...'\n",
      "Processing review 155/5929 (Review ID: 162, Station: KG04 KWASA DAMANSARA): 'nice places allnew mrtbig and comfortable...'\n",
      "Processing review 156/5929 (Review ID: 163, Station: KG04 KWASA DAMANSARA): 'the prayer hall and toilets are clean no bus service is available...'\n",
      "Processing review 157/5929 (Review ID: 164, Station: KG04 KWASA DAMANSARA): 'still not fully functional the way in and out is quite difficult if you want to park better go to kw...'\n",
      "Processing review 158/5929 (Review ID: 165, Station: KG04 KWASA DAMANSARA): 'the parking lot near the mrt building will flood if it rains heavily i always park in the middle are...'\n",
      "Processing review 159/5929 (Review ID: 166, Station: KG04 KWASA DAMANSARA): 'cctv staff should be more alert to see male passengers sitting in female coaches...'\n",
      "Processing review 160/5929 (Review ID: 167, Station: KG04 KWASA DAMANSARA): 'net punctuality  coaches are provided for women but there are still many men who are not sensitive a...'\n",
      "Processing review 161/5929 (Review ID: 168, Station: KG04 KWASA DAMANSARA): 'its a bit disappointing isnt itbecause the travel time is stopped until pm on sundaywhile we have to...'\n",
      "Processing review 162/5929 (Review ID: 169, Station: KG04 KWASA DAMANSARA): 'internet line cemsiyelll yesterday was a nightmare to walk out to shelll jlan sungai buloh just to o...'\n",
      "Processing review 163/5929 (Review ID: 170, Station: KG04 KWASA DAMANSARA): 'ride from mrt damansara area to lrt bandar tasik selatan just rm only very cheap and nice love it...'\n",
      "Processing review 164/5929 (Review ID: 171, Station: KG04 KWASA DAMANSARA): 'its ok where does the mrt leak when it rains xde kiosk shop...'\n",
      "Processing review 165/5929 (Review ID: 172, Station: KG04 KWASA DAMANSARA): 'another great project by najib razak for the people that blasphemes...'\n",
      "Processing review 166/5929 (Review ID: 173, Station: KG04 KWASA DAMANSARA): 'exchange station no need to exit the station take seconds to exchange kwasa damansara kajang kwasa d...'\n",
      "Processing review 167/5929 (Review ID: 174, Station: KG04 KWASA DAMANSARA): 'clean less crowd and parking...'\n",
      "Processing review 168/5929 (Review ID: 175, Station: KG04 KWASA DAMANSARA): 'beautiful and clean...'\n",
      "  Warning: No valid segments found for review 175. Processing original review as one unit.\n",
      "Processing review 169/5929 (Review ID: 176, Station: KG04 KWASA DAMANSARA): 'the best is very suitable for all groups of people to use this mrt facility...'\n",
      "Processing review 170/5929 (Review ID: 177, Station: KG04 KWASA DAMANSARA): 'st time coming here...'\n",
      "Processing review 171/5929 (Review ID: 178, Station: KG04 KWASA DAMANSARA): 'ordinary light rail station...'\n",
      "Processing review 172/5929 (Review ID: 179, Station: KG04 KWASA DAMANSARA): 'the area that used to be full of forest is now rapidly developing...'\n",
      "Processing review 173/5929 (Review ID: 180, Station: KG04 KWASA DAMANSARA): 'the relatively hot environment may be due to the lack of green trees...'\n",
      "Processing review 174/5929 (Review ID: 181, Station: KG04 KWASA DAMANSARA): 'if you close update in maps in the internet world the service is right...'\n",
      "Processing review 175/5929 (Review ID: 182, Station: KG04 KWASA DAMANSARA): 'an interchange station from kl to putrajaya...'\n",
      "Processing review 176/5929 (Review ID: 183, Station: KG04 KWASA DAMANSARA): 'the parking lot is spacious and the facilities provided are sufficient...'\n",
      "Processing review 177/5929 (Review ID: 184, Station: KG04 KWASA DAMANSARA): 'my station is running i cant believe im running 🤭...'\n",
      "Processing review 178/5929 (Review ID: 185, Station: KG04 KWASA DAMANSARA): 'pleasant exchange center a very convenient interchange...'\n",
      "Processing review 179/5929 (Review ID: 186, Station: KG04 KWASA DAMANSARA): 'beautifully spacious and comfortable...'\n",
      "Processing review 180/5929 (Review ID: 187, Station: KG04 KWASA DAMANSARA): 'comfortable the best my boss...'\n",
      "Processing review 181/5929 (Review ID: 188, Station: KG04 KWASA DAMANSARA): 'clean place many facilities provided...'\n",
      "Processing review 182/5929 (Review ID: 189, Station: KG04 KWASA DAMANSARA): 'lancorrr the journey...'\n",
      "Processing review 183/5929 (Review ID: 190, Station: KG04 KWASA DAMANSARA): 'the place is a bit quiet but it will progress one day...'\n",
      "Processing review 184/5929 (Review ID: 191, Station: KG04 KWASA DAMANSARA): 'there arent many people here im a little scared...'\n",
      "Processing review 185/5929 (Review ID: 192, Station: KG04 KWASA DAMANSARA): 'easy way to go to sungai buloh...'\n",
      "Processing review 186/5929 (Review ID: 193, Station: KG04 KWASA DAMANSARA): 'good clean and comfortable...'\n",
      "Processing review 187/5929 (Review ID: 194, Station: KG04 KWASA DAMANSARA): 'wide and big not too crowded...'\n",
      "Processing review 188/5929 (Review ID: 195, Station: KG04 KWASA DAMANSARA): 'interchange line kajang and line putrajaya...'\n",
      "Processing review 189/5929 (Review ID: 196, Station: KG04 KWASA DAMANSARA): 'equipped with paid car parking...'\n",
      "Processing review 190/5929 (Review ID: 197, Station: KG04 KWASA DAMANSARA): 'they are silent...'\n",
      "Processing review 191/5929 (Review ID: 198, Station: KG04 KWASA DAMANSARA): 'the toilets and surau are clean...'\n",
      "Processing review 192/5929 (Review ID: 199, Station: KG04 KWASA DAMANSARA): 'clear station...'\n",
      "Processing review 193/5929 (Review ID: 200, Station: KG04 KWASA DAMANSARA): 'kwasa damansara mrt station...'\n",
      "Processing review 194/5929 (Review ID: 202, Station: KG04 KWASA DAMANSARA): 'cant see the toilet...'\n",
      "Database connection closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 335\u001b[0m\n\u001b[0;32m    332\u001b[0m     review_text_col_in_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# Call the main processing function directly\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m \u001b[43mprocess_reviews_from_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_text_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreview_text_col_in_db\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 296\u001b[0m, in \u001b[0;36mprocess_reviews_from_database\u001b[1;34m(review_text_column)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Warning: No valid segments found for review \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreviews_id_db\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Processing original review as one unit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, segment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(segments):\n\u001b[1;32m--> 296\u001b[0m     bert_aspect_terms_segment \u001b[38;5;241m=\u001b[39m \u001b[43mextract_aspect_terms_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     rule_based_found_categories_segment \u001b[38;5;241m=\u001b[39m identify_aspects_rule_based(segment, aspect_dictionary)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m# Insert into AspectSentiments table (only for Hybrid results, as before)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 161\u001b[0m, in \u001b[0;36mextract_aspect_terms_bert\u001b[1;34m(segment_text, tokenizer, ate_model, device, max_len)\u001b[0m\n\u001b[0;32m    158\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 161\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    164\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1869\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1863\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1869\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1875\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1876\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1877\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1881\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1883\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1144\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1156\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1157\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CODE TO FIND ASPECT AND SENTIMENT THEN SEND TO DATABASE\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- Database Configuration ---\n",
    "DB_NAME = r\"C:\\Users\\unitf\\OneDrive\\Desktop\\mrt_reviews.db\" # <<< --- IMPORTANT: CONFIRM THIS PATH\n",
    "\n",
    "# --- NLTK Data Download Check (Internal Check, assumes external download done) ---\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK 'punkt' tokenizer data found.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' tokenizer data NOT found by script. Please ensure it's downloaded manually.\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    print(\"NLTK 'punkt_tab' tokenizer data found.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt_tab' tokenizer data NOT found by script. Please ensure it's downloaded manually.\")\n",
    "\n",
    "\n",
    "# --- 1. Define Model Configurations and Mappings ---\n",
    "ATE_LABELS = ['non-aspect', 'b-term', 'i-term']\n",
    "ATE_ID2LABEL = {i: label for i, label in enumerate(ATE_LABELS)}\n",
    "ATE_LABEL2ID = {label: i for i, label in enumerate(ATE_LABELS)}\n",
    "NUM_ATE_LABELS = len(ATE_LABELS)\n",
    "\n",
    "ABSA_LABELS = ['Negative', 'Neutral', 'Positive']\n",
    "ABSA_ID2LABEL = {i: label for i, label in enumerate(ABSA_LABELS)}\n",
    "ABSA_LABEL2ID = {label: i for i, label in enumerate(ABSA_LABELS)}\n",
    "NUM_ABSA_LABELS = len(ABSA_LABELS)\n",
    "\n",
    "# --- 2. Load Tokenizer and Models ---\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load ATE Model\n",
    "try:\n",
    "    ate_model = BertForTokenClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=NUM_ATE_LABELS, id2label=ATE_ID2LABEL, label2id=ATE_LABEL2ID, ignore_mismatched_sizes=True\n",
    "    )\n",
    "    ate_model.load_state_dict(torch.load('ate_model_v1.pkl', map_location=device), strict=False)\n",
    "    ate_model.to(device)\n",
    "    ate_model.eval()\n",
    "    print(f\"ATE model 'ate_model_v1.pkl' loaded successfully on {device} (with strict=False)!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ate_model_v1.pkl' not found. Please ensure the path is correct.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ATE model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load ABSA Model\n",
    "try:\n",
    "    absa_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=NUM_ABSA_LABELS, id2label=ABSA_ID2LABEL, label2id=ABSA_LABEL2ID\n",
    "    )\n",
    "    absa_model.load_state_dict(torch.load('absa_model_v1.pkl', map_location=device))\n",
    "    absa_model.to(device)\n",
    "    absa_model.eval()\n",
    "    print(f\"ABSA model 'absa_model_v1.pkl' loaded successfully on {device}!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'absa_model_v1.pkl' not found. Please ensure the path is correct.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ABSA model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Load Aspect Dictionary ---\n",
    "aspect_dictionary = {}\n",
    "ASPECT_DICT_PATH = 'aspect_dictionary.csv'\n",
    "\n",
    "try:\n",
    "    aspect_dict_df = pd.read_csv(ASPECT_DICT_PATH)\n",
    "    if 'term' in aspect_dict_df.columns and 'category' in aspect_dict_df.columns:\n",
    "        for index, row in aspect_dict_df.iterrows():\n",
    "            term = str(row['term']).strip().lower()\n",
    "            category = str(row['category']).strip().lower()\n",
    "            if term and category:\n",
    "                if term not in aspect_dictionary:\n",
    "                    aspect_dictionary[term] = []\n",
    "                if category not in aspect_dictionary[term]:\n",
    "                    aspect_dictionary[term].append(category)\n",
    "        print(f\"Aspect dictionary '{ASPECT_DICT_PATH}' loaded successfully with {len(aspect_dictionary)} terms.\")\n",
    "    else:\n",
    "        print(f\"Warning: '{ASPECT_DICT_PATH}' must contain 'term' and 'category' columns. Rule-based aspect identification will be skipped.\")\n",
    "        aspect_dictionary = {}\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Aspect dictionary file '{ASPECT_DICT_PATH}' not found. Rule-based aspect identification will be skipped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading aspect dictionary: {e}. Rule-based aspect identification will be skipped.\")\n",
    "    aspect_dictionary = {}\n",
    "\n",
    "# --- 4. Text Preprocessing (Minimal) ---\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Sentence Splitting Function ---\n",
    "def split_review_into_sentences(review_text):\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        punkt_available = True\n",
    "    except LookupError:\n",
    "        punkt_available = False\n",
    "\n",
    "    if not punkt_available:\n",
    "        print(\"Warning: NLTK 'punkt' tokenizer not available. Using simpler splitting for this run.\")\n",
    "        sentences = re.split(r'[.!?]', review_text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    else:\n",
    "        sentences = nltk.sent_tokenize(review_text)\n",
    "\n",
    "    final_segments = []\n",
    "    split_conjunctions_pattern = r'\\b(?:' + '|'.join(re.escape(conj) for conj in ['and', 'but', 'or', 'because', 'so']) + r')\\b'\n",
    "    internal_split_punct_pattern = r'[;:]'\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_segments = [sentence]\n",
    "\n",
    "        temp_segments_punct = []\n",
    "        for seg in current_segments:\n",
    "            parts = re.split(internal_split_punct_pattern, seg)\n",
    "            temp_segments_punct.extend([p.strip() for p in parts if p.strip()])\n",
    "        current_segments = temp_segments_punct\n",
    "        \n",
    "        temp_segments_conj = []\n",
    "        for seg in current_segments:\n",
    "            parts = re.split(split_conjunctions_pattern, seg, flags=re.IGNORECASE)\n",
    "            temp_segments_conj.extend([p.strip() for p in parts if p.strip()])\n",
    "        current_segments = temp_segments_conj\n",
    "\n",
    "        final_segments.extend([s.strip() for s in current_segments if s.strip()])\n",
    "\n",
    "    return [segment for segment in final_segments if segment and len(segment.split()) > 1]\n",
    "\n",
    "# --- 5. Aspect Term Extraction (ATE) Function (BERT-based) ---\n",
    "def extract_aspect_terms_bert(segment_text, tokenizer, ate_model, device, max_len=128):\n",
    "    preprocessed_text = preprocess_text(segment_text)\n",
    "    if not preprocessed_text: return []\n",
    "\n",
    "    encoding = tokenizer.encode_plus(preprocessed_text, add_special_tokens=True, max_length=max_len,\n",
    "                                     padding='max_length', truncation=True, return_tensors='pt', return_token_type_ids=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding['token_type_ids'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = ate_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().cpu().numpy()\n",
    "    extracted_aspects = []\n",
    "    current_aspect_tokens = []\n",
    "    original_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())\n",
    "    valid_length = (attention_mask.squeeze() == 1).sum().item()\n",
    "    \n",
    "    for i in range(1, valid_length - 1):\n",
    "        token = original_tokens[i]\n",
    "        predicted_label = ATE_ID2LABEL[predictions[i]]\n",
    "        if predicted_label == 'b-term':\n",
    "            if current_aspect_tokens: extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "            current_aspect_tokens = [token]\n",
    "        elif predicted_label == 'i-term':\n",
    "            if current_aspect_tokens: current_aspect_tokens.append(token)\n",
    "        else:\n",
    "            if current_aspect_tokens: extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', '')); current_aspect_tokens = []\n",
    "    if current_aspect_tokens: extracted_aspects.append(tokenizer.convert_tokens_to_string(current_aspect_tokens).replace(' ##', ''))\n",
    "    return list(set([aspect.strip() for aspect in extracted_aspects if aspect.strip()]))\n",
    "\n",
    "# --- 6. Rule-Based Aspect Identification Function ---\n",
    "def identify_aspects_rule_based(segment_text, aspect_dict):\n",
    "    found_categories = set()\n",
    "    preprocessed_segment = preprocess_text(segment_text)\n",
    "    sorted_terms = sorted(aspect_dict.keys(), key=len, reverse=True)\n",
    "    for term in sorted_terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', preprocessed_segment):\n",
    "            for cat in aspect_dict[term]: found_categories.add(cat)\n",
    "    return sorted(list(found_categories))\n",
    "\n",
    "# --- 7. Aspect-Based Sentiment Analysis (ABSA) Function ---\n",
    "def analyze_aspect_sentiment(segment_text, aspect_term, tokenizer, absa_model, device, max_len=128):\n",
    "    preprocessed_segment = preprocess_text(segment_text)\n",
    "    preprocessed_aspect = preprocess_text(aspect_term)\n",
    "    if not preprocessed_segment or not preprocessed_aspect: return \"N/A\"\n",
    "\n",
    "    inputs = tokenizer.encode_plus(preprocessed_segment, preprocessed_aspect, add_special_tokens=True,\n",
    "                                   max_length=max_len, padding='max_length', truncation=True,\n",
    "                                   return_attention_mask=True, return_tensors='pt', return_token_type_ids=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = absa_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "    sentiment = ABSA_ID2LABEL.get(torch.argmax(logits, dim=1).item(), \"Unknown Sentiment\")\n",
    "    return sentiment\n",
    "\n",
    "# --- Database Insertion Functions ---\n",
    "def get_or_create_station_id(cursor, station_name_full):\n",
    "    cursor.execute(\"SELECT station_id FROM stations WHERE station_name = ?\", (station_name_full,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cursor.execute(\"INSERT INTO stations (station_name) VALUES (?)\", (station_name_full,))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "def insert_review_data(cursor, station_id, station_name_full, review_text_content, review_date_str):\n",
    "    cursor.execute(\"INSERT INTO reviews (station_id, station_name, raw_reviews, review_date) VALUES (?, ?, ?, ?)\",\n",
    "                   (station_id, station_name_full, review_text_content, review_date_str))\n",
    "    return cursor.lastrowid\n",
    "\n",
    "def insert_aspect_sentiment_data(cursor, review_id, station_id, segment_index, segment_text,\n",
    "                                 aspect_category, sentiment_polarity, extracted_aspect_term=None,\n",
    "                                 analysis_method=None):\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO AspectSentiments (\n",
    "            review_id, station_id, segment_index, segment_text, aspect_category,\n",
    "            sentiment_polarity, extracted_aspect_term, analysis_method\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (review_id, station_id, segment_index, segment_text, aspect_category,\n",
    "         sentiment_polarity, extracted_aspect_term, analysis_method)\n",
    "    )\n",
    "\n",
    "# --- Main Database Processing Function (MODIFIED TO FETCH reviews_id) ---\n",
    "def process_reviews_from_database(review_text_column='raw_reviews'):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_NAME)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure AspectSentiments table exists with the correct schema\n",
    "        create_aspect_sentiments_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS AspectSentiments (\n",
    "            aspect_sentiment_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            review_id INTEGER NOT NULL,\n",
    "            station_id INTEGER NOT NULL,\n",
    "            segment_index INTEGER NOT NULL,\n",
    "            segment_text TEXT NOT NULL,\n",
    "            aspect_category TEXT NOT NULL,\n",
    "            sentiment_polarity TEXT NOT NULL,\n",
    "            extracted_aspect_term TEXT,\n",
    "            analysis_method TEXT,\n",
    "            FOREIGN KEY (review_id) REFERENCES reviews (review_id),\n",
    "            FOREIGN KEY (station_id) REFERENCES stations (station_id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_aspect_sentiments_table_sql)\n",
    "        conn.commit()\n",
    "        print(\"Database connected. AspectSentiments table ensured.\")\n",
    "\n",
    "        # Fetch all reviews from the reviews table, explicitly selecting reviews_id\n",
    "        # Ensure 'reviews_id' is the correct primary key name in your 'reviews' table\n",
    "        # Also ensure 'station_id', 'station_name', 'review_date' exist\n",
    "        cursor.execute(f\"SELECT reviews_id, station_id, station_name, review_date, {review_text_column} FROM reviews\")\n",
    "        all_reviews = cursor.fetchall()\n",
    "        \n",
    "        print(f\"Successfully loaded {len(all_reviews)} reviews from the database.\")\n",
    "\n",
    "        for index, review_row in enumerate(all_reviews):\n",
    "            # Map columns from the fetched row, now using 'reviews_id_db' for clarity\n",
    "            reviews_id_db, station_id_db, station_name_db, review_date_db, review_text_db = review_row\n",
    "\n",
    "            original_review_text = review_text_db\n",
    "            review_text_processed = str(original_review_text).strip()\n",
    "\n",
    "            if pd.isna(original_review_text) or not review_text_processed:\n",
    "                print(f\"Skipping empty or NaN review_id {reviews_id_db}.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing review {index + 1}/{len(all_reviews)} (Review ID: {reviews_id_db}, Station: {station_name_db}): '{review_text_processed[:100]}...'\")\n",
    "\n",
    "            # --- Sentence Splitting ---\n",
    "            segments = split_review_into_sentences(review_text_processed)\n",
    "            if not segments:\n",
    "                segments = [review_text_processed]\n",
    "                print(f\"  Warning: No valid segments found for review {reviews_id_db}. Processing original review as one unit.\")\n",
    "\n",
    "            for i, segment in enumerate(segments):\n",
    "                bert_aspect_terms_segment = extract_aspect_terms_bert(segment, tokenizer, ate_model, device)\n",
    "                \n",
    "                rule_based_found_categories_segment = identify_aspects_rule_based(segment, aspect_dictionary)\n",
    "\n",
    "                # Insert into AspectSentiments table (only for Hybrid results, as before)\n",
    "                if rule_based_found_categories_segment:\n",
    "                    for category in rule_based_found_categories_segment:\n",
    "                        sentiment = analyze_aspect_sentiment(segment, category, tokenizer, absa_model, device)\n",
    "                        \n",
    "                        insert_aspect_sentiment_data(\n",
    "                            cursor, reviews_id_db, station_id_db, i, segment, category, sentiment, None, \"Hybrid\"\n",
    "                        )\n",
    "            \n",
    "        conn.commit()\n",
    "        print(\"\\nAll reviews processed and results inserted into AspectSentiments table.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        if conn: conn.rollback()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        if conn: conn.rollback()\n",
    "    finally:\n",
    "        if conn: conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "\n",
    "\n",
    "# --- How to use this new functionality ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- MRT Review Aspect-Based Sentiment Analysis (Database Batch Mode) ---\")\n",
    "    print(f\"Attempting to load aspect dictionary from: {ASPECT_DICT_PATH}\")\n",
    "    \n",
    "    # Prompt for the column containing review text in your reviews table\n",
    "    # This should be 'raw_reviews' based on your screenshot\n",
    "    review_text_col_in_db = input(\"Enter the name of the review text column in your 'reviews' table (default is 'raw_reviews'): \")\n",
    "    if not review_text_col_in_db.strip(): \n",
    "        review_text_col_in_db = 'raw_reviews'\n",
    "\n",
    "    # Call the main processing function directly\n",
    "    process_reviews_from_database(review_text_column=review_text_col_in_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab35168-f12c-43ae-b8f3-2be03cd566a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
